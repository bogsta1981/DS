{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessor programming\n",
    "Created by Francesc Dantí & Lluis Garrido  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture is centered in Multiprocessor programming\n",
    "\n",
    "+ **Multiprocessor programming** is a programming technique that enables taking profit from the multiple processors from a single computer or a set of computers interconnected between them.\n",
    "+ **Big Data** is a term that encompasses analyzing, visualizing and storing large data sets. Big data is usually difficult to deal with most desktop statistic and visualization packages, requiring therefore parallel software running on tens, hundreds or even thousands of processors. You'll see this with Francesc Dantí. Jordi Nin will introduce you into Spark, a specialized package for this kind of problems.\n",
    "\n",
    "In this lecture we will focus on single computer multiprocessor programming, that is, on using the multiple processors available in a desktop computer to reduce the computation time necessary to solve a problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Major chip manufactores have give up trying to make processors run faster. Moore's law continues to be true: each year more and more transistors fit into the same space, but their clock speed cannot be increased without overheating. With the current technology, a silicon based chip would melt at a clock speed of 5 GHz! Instead, manufactorers have turned to \"multicore\" architectures, in which multiple processors (cores) communicate through hardware. \n",
    "\n",
    "The operating system is the application that manages these multiple cores. If two computation intensive processes (i.e. applications) are run on the computer, the operating system manages to run each task on two different cores. If we only have a single computation intensive task it will only run on one core, even if our computer has multiple cores. If nothing is done explicitly, we will waste a lot of computation power!\n",
    "\n",
    "Our objective in these lectures is to show the tools python offers us to take advantage of the multiple processors. In order to take profit of the multicore capabilities the amount of tasks should equal the number of processors. There is no much sense in defining more tasks than cores we have, that is, there is no sense in manually defining 8 computation intenstive tasks if our computer has only 4 cores. In this latter case the operating sytem will try to run 8 tasks using 4 cores. Technically, you'll see a loss of efficiency in this case since the operating system will loose compuptation time switching between the tasks.\n",
    "\n",
    "We will see how to do this in these lectures from a low-level point of view. That is, we will manually split the computation work in\n",
    "multiple tasks so that each one is executed in different cores. This is what usually has to be done: the programmer\n",
    "has to manually perform the split and the operating system will automatically execute\n",
    "each task on a different core. This is the principle of parallel programming: harnessing\n",
    "multiple processors to work on a single task by dividing it in multiple (smaller)\n",
    "tasks. \n",
    "\n",
    "Parallelization can also be performed by means of distributed computing. Whereas in multicore systems the cores communicate between themselves through the bus at the hardware level, in distributed systems a software communicates and coordinates the actions of computational entities located on a network. The computational entities are usually computers. In distributed computing a large number of discrete computers, named nodes, distributed across a network (e.g., the Internet) devote some or all of their computation time to solve a common problem; each node receives and completes many\n",
    "small tasks, reporting the results to a central server which integrates the reults into the overall solution. However, since information is exchanged trough the network, care must be taken in order to select the amount of information that is passed in order to optimize the computational performance.\n",
    "\n",
    "IPython offers indeed an environment capable of dealing with both architectures in a transparent\n",
    "manner for the programmer. The user should be aware of the underlying\n",
    "architecture in which the application will be run in order to avoid performance\n",
    "degradation.\n",
    "\n",
    "## What can be parallelized?\n",
    "\n",
    "What kind of algorithms can be parallelized? Well, stricly speaking we may say that all algorithms can be parallelized up to certain degree. The question would be: what kind of algorithms can be easily parallelized? The answer is simple: those algorithms in which we need to process the elements of the database in an independent manner. For instance, matrix multiplication is parallelizable task. Take a look at the figure below: each element of the resulting matrix is computed in an independent way with respect the other elements. This means that we can split the resulting matrix; each part can be assigned to a different task. Matrix-like multiplication problems appear often in different kind of problems: think, for instance, in the matrix refactorization algorithm used by recommenders.\n",
    "\n",
    "Other kind of problems that can be easily parallelized are document analysis. For instance, assume we have several documents which need to be cleaned previous to analysis. This cleaning can be easily parallelized since usually each document is processed in an independent manner. The analysis itself may be parallelizable if each document can be processed in an independent manner with respect the others. Other kind of problems are counting words in a document, for instance.\n",
    "\n",
    "<br/>\n",
    "<center><img width=\"40%\" src=\"images/matmul.png\"></center>\n",
    "<br/>\n",
    "\n",
    "What is not easily parallelizable? With your knowledge, don't try to parallelize algorithms such as the supervised learning or graph algorithms. These kind of algorithms need \"all the data at once\" to learn and thus it is not an easy parallelizable problem. Another completely different problem would be to train several different support vector machines. In this case each machine can be trained in an independent manner with respect to the others. \n",
    "\n",
    "In conclusion, you can parallelize those problems in which you are able to divide the problem into independent tasks.\n",
    "\n",
    "#### How can a code be parallelized?\n",
    "\n",
    "This is a difficult question to answer but some hints are given here. In order to be able to transform a sequential application into a parallel application, one needs to detect those parts of the code in which independent tasks are performed. Once those parts have been identified, the code has to be reorganized to take advantge of this independence.\n",
    "\n",
    "Two different decomposition techniques (of the code) can be used:\n",
    "+ Task decomposition: identify within the code those tasks that are independent with respect each other. For instance, independent function calls, independent iterations of a loop, and so on. E.g.: performing different matrix multiplications, classifying multiple elements using a SVM, ... \n",
    "+ Data decomposition: it is based on dividing the data into smaller parts. Each part is assigned to a different task. E.g.: the matrix multiplication by dividing it into different parts, a big file that is divided into parts so that each part is processed by a different engine (see New York example below).\n",
    "\n",
    "There is no magic way to identify the way the code can be reorganized. However, it is useful to begin with those parts of the code that have a high computational load. \n",
    "\n",
    "Finally, one word of advise: do not enter the world of parallelization unless you are sure your algorithms works. Once you are sure it works, you may try parallezation if execution time is important for you. Use it if you have a lot of data to process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture overview\n",
    "\n",
    "We show here a simplified version of the architecture that allows us to take advantage of multiple cores (for a more detailed description please see this [link](http://ipython.org/ipython-doc/2/parallel/parallel_intro.html#architecture-overview)).\n",
    "\n",
    "<br/>\n",
    "<center><img width=\"50%\" src=\"images/architecture.png\"></center>\n",
    "\n",
    "Each of the blocks explained below:\n",
    "\n",
    "+ **Each engine is a Python instance**, usually a Python interpreter, that receives commands through a network connection. When multiple engines are started, parallel and distributed computing becomes possible.\n",
    "+ The **client is a Python object** created at an IPython interpreter or a notebook. This object will allow us to send commands to the Python engines.\n",
    "+ The scheduler is an application that distributes the commands to the engines. We will see that there are two ways of distributing this work: the direct view and the load balanced view.\n",
    "\n",
    "Take into account that\n",
    "\n",
    "+ Each engine is an independent instance of a Python interpreter. All the variables declared at, e.g. engine 1 are not visible at the remaining engines. In a similar way, if we want to work with `numpy` functions we should import this package at all engines.\n",
    "+ As commented before, we won't be able to control on which CPU each engine is executed. What we can do is to send a different job to each engine and the operating system will execute each python instance (with its job) on a different CPU. The number of engines should be thus at most the number of available CPUs. In Python there are, however, some details that should be taken into account in order to maximize efficiency.\n",
    "  \n",
    "\n",
    "This lecture will assume that the client and the engines all run on the same computer. In **Big Data II** lecture we've focused on a distributed architecture, that is, an architecture in which engines, schedulers and clients may run on different computers, interconnected by a network interface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "The first thing to do is to start the cluster. There are two ways for doing it:\n",
    "\n",
    "### From the notebook interface\n",
    "\n",
    "This is the simplest way of proceeding and is the recommended way for newbies in this topic. Within the IPython notebook, you can use the Clusters tab of the dashboard, and press Start with the desired number of cores, under the desired profile. This will automatically run the correct commands to start your IPython cluster.\n",
    "\n",
    "The notebook will be used as interface to the cluster, that is, we will be able to send diverse tasks to the engines.\n",
    "\n",
    "### From the command-line\n",
    "\n",
    "On the command-line, you can run the following command to start an IPython cluster:\n",
    "\n",
    "    $ ipcluster start\n",
    "\n",
    "The latter command will create a cluster with N engines, where N equals the N umber of cores. If you want to create a cluster with a different number of engines just run\n",
    "\n",
    "    $ ipcluster start -n 4\n",
    "    \n",
    "With the latter command we start a cluster with 4 engines. \n",
    "\n",
    "In order to send tasks to the latter cluster you just need to run an ipython interpreter.\n",
    "\n",
    "    $ ipython\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the cluster (the engines)\n",
    "\n",
    "The last section has shown us how to initialize the cluster. No matter which is the way you have selected, the following commands allows you to connect to it. These commands should be either introduced through the notebook (as done here) or typed into the ipthon command line interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as parallel\n",
    "engines = parallel.Client()\n",
    "engines.block = True\n",
    "print(engines.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous command is executed by the notebook and outputs the number of engines in the cluster. If an error is shown when running the commands, the cluster has not been correctly created. We will explain later on the meaning of the **block** attribute.\n",
    "\n",
    "The variable **engines** is an object that represents the available engines to which commands can be sent. Let us see now two different ways by which we may send tasks to the engines: the first, called **direct view**, is the simplest one and allows the user to directly control which tasks are sent to which engines; the second, called **load balanced view**, delegates in the controller to which engines each task is sent. \n",
    "\n",
    "As will be seen later, the first one is useful if a task can be computationally evenly distributed in smaller tasks wheras the second is more useful if such subdivison cannot be easily done. For instance, if we have\n",
    "to analyze multiple data files the direct view is a good approach\n",
    "if all files have approximately the same size. But if files differ\n",
    "(quite a lot) in size the load balanced view is a better approach.\n",
    "Let us now see both approaches.\n",
    "\n",
    "\n",
    "\n",
    "We will now see both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct view of engines\n",
    "\n",
    "#### Calling Python functions\n",
    "\n",
    "You may run any command as usual in the notebook (or IPython command-line interpreter). Note that we currently do not take advantage of the multiple engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 10\n",
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we do to send a command to the cluster? For the moment we just show the principles and then will go into details. Recall that the engines variable just defined represents the engines of the cluster. Within the direct view, `engines[0]` respresents the first engine, `engines[1]` the second engine, and so on. The next commands sends commands to the first engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines[0].execute('a = 2')\n",
    "engines[0].execute('b = 10')\n",
    "engines[0].execute('c = a + b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter command executes the same commands as before but on the first engine. We can retrieve the result by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines[0].pull('c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we do not have direct access to the command line of the\n",
    "first engine. Rather, we may send commands to it through the client.\n",
    "\n",
    "What about parallelization? Let us try the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines[0].execute('a = 2')\n",
    "engines[0].execute('b = 10')\n",
    "engines[1].execute('a = 9')\n",
    "engines[1].execute('b = 7')\n",
    "engines[0:2].execute('c = a + b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous commands initialize different values for a and b at engines\n",
    "0 and 1 and executes the sum at both engines. Since each engine is\n",
    "an independent process, the operating system may schedule each engine\n",
    "in different cores and thus execution is performed in parallel. Again,\n",
    "as before, we can retrieve both results using the pull command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines[0:2].pull('c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous commands we are directly accessing the engines and therefore this type of approach is direct view. We may use Python commands to simplify a bit the previous commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview2 = engines[0:2]                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable 'dview2' references the first two engines. In order to access both engines at the same time we may just write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview2.execute('d = 2 * a + b')\n",
    "dview2.pull('d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will use the `dview2` variable to simultaneously execute commands on engines 0 and 1. \n",
    "\n",
    "In order to define a variable that refers to all the engines we may just write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = engines.direct_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter variable allows to send jobs to all the engines at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now show that we are really doing computations in parallel. Let us try with something bigger! Two matrix multiplication, for instance. We begin by doing serialized computations on the notebook and compute the total processing time. We fill perform the product manually, since these type of operations are usually performed in the gradient descent of the factorization in the recommenders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "\n",
    "size = 300\n",
    "\n",
    "# Create two size x size matrix\n",
    "A = numpy.random.rand(size,size)\n",
    "B = numpy.random.rand(size,size)\n",
    "\n",
    "# Resulting matrix\n",
    "C = numpy.zeros(shape=(size, size))\n",
    "\n",
    "print(\"We begin computations\")\n",
    "t0 = time.time() \n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        result = 0\n",
    "        for r in range(size):\n",
    "            result += A[i][r] * B[r][j]\n",
    "        C[i][j] = result\n",
    "    \n",
    "print(\"Time in seconds: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will do computations in parallel. The idea is to do use two cores, and for this issue we will do half of the computations at each of the cores. On the first core the variable i will run from 0 to size/2, and on the other core it will run from size/2 to size. \n",
    "\n",
    "For this we write the next code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(A, B, size, i0, i1):\n",
    "    import numpy\n",
    "    C = numpy.zeros(shape=(i1-i0,size))\n",
    "    for i in range(i0,i1):\n",
    "        for j in range(size):\n",
    "            result = 0\n",
    "            for r in range(size):\n",
    "                result += A[i][r] * B[r][j]\n",
    "            C[i-i0][j] = result\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `mul` is defined locally but it will be run on each of both engines. We need to execute `import numpy` on the function so to ensure that that the scientific computing library becomes available on each engine. By means of the `apply` function we may remotely execute commands that are defined locally. For instance, we may run matrix in the two cores as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 300\n",
    "half = 150\n",
    "\n",
    "# This is the resulting matrix\n",
    "C = numpy.zeros(shape=(size, size))\n",
    "\n",
    "print(\"We begin computations\")\n",
    "t0 = time.time()\n",
    "\n",
    "# We perform the products on each of the engines\n",
    "[C0, C1] = dview2.map(mul, [A, A], [B, B], [size, size], [0, half], [half, size])\n",
    "\n",
    "# And now construct matrix C\n",
    "C[0:half,:] = C0\n",
    "C[half:size,:] = C1\n",
    "\n",
    "print(\"Time in seconds: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen the total computing time has decreased thanks to the divison of the computation in multiple (two) tasks. We would like to comment that\n",
    "+ The function `mul` is defined locally but has been executed remotely on engine 0 and engine 1 via the `map` function: technically, this is called a **remote call**. In this example engine 0 executes `mul(A, B, size, 0, half)` whereas engine 1 executes `mul(A, B, size, half, size)`.\n",
    "+ The `map` function does not return (technically, it blocks) until the engines are done with their computations. This is due to the fact that we have set `engines.block` to `True` at the beginning. We will go on this issue later on again.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "For those that have 4 (or more cores), can you rewrite the code to that 4 of your cores are used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the task `mul(A0,B0)` is executed on one engine and `mul(A1, B1)` is executed on another one. Which command is executed on each engine? What happens if the list of arguments to map includes three or more matrices? Let us see it with the following example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines[0].execute('my_id = \"engineA\"')\n",
    "engines[1].execute('my_id = \"engineB\"')\n",
    "\n",
    "def sleep_and_return_id(sec):\n",
    "    import time\n",
    "    time.sleep(sec)                 \n",
    "    return my_id,sec\n",
    "\n",
    "dview2.map(sleep_and_return_id, [3,3,3,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the returned result indicates us which engine executed the function. In other words, it shows us how tasks are distributed among the engines. You may repeat this experment as many times as you wish, but the result will always be the same. The tasks are distributed in a uniform way among the\n",
    "engines before executing them no matter which is the delay we pass\n",
    "as argument to the function `sleep_and_return_id`. This is in fact a characteristic of the direct view interface: **the tasks are distributed among the engines before executing them**. \n",
    "\n",
    "For the proposed exeample, is this efficient? No, for sure! The total amount of time you'll have to wait is 9 seconds since the `map` call is blocking until all engines have finished.  \n",
    "\n",
    "As we have commented before the direct view is a good way of proceeding if you expect each task to take the same amount of time. But if not, use the load balanced view as we will see shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total computation time using the cluster may be higher than the local execution time. That may happen! Note that the local computation includes only the matrix product. On the other hand, the total computation time including the cluster includes the following procedures: scatter, push, matrix product, and gather. Parallelization may not necessarily improve the overall performance since we had to include (with respect to the local computation) new procedures that were previously not necessary.\n",
    "\n",
    "#### Blocking and non-blocking commands\n",
    "\n",
    "Let us now focus on the blocking issue. Recall that at the beginning we have set `engines.block = True`. Then, by default, all the calls to functions such as `execute`, `map`, `push` or `pull` block and do not return until all engines are finished with their execution. We have seen this with the previous examples. \n",
    "\n",
    "We may change the default behaviour and make the calls non-blocking. Let us see it with the matrix multiplication example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview2.block = False\n",
    "async = dview2.map(mul, [A, A], [B, B], [size, size], [0, half], [half, size])\n",
    "print(\"Hey, it's me again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `dview2.block` to `False` the `map` call just sends the request to the engines and returns immediately. \n",
    "\n",
    "When running `map` in non-blockig mode, it returns an `Asyncresult` object (the same is valid for the other functions we  have seen). The returned object can be used to request if computations have finished. For instace, for the matrix multiplication performed previously we can check if the result is already avaiable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async.ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The member function `ready` is a non-blocking function (i.e. it will return immediately) that will return `False` if result is not ready, and `True` otherwise.\n",
    "\n",
    "We may use the `get` member to retrieve the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[C0, C1] = async.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter function will **block** until the result is available, no matter if the result is available or not when calling this function. Thus, from a practical point of view, the following blocking code \n",
    "```python\n",
    "[C0, C1] = dview2.map(mul, [A, A], [B, B], [size, size], [0, half], [half, size], block = True)\n",
    "```\n",
    "is equivalent to\n",
    "```python\n",
    "async = dview2.map(mul, [A, A], [B, B], [size, size], [0, half], [half, size], block = False)\n",
    "[C0, C1] = async.get()\n",
    "```\n",
    "Which of both behaviours is better, if any, blocking or non-blocking? If you want to obtain good efficiency, we recommend to use the non-blocking mode since it allows the client to perform other tasks while the engines are performing computations, e.g. retrieve data from internet or disc for the next tasks the engines will have to perform. If the task the client performs is computationally intensive you may need to \"reserve\" one core for this issue. That is, rather than having one engine running on each CPU the number of engines should be one minus the number of CPUs. In general, it is better that the number of (computationally intensive) tasks equals the number of CPUs. Otherwise, the operating system will be overwhelmed trying to plan the execution of more tasks than CPUs. If, however, the task the client performs is computationally low intenstive there should be no problem in having all tasks running at once. You should select one or other method depending on the performance of your experimental results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "As we have seen, parallelization can be used to reduce execution time. If sequential time execution takes T seconds, parallelization with the presented technique allows to reduce execution time up to T/N secons, where N is the number of cores. Take into account that\n",
    "\n",
    "+ It is not recommended to use parallelization until you are sure that the algorithm you have implemented works properly. \n",
    "+ Parallization usually implies modifying the original sequential code. Some new functions may be needed, or data to process may be manually divided into several parts in order to be processed by each of the engines. Obtaining a good performance improvement may not be an easy task. \n",
    "\n",
    "\n",
    "## Load balanced view of the clients\n",
    "\n",
    "The load balanced view is an interface that allows, as the direct view interface, parallelizing tasks. With this interface, however, the user have no direct access to individual engines. It is the IPython scheduler that assigns work to each client. This interface is simultaneously simpler and more powerful.\n",
    "\n",
    "Let us first recall the way the interpreter connects to the clients using a **Direct View**:\n",
    "```python\n",
    "from IPython import parallel\n",
    "engines = parallel.Client()\n",
    "engines.block = True       \n",
    "engines = clients.direct_view()\n",
    "```\n",
    "\n",
    "To create a **Load Balanced View** we will use the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines.block = True\n",
    "lview2 = engines.load_balanced_view(targets=[0,1])    # Engines 1 and 2\n",
    "lview = engines.load_balanced_view()                  # All engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have created a load balanced view in blocking mode. \n",
    "\n",
    "Our first example wil be centered in the `sleep_and_return_id` function we have shown before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engines[0].execute('my_id = \"engineA\"')\n",
    "engines[1].execute('my_id = \"engineB\"')\n",
    "\n",
    "def sleep_and_return_id(sec):\n",
    "    import time\n",
    "    time.sleep(sec)                 \n",
    "    return my_id,sec\n",
    "\n",
    "lview2.map(sleep_and_return_id, [3,3,3,1,1,1])   # Experiment and change these values! For example: [10,1,2,2,2,2,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use a LoadBalanced scheduler! We can see that by default, LoadBalanced View scheduler assigns a new task to an engine when it becomes free. Changing this behavior is not the scope of this lesson but this <a href=\"http://ipython.org/ipython-doc/stable/parallel/parallel_task.html#schedulers\">link</a> can give you more details.\n",
    "\n",
    "We will now see a more complete example to see the usefullness of this kind of behaviour. We will count the words of a set of files! For that issue we will see first the execution in a non-parallel computation, using the direct view, and in the non-balanced view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "cnt_total = Counter()\n",
    "\n",
    "# This is a remote function\n",
    "def count_words(file):\n",
    "    cnt = Counter()\n",
    "    words = re.findall(r'\\w+', open(file).read().lower())\n",
    "    for word in words:\n",
    "        cnt[word] += 1\n",
    "    return cnt\n",
    "\n",
    "# We now read the file that contains the words        \n",
    "with open(\"base_dades/llista_ordenada_120.cfg\", \"r\") as myfile:\n",
    "  data=myfile.read().split() \n",
    "\n",
    "print(\"We begin computations\")\n",
    "t0 = time.time()\n",
    "\n",
    "for file in data:\n",
    "    cnt_total += count_words(file)\n",
    "\n",
    "print(\"Time in seconds: \", time.time() - t0)\n",
    "\n",
    "cnt_total.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the implementation using the direct view. Observe how the counter is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "dview2.push(dict(cnt = cnt))\n",
    "\n",
    "# This is a remote function\n",
    "def count_words(file):\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    words = re.findall(r'\\w+', open(file).read().lower())\n",
    "    for word in words:\n",
    "        cnt[word] += 1\n",
    "\n",
    "# We now read the file that contains the words        \n",
    "with open(\"base_dades/llista_ordenada_120.cfg\", \"r\") as myfile:\n",
    "  data=myfile.read().split()       \n",
    "\n",
    "print(\"We begin computations\")\n",
    "t0 = time.time()\n",
    "\n",
    "dview2.map(count_words, data);\n",
    "\n",
    "[cnt1, cnt2] = dview2.pull('cnt')\n",
    "\n",
    "cnt_total=cnt1+cnt2\n",
    "\n",
    "print(\"Time in seconds: \", time.time() - t0)\n",
    "\n",
    "cnt_total.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how much time it takes to process these files. In this case, files are distributed uniformly among the two engines. But the files have been ordered by size, so computation is not evenvly distributed among the the engines. \n",
    "\n",
    "The code we have seen is the principle of the **MapReduce** programming model: a MapReduce program is composed of a Map() procedure that performs filtering and sorting (such as counting the number of times each word appears in a file) and a Reduce() procedure that performs a summary operation (that is, taking each of the results and computing the overall result).\n",
    "\n",
    "Let us try the load balanced view!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "dview2.push(dict(cnt = cnt))\n",
    "\n",
    "print(\"We begin computations\")\n",
    "t0 = time.time()\n",
    "\n",
    "lview2.map(count_words, data);   # The code changes here!!!!!\n",
    "\n",
    "[cnt1, cnt2] = dview2.pull('cnt')\n",
    "\n",
    "cnt_total=cnt1+cnt2\n",
    "\n",
    "print(\"Time in seconds: \", time.time() - t0)\n",
    "\n",
    "cnt_total.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we have reduced the computation time to process the files. Great! Observe, however, that with the load balanced view we do not know at which engine the task will be executed. We use the direct view to `push` and `pull` the results from the engines.\n",
    "\n",
    "Some general conclusions\n",
    "+ We may use both the direct view and the load blanced view interface for executing commands on engines. Just use the one that fits best your needs.\n",
    "+ With the load balanced view, by default, the Python scheduler assigns work to each engine as soon as they finish with the previous assigned tasks.\n",
    "+ The `map` function returns as soon as all tasks have finished since we are using the blocking mode.\n",
    "\n",
    "You can try now to process all files (`llista_tot.cfg`) using a direct view and a load balanced view. Take into acount that here files are not ordered by size. Thus, the improment you obtain may not be very big. Use all the processors you have for this isssue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example using non-blocking commands: the New York taxi trips database\n",
    "\n",
    "This section presents a real application of the parallel capabilities\n",
    "of IPython and the discussion of several approaches to it. The dataset\n",
    "is a database of taxi trips in New York and it has been obtained through\n",
    "a Freedom of Information Law (FOIL) request from the New York City\n",
    "Taxi \\& Limousine Commission (NYCT\\&L) by University of Illinois at\n",
    "Urbana-Champaign (http://publish.illinois.edu/dbwork/open-data/).\n",
    "The dataset consists in $12\\times2$GBytes Comma Separated Files (CSV)\n",
    "files. Each file has approximately $14$ million entries (lines) and\n",
    "is already cleaned. Thus no special preprocessing is needed to be\n",
    "able to process it. For our purposes we are interested only in the\n",
    "following information from each entry: \n",
    "\n",
    "+ `pickup_datetime`: start time of the trip, mm-dd-yyyy hh24:mm:ss\n",
    "EDT. \n",
    "\n",
    "How many pickups are performed\n",
    "during week days and how many during weekends? And how many pickups\n",
    "are performed in the morning? \n",
    "\n",
    "Implementing the previous classification is rather simple since it\n",
    "only requires checking, for each entry, check the pickup datetime. Performing this task in\n",
    "a sequential may take a rather large amount of time since the number\n",
    "of entries, for a single CSV file, is rather large. In addition, special\n",
    "care has to be taken when reading the file since a 2GByte file may\n",
    "not fully fit into the computer's memory. \n",
    "\n",
    "We may take advantage of the parallelization capabilities in order\n",
    "to reduce the processing time. The idea is to divide the input data\n",
    "into chunks so that each engine takes care of classifying the entries\n",
    "of their corresponding chunks (i.e. Data decomposition). We propose here an approach which \n",
    "is based on implementing a producer-consumer paradigm\n",
    "in order to distribute the tasks. The producer, associated to the\n",
    "client, reads the chunks from disc and distributes them among the\n",
    "engines using a round robin technique. No explicit `map` function\n",
    "is used in this case. Rather, we simulate the behavior of the `map`\n",
    "function in order to have fine control of the parallel problem. Recall\n",
    "that each engine is an independent process. Since we assign different\n",
    "tasks to each engine, the operating system will try to execute each\n",
    "engine on a different process.\n",
    "\n",
    "### The source code\n",
    "\n",
    "We begin by initializing the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import ipyparallel as parallel\n",
    "from itertools import islice\n",
    "from itertools import cycle\n",
    "\n",
    "#Connect to the Ipython cluster    \n",
    "engines = parallel.Client()\n",
    "\n",
    "# By default we use non-blocking feature\n",
    "engines.block = False\n",
    "\n",
    "#Create a DirectView to all engines\n",
    "dview = engines.direct_view()\n",
    "\n",
    "print(\"The number of engines in the cluster is: \" + str(len(engines.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next declare the functions that will be executed on the engines. We do this thanks to the `%%px` parallel magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# The %%px magic executes the code of this cell on each engine.\n",
    "\n",
    "# Define instructions to be executed on each engine.\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "def is_morning(d):\n",
    "    \"Given a datetime, returns if it was on morning or not\"\n",
    "    h = datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\").hour\n",
    "    if (0 <= h and h < 12):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_weekend(d):\n",
    "    \"Given a datetime, returns if it was on weekend or not\"\n",
    "    wday = datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\").weekday() #strptime transforms str to date\n",
    "    if (4 < wday <= 6):\n",
    "       return 0\n",
    "    else:\n",
    "       return 1\n",
    "\n",
    "# Function that given a dictionary (data), applies classify function on each element\n",
    "# and returns an histogram in a Counter object\n",
    "def process(b):\n",
    "    #Recives a block (list of strings) and updates result in global var local_total()\n",
    "    global local_total\n",
    "    \n",
    "    #Create an empty df. Preallocate the space we need by providing the index (number of rows)\n",
    "    df = pd.DataFrame(index=np.arange(0,len(b)), columns=['datetime', 'is_morning', 'is_weekend'])\n",
    "   \n",
    "    # Data is a list of lines, containing datetime at col 5 and latitude at row 11.\n",
    "    # Allocate in the dataFrame the datetime and latitude and longitude dor each line in data\n",
    "    count = 0\n",
    "    for line in b:\n",
    "        elements = line.split(\",\")\n",
    "        df['datetime'].iloc[count] = elements[5]\n",
    "        count += 1\n",
    "\n",
    "    #Delete NaN values from de DF\n",
    "    df.dropna(thresh=(len(df.columns) - 1), axis=0)\n",
    "   \n",
    "    #Apply classify function to the dataFrame\n",
    "    df['is_morning'] = df['datetime'].apply(is_morning)\n",
    "    df['is_weekend'] = df['datetime'].apply(is_weekend)\n",
    "\n",
    "    cdf = df.drop('datetime', axis=1)\n",
    "\n",
    "    #Increment the global variable local_total\n",
    "    count_morning = cdf['is_morning'].sum()\n",
    "    count_weekend = cdf['is_weekend'].sum()\n",
    "    local_total += Counter({'is_morning': count_morning, 'is_weekend': count_weekend})\n",
    "\n",
    "# Initialization function\n",
    "def init():\n",
    "    #Reset total var\n",
    "    global local_total\n",
    "    local_total = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we show the code executed by the client. The next code performs the next task\n",
    "\n",
    "+ It reads a chunk of `lines_per_block` lines form the file. The chunk is assigned to an engine which performs the classification. The result of the classification is updated on a local variable on each engine. This process is repeated until all chunks have been processed by the engines.\n",
    "+ Once finished, the client retrieves the local variable of each engine and computes the overall result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main code executed on the client\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "t0 = time.time() \n",
    "\n",
    "#File to be processed\n",
    "filename = './head_100000.csv'\n",
    "\n",
    "#Defines a generator that reads lines from a file and serves it in blocks\n",
    "def get_chunk(f,N,first):\n",
    "    #Permit to delete header on first chunk\n",
    "    if first: \n",
    "        line0 = 1\n",
    "    else:\n",
    "        line0 = 0\n",
    "\n",
    "    # Returns blocks of N lines from the file f\n",
    "    while True:\n",
    "        yield list(islice(f, line0, N))\n",
    "\n",
    "# A simple counter to verify execution\n",
    "chunk_n = 0\n",
    "\n",
    "# Number of lines to be sent to each engine at a time. Use carefully!\n",
    "lines_per_block = 2000\n",
    "\n",
    "# Create an emty list of async tasks. One element for each engine\n",
    "async_tasks = [None] * len(engines.ids)\n",
    "\n",
    "# Cycle Object to get an infinite iterator over the list of engines\n",
    "c_engines = cycle(engines.ids)\n",
    "\n",
    "# Initialize each engine. Observe that the execute is performed\n",
    "# in a non-blocking fashion.\n",
    "for i in engines.ids:\n",
    "    async_tasks[i] = engines[i].execute('init()')\n",
    "\n",
    "# The variable to store results\n",
    "global_result = Counter()\n",
    "\n",
    "# Open the file in ReadOnly mode\n",
    "f = open(filename, 'r') #iterable\n",
    "\n",
    "# Keep track of the first chunk to be able to remove the header\n",
    "first_chunk = True\n",
    "\n",
    "# Used to show the progress\n",
    "print('We begin sending data')\n",
    "\n",
    "# While the generator returns new chunk, sent them to the engines\n",
    "while True:\n",
    "    # Read a new chunk from generator\n",
    "    new_chunk = get_chunk(f,lines_per_block,first_chunk).__next__()\n",
    "    if not new_chunk: #if the list is empty, break the loop\n",
    "        break\n",
    "    \n",
    "    #After the first loop, first_chunk is False. \n",
    "    first_chunk = False\n",
    "    \n",
    "    #Decide the engine to be used to classify the new chunk\n",
    "    run_engine = c_engines.__next__()\n",
    "    \n",
    "    # Wait until the engine is ready\n",
    "    while ( not async_tasks[run_engine].ready() ):\n",
    "        print(\"Wait till the engine finalizes its previous task\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #Send data to the assigned engine.\n",
    "    mydict = dict(data = new_chunk)\n",
    "    \n",
    "    # The data is sent to the engine in blocking mode. The push function does not return\n",
    "    # until the engine has received the data. \n",
    "    engines[run_engine].push(mydict,block=True)\n",
    "\n",
    "    # We execute the classification task on the engine. Observe that the task is executed\n",
    "    # in non-blocking mode. Thus the execute function reurns immediately. \n",
    "    async_tasks[run_engine] = engines[run_engine].execute('process(data)')\n",
    "    \n",
    "    # Increase the counter    \n",
    "    chunk_n += 1\n",
    "    \n",
    "    # Update the progress\n",
    "    if chunk_n % 1000 == 0:\n",
    "        print(\"Chunk: \" + str(chunk_n))\n",
    "\n",
    "# Get the results from each engine and accumulate in global_result\n",
    "for engine in engines.ids:\n",
    "    # Be sure that all async tasks are finished\n",
    "    while ( not async_tasks[engine].ready() ):\n",
    "        print(\"Wait till the engine finalizes its last task\")\n",
    "        time.sleep(1)\n",
    "    global_result += engines[engine].pull('local_total', block=True)\n",
    "\n",
    "#Close the file\n",
    "f.close()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Total number of chunks processed: \" + str(chunk_n))\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "print(\"Time in seconds: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several experiments have been performed previously on a i7-4790 CPU with 4 physical cores\n",
    "with HiperThreading and 8Gb of RAM. We have made experiments with\n",
    "different number of engines and different number of lines per block (variable\n",
    "`lines_per_block` in previous subsection). \n",
    "\n",
    "### Number of lines per block\n",
    "\n",
    "We begin with the effect of the number of lines per block. Experiments\n",
    "have been made using 8 engines, that is, the number of processors of the computer. Thus, in our environment there will be a total of 9 processes running:\n",
    "the client (i.e. the producer), which is in charge of reading the CSV file and distributing them\n",
    "among the engines in blocks defined by the variable associated to lines per\n",
    "block, and 8 engines (i.e. the consumers) that will take the blocks of data of the producer and\n",
    "process them. For the experiments only 1 million lines have been processed\n",
    "(one may generate a file with one million lines by using i.e. the Unix command `\"head -n 1000000 file.csv\"`).\n",
    "\n",
    "</br>\n",
    "    <center><img width=\"60%\" src=\"images/1M8cores.png\"></center>\n",
    "    \n",
    "As can be seen, an optimal execution\n",
    "time is located near 2,000 lines per block. With fewer number of lines per\n",
    "block, efficiency is lost because most of the time engines are idle and thus cores\n",
    "also are idle. Recall that the client begins by distributing the chunks among the\n",
    "engines. If the chunk is to short it may happen that engine 0 or 1 have already\n",
    "finished processing their chunk at the moment at which the client sends the chunk\n",
    "to engine 3 or 4, for instance. Chunks should be large enough so that the engine\n",
    "finishes processing its chunk just before the client sends it a new chunk. \n",
    "In addition to the prevoius inconvenient, using small chunks may waste lots of \n",
    "computational time managing short messages between processes. \n",
    "\n",
    "On the other hand, when working with more than 6,000 lines\n",
    "per block, the messages to be passed between processes are too big to be\n",
    "moved quickly. Moreover, an engine may have not finished processing its \n",
    "chunk when the client asks him if it has finished. If the engine has\n",
    "not finished the client will wait for one second and ask again. This\n",
    "waiting time reduces the overall performance.\n",
    "\n",
    "The effect of the previous waiting time is interesting. Tests can be done to show that with a\n",
    "lower waiting time the optimal lines per block value is reduced. Nevertheless,\n",
    "optimal execution time does not change because the optimal execution time\n",
    "is based on not having idle cores.\n",
    "\n",
    "### Number of engines\n",
    "\n",
    "The number of engines is associated to the level of parallelization that the\n",
    "code can reach. We have tested our algorithm using 2000 lines per block and\n",
    "different number of engines using again a reduced version of one CSV file. In\n",
    "this case 100,000 lines have been processed. \n",
    "\n",
    "</br>\n",
    "    <center><img width=\"60%\" src=\"images/parallelization.png\"></center>\n",
    "    \n",
    "Once the minimum\n",
    "is reached (in this case for 8 cores) there aren’t any benefits on parallelizing\n",
    "the job with more engines; on the contrary, with more processes, the operating system scheduler is going to spend more time managing processes so the\n",
    "execution time may downgrade. That is, the operating system scheduler may\n",
    "become a bottleneck.\n",
    "\n",
    "### Overall results\n",
    "\n",
    "With this optimal value of 2,000 for lines per block variable we have executed\n",
    "our algorithm over a whole CSV file made up of 14.7 million lines. Execution\n",
    "time with 8 engines is 1009 seconds (17 minutes) and, with 4 engines, time increases to\n",
    "1895 seconds (32 minutes).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
