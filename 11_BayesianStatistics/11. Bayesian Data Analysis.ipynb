{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The \"formula\"\n",
        "\n",
        "Bayes' formula is an important method for computing **conditional probabilities**. It is often used to compute **posterior probabilities** (as opposed to prior probabilities) given observations. For example, a patient is observed to have a certain symptom ($E$), and Bayes' formula can be used to compute the probability that a diagnosis ($H$) is correct, given that observation. \n",
        "\n",
        "$$ P(H | E) = \\frac{P(E | H)P(H)}{P(E)}$$\n",
        "\n",
        "Bayesian statistics **interprets probabilities as measures of believability** (how confident we are) in an event, **not as the long-run frequency of events**.\n",
        "\n",
        "Beliefs mesaures are applied to individuals, not to nature, so there is room for conflicting belives among individuals. Different beliefs are not intepreted as errors but as **different states of knowledge about an event**. \n",
        "\n",
        "The formula is interpreted as an **updating of belief after observing data**.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subjectivity\n",
        "\n",
        "> Bayesian methods are often characterized as “subjective” because the user must choose a prior distribution, that is, a mathematical expression of prior information. \n",
        "\n",
        "> The prior distribution requires information and user input, that’s for sure, but I don’t see this as being any more “subjective” than other aspects of a statistical procedure, such as the choice of model for the data (for example, logistic regression) or the choice of which variables to include in a prediction, the choice of which coefficients should vary over time or across situations, the choice of statistical test, and so forth. \n",
        "\n",
        "> Indeed, Bayesian methods can in many ways be more “objective” than conventional approaches in that Bayesian inference, with its smoothing and partial pooling, is well adapted to including diverse sources of information and thus can reduce the number of data coding or data exclusion choice points in an analysis. \n",
        "\n> *Andrew Gelman, Professor of statistics and political science and director of the Applied Statistics Center at Columbia University*. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Data Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilities: Bayesianism.\n",
        "\n\n",
        "Let's now imagine the following scenario. We want to know the *probability* of getting the Oscar for the last film an actor has starred. \n",
        "\n",
        "In this case, the *frequentist* notion of **series of trials** is not well defined: every year the situation is different, there are no series of identical trials to consider. We can conclude that the classical notion of probability does not apply to this situations.\n",
        "\n",
        "But **Bayesianism** defines probability in a different way: **the degree of belief that an event will occur**.\n",
        "\nWhat is the probability that John Lennon is dead? For a frequentist there is no probability for this event because there are no possible trials (Bin Laden is dead or is not, it's not a question of probability). A Bayesianist would assign a probability to this event based on her **state of knowledge**. The state of knowledge changes when new information is available. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Bayes rule\n",
        "\nThe main tool of Bayesian Analysis is the Bayes theorem, presented in 1763:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Bayes Theorem </b> <br>\n",
        "$$ P(A | B) = \\frac{P(B | A)P(A)}{P(B)}$$\n",
        "</div>   \n",
        "\n\n",
        "This theorem describes the relationship between the conditional probabilities of two events.\n",
        "\n",
        "It is easy to show that this is true. It's only basic arithmetic based on probability rules:\n",
        "\n",
        "+ We know that $ P(A \\mbox{ and } B) = P(A)P(B | A) $.\n",
        "+ But it also true that $ P(A \\mbox{ and } B) = P(B)P(A | B)$.\n",
        "+ So, $ P(A)P(B | A) = P(B)P(A | B)$. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although this is called Bayes’ theorem, the **general form** of it as stated here was actually first written down not by Thomas Bayes, but by Pierre-Simon Laplace. \n",
        "\nWhat Bayes did was derive the special case of this formula for “inverting” the binomial distribution. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypotheses and evidences.\n",
        "\n",
        "The most common interpretation of Bayes's Theorem is based in considering that $A$ is an hypothesis $H$ and $B$ a new evidence $E$ that should modify our belief in $H$:\n",
        "\n",
        "$$P(H | E) = P(H) \\frac{P(E|H)}{P(E)}$$\n",
        "\n",
        "This is called the **diachronic interpretation** because it describes how *an hypothesis must be updated over time every time a new evidence is found*. \n",
        "\n\n",
        "+ $P(H | E)$ is called the **posterior**.\n",
        "+ $P(H)$ is called the **prior probability** of the hypothesis.\n",
        "+ $P(E | H)$ is called the **likelihood** of the evidence.\n",
        "+ $P(E)$ is a normalizing constant. If there are $n$ hypotheses that are *mutually exclusive* and *collectivelly exahustive*, we can compute $P(E)$ as:\n",
        "\n",
        "$$ P(E) = P(H_1)P(E|H_1) + \\dots + P(H_n)P(E|H_n)$$\n",
        "\n\nIn general, $P(H | E), P(H), P(E|H), P(E)$ are functions! We can extract point estimates, set estimates and probabilistic propositions from $P(H | E)$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of the Bayes Rule<br>\n",
        "\n",
        "Say $P(H_{yes})=5\\%$ is the prevalence of a disease (% of red dots on top fig). \n",
        "\n",
        "Each individual is given a test with accuracy $P(E_{yes}|H_{yes})=P(E_{no}|H_{no}) = 90\\%$.  \n",
        "\n",
        "We want to know the **probability of having the disease if you tested positive**: \n",
        "$$Pr(H_{yes}|E_{yes})$$. \n",
        "\n",
        "We can use the Bayes rule to compute this posterior:\n",
        "\n",
        "$$ P(H_{yes}|E_{yes}) = \\frac{P(H_{yes}) P(E_{yes}|H_{yes}) }{P(E_{yes})} = $$\n",
        "\n",
        "$$ = \\frac{P(H_{yes})P(E_{yes}|H_{yes}) }{P(H_{yes})P(E_{yes}|H_{yes}) + P(H_{no})P(E_{yes}|H_{no})} $$\n",
        "\n",
        "That is\n",
        "\n",
        "$$ = \\frac{0.05 \\times 0.9}{0.05 \\times 0.9 + 0.95 \\times 0.1} \\approx 0.32 $$\n",
        "\n",
        "Many find it counterintuitive that this probability is much lower than $90\\%$; this animated gif is meant to help.\n",
        "\nThe `O` in the middle turns into an `X` when the test fails. The rate of `X`s is $1-Pr(E_{yes}|H_{yes})$. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('images/bayes.gif')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Monty Hall Problem\n",
        "\n",
        "> \"*Let's Make a Deal*\" is a television game show which originated in the United States and has since been produced in many countries throughout the world. The show is based around deals offered to members of the audience by the host. The traders usually have to weigh the possibility of an offer for valuable prizes, or undesirable items, referred to as \"Zonks\". \n",
        "\n",
        ">*Source: Wikipedia*.\n",
        "\n",
        "Monty Hall was the original host of the game. The Monty Hall problem is based on one of the regular games of the show. It is a stick or switch problem:\n",
        "> Suppose you're on the game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats.\n",
        "> You pick a door, say Door A (the door is not open), and the host, who knows what's behind the doors, opens Door B, which has a goat.\n",
        "> He then says to you, \"Do you want to pick Door C?\" Is it to your advantage to switch your choice?\n",
        "> *Source: Wikipedia*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "Image('images/lets.jpg')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most people intuitively thinks that it makes no difference to stick or to switch, but this is wrong!\n",
        "\n",
        "The truth is that if you stick the probability of winning is 1/3; if you switch your chances are 2/3.\n",
        "\n",
        "We can use the Bayesian point of view to solve this problem. At the beginning, there are different hypotheses $H$ with their corresponding **prior** probabilities:\n",
        "\n",
        "+ A: the car is behind Door A; $P(H=\\mbox{'A'}) = 1/3$\n",
        "+ B: the car is behind Door B; $P(H=\\mbox{'B'}) = 1/3$\n",
        "+ C: the car is behind Door C; $P(H=\\mbox{'C'}) = 1/3$\n",
        "\n",
        "You choose A randomly. If you stick to A after Monty opens the door B (this a our evidence *E*). We can compute  $P(H=\\mbox{'A'}|E)$:\n",
        "\n",
        "$$ P(H=\\mbox{'A'}|E) = \\frac{P(H=\\mbox{'A'})P(E|H=\\mbox{'A'})}{P(E)} $$\n",
        "$$= \\frac{1/3 \\times 1/2}{1/3 \\times 1/2 + 1/3 \\times 0 + 1/3 \\times 1} = 1/3$$ \n",
        "\nThe denominator can be understood in this way: We are assuming we initially chose A. It follows that if the car is behind A, Monty will show us a goat behind B half the time. If the car is behind B, Monty never shows us a goat behind B. Finally, if the car is behind C, Monty shows us a goat behind B every time. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the probability if we switch?**\n",
        "\n",
        "To know $P(H=\\mbox{'C'}|E)$ you could also apply Bayes's Theorem directly, but there is a simpler way to compute it:  Since the probability that it's behind A is 1/3 and the sum of the two probabilities must equal 1, the probability the car is behind C is 1−1/3=2/3. \n",
        "\n\n",
        "Samuel Arbesman, Wired, 11.26.14: \n",
        "\n> In fact, Paul Erdős, one of the most prolific and foremost mathematicians involved in probability, when initially told of the Monty Hall problem also fell victim to not understanding why opening a door should make any difference. Even when given the mathematical explanation multiple times, he wasn’t really convinced. It took several days before he finally understood the correct solution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a ** simulation ** of the game to compute $P(H=\\mbox{'C'}|E)$:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "iterations = 100000\n",
        "doors = [\"goat\"] * 2 + [\"car\"]\n",
        "change_wins = 0\n",
        "change_loses = 0\n",
        "\n",
        "for i in range(iterations):\n",
        "    random.shuffle(doors)\n",
        "    \n",
        "    # you pick door n:\n",
        "    n = random.randrange(3)\n",
        "    \n",
        "    # monty picks door k, k!=n and doors[k]!=\"car\"\n",
        "    sequence = list(range(3))\n",
        "    random.shuffle(sequence)\n",
        "    for k in sequence:\n",
        "        if k == n or doors[k] == \"car\":\n",
        "            continue\n",
        "    \n",
        "    # now if you change, you lose iff doors[n]==\"car\"\n",
        "    if doors[n] == \"car\":\n",
        "        change_loses += 1\n",
        "    else:\n",
        "        change_wins += 1\n",
        "\n",
        "perc = (100.0 * change_wins) / (change_wins + change_loses)\n",
        "print(\"Switching has %s wins and %s losses: you win %.1f%% of the time\" % (change_wins, change_loses, perc))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise \n",
        "\nCompute $P(H=\\mbox{'C'}|E)$ by applying Bayes' Rule and check the result."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# your solution here\n",
        "\n",
        "P = 0\n",
        "print(P)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "**What is the evidence of a disease after a clinical test?**\n",
        "\n",
        "Data:\n",
        "\n",
        "+ 1% of women at age forty who participate in routine screening have breast cancer. \n",
        "+ 80% of women with breast cancer will get positive mammographies. \n",
        "+ 9.6% of women without breast cancer will also get positive mammographies. \n",
        "\n",
        "A woman in this age group had a positive mammography in a routine screening.\n",
        "\n",
        "- **What is the probability that she actually has breast cancer?**\n",
        "\n",
        "- **Is this a \"good\" test for detecting breast cancer?**\n",
        "\nThis simple puzzle is not all that simple in practice. Only 15% of doctors, when presented with this situation, come up with the correct answer."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data:\n",
        "\n",
        "+ $P(\\mbox{cancer}) = 0.01$ and $P(\\mbox{no cancer}) = 0.99$.\n",
        "+ If you get $+$, then $P(\\mbox{ + } | \\mbox{ cancer}) = 0.8$ and $P(\\mbox{ + } | \\mbox{ no cancer}) = 0.096$.\n",
        "+ If you get $-$, then $P(\\mbox{ - } | \\mbox{ cancer}) = 0.2$ and $P(\\mbox{ - } | \\mbox{ no cancer}) = 0.904$.\n",
        "\n",
        "Let's give an answer to the second question. The answer can be given by comparing $P(\\mbox{cancer }|\\mbox{ + })$ and $P(\\mbox{no cancer }|\\mbox{ + })$:\n",
        "\n",
        "$$ P(\\mbox{cancer }|\\mbox{ + }) \\propto P(\\mbox{cancer}) P(\\mbox{ + } | \\mbox{ cancer}) = 0.008$$\n",
        "\n",
        "$$ P(\\mbox{no cancer }|\\mbox{ + }) \\propto P(\\mbox{no cancer}) P(\\mbox{ + } | \\mbox{ no cancer}) = 0.09504$$\n",
        "\nObviously, this is not a good test!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "** What is the probability that she actually has breast cancer? ** Remember that:\n",
        "\n$$P(E) = \\sum_i P(H_i) p (E | H_i)$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "Let's suppose that the result of a second test for a given woman *is independent* from the first one (this is clearly a wrong assumption in the real world, but let's pretend it is true!). **What is the probability of cancer after a positive in a second test?**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Your solution here.\n",
        "\n",
        "P2 = 0\n",
        "print(P2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Estimation\n",
        "\n\n",
        "**The locomotive problem.**\n",
        "\n",
        "A railroad company numbers its locomotives in order $1 \\dots N$. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad company has.\n",
        "\n",
        "> During World War II, the *Economic Warfare Division* of the American Embassy in London used statistical analysis to estimate German production of tanks and other equipment.\n",
        "The Western Allies had captured log books, inventories, and repair records that included chassis and engine serial numbers for individual tanks.\n",
        "\n",
        "> Analysis of these records indicated that serial numbers were allocated by manufacturer and tank type in blocks of 100 numbers, that numbers in each block were used sequentially, and that not all numbers in each block were used. So the problem of estimating German tank production could be reduced, within each block of 100 numbers, to a form of the locomotive problem.\n",
        "\n",
        "> Based on this insight, American and British analysts produced estimates substantially lower than estimates from other forms of intelligence. And after the war, records indicated that they were substantially more accurate.\n",
        "\n",
        "> *(Source: http://greenteapress.com/thinkbayes/html/thinkbayes004.html#toc24)*\n",
        "\n",
        "Based on the observation, we know the railroad has 60 or more locomotives. But how many more? \n",
        "\n",
        "To apply Bayesian reasoning, we can break this problem into two steps:\n",
        "\n",
        "+ What did we know about $N$ before we saw the data?\n",
        "+ For any given value of $N$, what is the likelihood of seeing the data (a locomotive with number 60)?\n",
        "\n",
        "The answer to the first question is the **prior**, $P(H)$. The answer to the second is the **likelihood**, $P(E|H)$.\n",
        "\nWe don’t have much basis to choose a prior, but we can start with something simple and then consider alternatives. Let’s assume that $N$ can be any value from 1 to 1000 and a flat prior."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "hypos = np.array(range(1, 1001))\n",
        "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(10,2))\n",
        "    plt.plot(hypos, priors)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all we need is a **likelihood function**, $P(E|H)$. \n",
        "\n",
        "In a hypothetical fleet of $N$ locomotives, what is the probability that we would see number 60? \n",
        "\nIf we assume that there is only one train-operating company (or only one we care about) and that we are equally likely to see any of its locomotives, then the chance of seeing any particular locomotive is $1/N$ and that there are at least $N$ locomotives."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def Likelihood(data, hypo):\n",
        "    if hypo < data:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return 1.0/hypo"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plug our data into the model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def Posterior(data, hypos, priors):\n",
        "    import numpy as np\n",
        "    posterior = np.array([Likelihood(data, hypo) for hypo in hypos]) * priors\n",
        "    return posterior\n",
        "\n",
        "# After an update, the distribution is no longer normalized, \n",
        "# but because these hypotheses are mutually exclusive and \n",
        "# collectively exhaustive, we can renormalize.\n",
        "\n",
        "def Normalize(d):\n",
        "    total = d.sum()\n",
        "    factor = 1.0 / total\n",
        "    for i in range(len(d)):\n",
        "        d[i] *= factor\n",
        "    return d\n",
        "\n",
        "posterior = Normalize(Posterior(60, hypos, priors))\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(10,2))\n",
        "    plt.ylim([-0.001,0.006])\n",
        "    plt.plot(hypos, posterior)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most likely value, if you had to guess, is 60. That might not seem like a very good guess; after all, what are the chances that you just happened to see the train with the highest number?\n",
        "\n",
        "Nevertheless, if you want to maximize the chance of getting the answer exactly right, you should guess 60.\n",
        "\nBut maybe that’s not the right goal. An alternative is to compute the **hypothesys that corresponds to the mean value of the posterior distribution**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def Meanp(hypos, posterior):\n",
        "    total = 0.0\n",
        "    s = hypos * posterior\n",
        "    return s.mean()*len(hypos)\n",
        "\nprint(int(Meanp(hypos, posterior)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to increse our knowledge, there are two ways to proceed:\n",
        "\n",
        "+ Get more data.\n",
        "+ Get more background information.\n",
        "\nFor example, suppose that in addition to train 60 we also see trains 30 and 90. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hypos = range(1, 1001)\n",
        "posterior =  Normalize(Posterior(60, hypos, priors))\n",
        "posterior2 = Normalize(Posterior(30, hypos, posterior))\n",
        "posterior3 = Normalize(Posterior(90, hypos, posterior2))\n",
        "\n",
        "print(int(Meanp(hypos, posterior3)*len(hypos)))\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(10,2))\n",
        "    plt.ylim([-0.001,0.025])\n",
        "    plt.plot(hypos, posterior3)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can refactor our functions in the following way:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def Normalize(d):\n",
        "    total = d.sum()\n",
        "    factor = 1.0 / total\n",
        "    for i in range(len(d)):\n",
        "        d[i] *= factor\n",
        "    return d\n",
        "\n",
        "def Likelihood1(datum, hypo):\n",
        "    if hypo < datum:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return 1.0/hypo\n",
        "    \n",
        "def Posterior(datum, hypos, priors, likelihood):\n",
        "    import numpy as np\n",
        "    posterior = np.array([likelihood(datum, hypo) for hypo in hypos]) * priors\n",
        "    return posterior\n",
        "\n",
        "def Posterior_n(data, hypos, priors, likelihood):\n",
        "    p = priors\n",
        "    for d in data:\n",
        "        posterior =  Normalize(Posterior(d, hypos, p, likelihood))\n",
        "        p = posterior\n",
        "    return posterior\n",
        "\n",
        "def Meanp(hypos, posterior):\n",
        "    total = 0.0\n",
        "    s = hypos * posterior\n",
        "    return s.mean()*len(hypos)\n",
        "\n",
        "hypos = np.array(range(1, 1001))\n",
        "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
        "posteriors = Posterior_n([60,30,90], hypos, priors, Likelihood1)\n",
        "\n",
        "print('Mean of the posterior distribution with 1000 hypotheses: ', \\\n",
        "        int(Meanp(hypos, posteriors)))\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(10,2))\n",
        "    plt.ylim([-0.001,0.025])\n",
        "    plt.plot(hypos, posteriors)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "With more data, **posterior distributions based on different priors tend to converge**. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hypos = np.array(range(1, 501))\n",
        "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
        "posteriors = Posterior_n([60,60,90], hypos, priors, Likelihood1)\n",
        "print('Mean of the posterior distribution with 500 hypotheses: ', \n",
        "      int(Meanp(hypos, posteriors)))\n",
        "\n",
        "hypos = np.array(range(1, 2001))\n",
        "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
        "posteriors = Posterior_n([60,60,90], hypos, priors, Likelihood1)\n",
        "print('Mean of the posterior distribution with 2000 hypotheses: ', \n",
        "      int(Meanp(hypos, posteriors)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If more data are not available, another option is to improve the priors by gathering more background information. It is probably not reasonable to assume that a train-operating company with 1000 locomotives is just as likely as a company with only 1.\n",
        "\n",
        "Let's suppose that the distribution of the total number of locomotives of a company tends to follow a **power law**. \n",
        "\n",
        "This law suggests that if there are 1000 companies with fewer than 10 locomotives, there might be 100 companies with 100 locomotives, 10 companies with 1000, and possibly one company with 10,000 locomotives.\n",
        "\n",
        "Mathematically, a power law means that the number of companies with a given size is inversely proportional to size:\n",
        "\n",
        "$$ f(x) = \\left( \\frac{1}{x} \\right)^\\alpha $$\n",
        "\nwhere $f(x)$ is the probability mass function of $x$ and $\\alpha$ is a parameter (that is often near 1)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def Likelihood2(data, hypo, alpha=1.0):\n",
        "    if hypo < data:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return hypo**(-alpha)\n",
        "\n",
        "alpha = 1.0\n",
        "hypos = np.array(range(1, 1001))\n",
        "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
        "\n",
        "print(int(Meanp(hypos, priors)*len(hypos)))\n",
        "    \n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(10,2))\n",
        "    plt.ylim([-0.01,0.14])\n",
        "    plt.xlim([-10,500])\n",
        "    plt.plot(hypos, priors)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 1.0\n",
        "\n",
        "hypos = np.array(range(1, 1001))\n",
        "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
        "\n",
        "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)\n",
        "\n",
        "print('Mean of the posterior distribution with 1000 hypotheses: ', int(Meanp(hypos, posteriors)))\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(10,2))\n",
        "    plt.ylim([-0.001,0.035])\n",
        "    plt.plot(hypos, posteriors)\n",
        "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 0.9, \\\n",
        "                linewidth=1, color='r')\n",
        "\nplt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hypos = np.array(range(1, 501))\n",
        "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
        "\n",
        "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)\n",
        "\n",
        "print('Mean of the posterior distribution with 500 hypotheses: ',int(Meanp(hypos, posteriors)))\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(6,2))\n",
        "    plt.ylim([-0.001,0.035])\n",
        "    plt.xlim([0,2000])\n",
        "    plt.plot(hypos, posteriors)\n",
        "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 0.9, \\\n",
        "                linewidth=1, color='r')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "hypos = np.array(range(1, 2001))\n",
        "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
        "\n",
        "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)\n",
        "\n",
        "print('Mean of the posterior distribution with 2000 hypotheses: ',int(Meanp(hypos, posteriors)))\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(6,2))\n",
        "    plt.ylim([-0.001,0.035])\n",
        "    plt.plot(hypos, posteriors)\n",
        "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 0.9, \\\n",
        "                linewidth=1, color='r')\n",
        "\nplt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAP and credible intervals\n",
        "\n",
        "Once we have the **posterior distribution** we could be interested in summarizing the result with a **point estimate** or a **credible interval**.\n",
        "\n",
        "For point estimates we can use, the mean, the median or the **mode**. This last estimate is called **maximum a posteriori estimate (MAP)**.\n",
        "\n",
        "For intervals we usually report two values computed so that there is a 95% chance that the unknown value falls between them (or any other probability). These values define a **credible interval**.\n",
        "\nA simple way to compute the 95% credible interval is to add up the probabilities in the posterior distribution and record the values that correspond to probabilities 2.5% and 97.5%. In other words, the 2.5th and 97.5th percentiles."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def Percentile(hypos, posterior, percentage):\n",
        "    import numpy as np\n",
        "    p = percentage / 100.0\n",
        "    cdf = np.cumsum(np.array(posterior))\n",
        "    total = 0\n",
        "    for i in range(len(hypos)):\n",
        "        total += posterior[i]\n",
        "        if total >= p:\n",
        "            return hypos[i] \n",
        "\n",
        "alpha = 1.0\n",
        "\n",
        "hypos = np.array(range(1, 1001))\n",
        "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
        "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)      \n",
        "\n",
        "print('The credible interval is [', Percentile(hypos, posteriors, 2.5),',', \\\n",
        "            Percentile(hypos, posteriors, 97.5), ']')\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    fig = plt.figure(figsize=(10,2))\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.ylim([-0.001,0.035])\n",
        "    plt.plot(hypos, posteriors)\n",
        "    plt.axvspan(Percentile(hypos, posteriors, 2.5), Percentile(hypos, posteriors, 97.5), facecolor='0.5', alpha=0.2)\n",
        "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 1, \\\n",
        "                linewidth=1, color='r')\n",
        "    ax.set_xlabel('The credible interval is [ 90 , 303 ]')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the previous example—the locomotive problem with a power law prior and three trains—the 90% credible interval is (90, 303). The width of this range suggests, correctly, that we are still quite uncertain about how many locomotives there are."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypotheses comparison.\n",
        "\n",
        "From a frequentist point of view, we say that an effect $H_A$ is statistically significant (or not) by computing the chances (likelihood) of the effect under a null hypothesis $P(E|H_0)$, but you can't conclude it is real.\n",
        "\n",
        "From a Bayesian point of view what we directly compute $P(H_A | E)$, where $H_A$ is the hypothesis the effect is real. \n",
        "\n",
        "By Bayes's Theorem:\n",
        "\n$$ P(H_A | E) = \\frac{P(E|H_A) P(H_A)}{P(E)} = \\frac{P(E|H_A) P(H_A)}{P(E | H_A)P(H_A) + P(E | H_0)P(H_0)} $$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute $P(E | H_A)$, the likelihood term, we can follow a similar approach to the one employed to compute $P(E | H_0)$, by generating 1000 sample pairs, one from each distribution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "file = open('files/2002FemPreg.dat', 'r')\n",
        "\n",
        "def chr_int(a):\n",
        "    if a == '  ':\n",
        "        return 0\n",
        "    else:\n",
        "        return int(a)\n",
        "        \n",
        "preg=[]\n",
        "for line in file:\n",
        "    lst  = [int(line[:12]), int(line[274:276]), int(line[276]), \\\n",
        "                 chr_int(line[277:279]), float(line[422:440])]\n",
        "    preg.append(lst)\n",
        "    \n",
        "\n",
        "df = pd.DataFrame(preg)\n",
        "df.columns = ['caseid', 'prglength', 'outcome', 'birthord', 'finalwgt']\n",
        "\n",
        "#data cleaning\n",
        "df2 = df.drop(df.index[(df.outcome == 1) & (df['prglength'] > df['prglength'].median() + 6)])\n",
        "df2[(df2.outcome == 1) & \n",
        "    (df2['prglength'] > df2['prglength'].median() + 6)]\n",
        "df3 = df2.drop(df2.index[(df2.outcome == 1) &\n",
        "                         (df2['prglength'] < df2['prglength'].median() -10)])\n",
        "df3[(df3.outcome == 1) & \n",
        "    (df3['prglength'] < df3['prglength'].median() - 10)]\n",
        "\n",
        "firstbirth = df3[(df3.outcome == 1) & (df3.birthord == 1)]\n",
        "othersbirth = df3[(df3.outcome == 1) & (df3.birthord >= 2)]\n",
        "\n",
        "x = firstbirth['prglength']\n",
        "y = othersbirth['prglength']\n",
        "p = abs(x.mean() - y.mean())\n",
        "N = 1000\n",
        "diff = list(range(N))\n",
        "for i in range(N):\n",
        "    p1 = [random.choice(x.values) for _ in range(len(x))]\n",
        "    p2 = [random.choice(y.values) for _ in range(len(y))]\n",
        "    diff[i] = abs(np.mean(p1)-np.mean(p2))\n",
        "diff2 = np.array(diff)\n",
        "w1 = np.where(diff2 > p)[0]\n",
        "print('p-value =', len(w1)/float(N), '(', len(w1)/float(N)*100 ,'%)', \n",
        "      'Difference =', p)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the probability of $P(E | H_A)$ is around 0.5. \n",
        "\n",
        "In absence of knowledge, $P(H_A) = P(H_0) = 0.5$. Then, we can compute the posterior probabilities of $H_A$ and $H_0$:\n",
        "\n",
        "$$ P(H_A | E) = \\frac{P(E|H_A) P(H_A)}{P(E)}$$\n",
        "\n",
        "$$ P(H_0 | E) = \\frac{P(E|H_0) P(H_0)}{P(E)}$$\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('P(H_A|E):', 0.5 * 0.5 / (0.5 * 0.5 + 0.02 * 0.5))\n",
        "print('P(H_0|E):', 0.02 * 0.5 / (0.5 * 0.5 + 0.02 * 0.5))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, by taking into account a new evidence we have increased our belief in the effect $H_A$ from 50% to 96%. This is not a decision rule but a change in our knowledge. It makes sense: the evidence supports the hypothesis!\n",
        "\nIn our problem, based on expert knowledge, it could also make sense to consider that $P(H_A) = 0.01$ and $P(H_0) = 0.99$. In this case:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('H_A:', 0.5 * 0.01 / (0.5 * 0.01 + 0.02 * 0.99))\n",
        "print('H_0:', 0.02 * 0.99 / (0.5 * 0.01 + 0.02 * 0.99))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have increased our belief in the effect $H_A$ from 1% to 20%, but we still can believe in $H_0$!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The change strongly depends on our prior belief: <br><br>\n",
        "\n<center><small>Image from: Sellke et al. \"Calibration of ρ values for testing precise null hypotheses.\" Am. Stat. 55.1 (2001): 62-71.</small></center>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('images/p-graphic.jpg')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian analysis of a parameter: the classical way.\n",
        "\n",
        "(from *Bayesian Methods for Hackers, Cam Davidson-Pilon, 2013*)\n",
        "\n",
        "Suppose, naively, that you are unsure about the probability of heads in a coin flip. You believe there is some true underlying ratio, call it $p$, but have no prior opinion on what $p$ might be.\n",
        "\n",
        "We begin to flip a coin, and record the observations: either ``H`` or ``T``. This is our observed data. \n",
        "\n",
        "An interesting question to ask is how our inference changes as we observe more and more data? More specifically, **what do our posterior probabilities look like when we have little data, versus when we have lots of data**?\n",
        "\n\n",
        "> In probability theory and statistics, the **Bernoulli distribution**, named after Swiss scientist Jacob Bernoulli, is the probability distribution of a random variable which takes value 1 with success probability $θ$ and value 0 with failure probability $1-θ$. \n",
        "\n",
        "> The probability mass function $f$ of this distribution is $ \\mbox{ } f(k;p) = \\begin{cases} θ & \\text{if }k=1, \\\\\n",
        "1-θ & \\text {if }k=0.\\end{cases} $\n",
        "\n",
        "> This can also be written as:\n",
        "\n",
        ">$$P(k|θ)=θ^k (1−θ)^{1−k}$$\n",
        "> where $k∈\\{1,0\\}$ and $θ∈[0,1]$.\n",
        "\n> *Source: Wikipedia*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the Bernoulli likelihood function to determine the probability of seeing a particular sequence of $N$ flips, given by the set ${k_1,...,k_N}$.\n",
        "\n",
        "Since each of these flips is independent of any other, the probability of the sequence occuring is simply the product of the probability of each flip occuring.\n",
        "\n",
        "If we have a particular fairness parameter $θ$, then the probability of seeing this particular stream of flips, given $θ$, is given by:\n",
        "\n",
        "$$ P(\\{k_1,...,k_N\\} | θ ) =  \\prod_i P(k_i|θ) = \\prod_i θ^{k_i} (1−θ)^{1−k_i} $$\n",
        "\n",
        "What about if we are interested in the number of heads, say, in $N$ flips? \n",
        "\n",
        "If we denote by $z$ the number of heads appearing, then the formula above becomes:\n",
        "\n",
        "$$ P(z,N|θ) = θ^{z} (1−θ)^{N-z} $$\n",
        "That is, the probability of seeing $z$ heads, in $N$ flips, assuming a fairness parameter $θ$. We will use this formula when we come to determine our posterior belief distribution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayes' Rule for Bayesian Inference**\n",
        "\n",
        "$$P(θ|D)=P(D|θ)P(θ)/P(D)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "+ $P(θ)$ is the prior. This is the strength in our belief of $θ$ without considering the evidence $D$. Our prior view on the probability of how fair the coin is.\n",
        "\n",
        "+ $P(θ|D)$ is the posterior. This is the (refined) strength of our belief of $θ$ once the evidence $D$ has been taken into account. \n",
        "\n",
        "+ $P(D|θ)$ is the likelihood. This is the probability of seeing the data $D$ as generated by a model with parameter $θ$. If we knew the coin was fair, this tells us the probability of seeing a number of heads in a particular number of flips.\n",
        "\n",
        "+ $P(D)$ is the evidence. This is the probability of the data as determined by summing (or integrating) across all possible values of $θ$, weighted by how strongly we believe in those particular values of $θ$. If we had multiple views of what the fairness of the coin is (but didn't know for sure), then this tells us the probability of seeing a certain sequence of flips for all possibilities of our belief in the coin's fairness.\n",
        "\n",
        "We are interested in the probability of the coin coming up heads as a function of the underlying fairness parameter $θ$.\n",
        "\n",
        "Regarding the likelihood: \n",
        "\n",
        "$$ P(D|θ) = P(z,N|θ) = θ^{z} (1−θ)^{N-z} $$\n",
        "\n",
        "Regarding the prior, we are going to choose the beta distribution:\n",
        "\n",
        "$$ P(θ | \\alpha, \\beta)=  \\frac{θ^{\\alpha-1} (1−θ)^{\\beta -1}}{B(\\alpha, \\beta)}  $$ \n",
        "\nwhere the term in the denominator is present to act as a normalising constant so that the area under the PDF actually sums to 1."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important reason for choosing a beta distribution is because it is a **conjugate prior** for the Bernoulli distribution.\n",
        "\n",
        "In Bayes' rule above we can see that the posterior distribution is proportional to the product of the prior distribution and the likelihood function:\n",
        "\n",
        "$$ P(θ|D) \\propto P(D|θ)P(θ) $$\n",
        "\n",
        "A conjugate prior is a choice of prior distribution, that when coupled with a specific type of likelihood function, provides a posterior distribution that is of the same family as the prior distribution.\n",
        "\n",
        "The prior and posterior both have the same probability distribution family, but with differing parameters.\n",
        "\n",
        "If our prior is given by $beta(θ|α,β)$ and we observe $z$ heads in $N$ flips subsequently, then the posterior is given by $beta(θ|z+α,N−z+β)$.\n",
        "\nBelow we plot a sequence of updating posterior probabilities as we observe increasing amounts of data (coin flips)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "from IPython.core.pylabtools import figsize\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import scipy.stats as stats\n",
        "figsize(10, 9)\n",
        "\n",
        "# we use a (continous) prior for p by using the beta function\n",
        "# more about this later...\n",
        "dist = stats.beta\n",
        "\n",
        "n_trials = [0,1,2,3,4,5,8,15, 50, 500]\n",
        "\n",
        "# synthetic data generation\n",
        "# random number generation with a Bernoulli distribution with p=0.5\n",
        "# the probability of this sequence is modelled by the binomial dist\n",
        "data = stats.bernoulli.rvs(0.5, size = n_trials[-1])\n",
        "print(data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0,1,100)\n",
        "\n",
        "for k, N in enumerate(n_trials):\n",
        "    sx = plt.subplot( len(n_trials)/2, 2, k+1)\n",
        "    plt.xlabel(\"$p$, probability of heads\") if k in [0,len(n_trials)-1] else None\n",
        "    plt.setp(sx.get_yticklabels(), visible=False)\n",
        "    \n",
        "    # counting the number of heads\n",
        "    heads = data[:N].sum()\n",
        "    \n",
        "    # in this case the posterior can be analitically computed\n",
        "    # because the conjugate prior of the Binomial distribution\n",
        "    # is a Beta distribution \n",
        "    y = dist.pdf(x, 1 + heads, 1 + N - heads )\n",
        "    \n",
        "    plt.plot( x, y, label= \"observe %d tosses,\\n %d heads\"%(N,heads) )\n",
        "    plt.fill_between( x, 0, y, color=\"#348ABD\", alpha = 0.4 )\n",
        "    plt.vlines( 0.5, 0, 4, color = \"k\", linestyles = \"--\", lw=1 )\n",
        "    leg = plt.legend()\n",
        "    leg.get_frame().set_alpha(0.4)\n",
        "    plt.autoscale(tight = True)\n",
        "\n",
        "plt.suptitle( \"Bayesian updating of posterior probabilities\", y = 1.02, fontsize = 14);\n",
        "plt.tight_layout()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example we have used a prior that was represented with a beta function, a very special function that have allowed us to update the posterior in a very straighforward way! \n",
        "\n",
        "> In probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval $[0, 1]$ parametrized by two positive shape parameters, denoted by $\\alpha$ and $\\beta$, that appear as exponents of the random variable and control the shape of the distribution.\n",
        "\n",
        "> *Source: Wikipedia*\n",
        "\nThe Beta function is very flexible for representing very different priors."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0,1,100)\n",
        "dist = stats.beta\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    y = dist.pdf(x, 1.5, 1.5)\n",
        "    plt.plot(x,y)\n",
        "    y = dist.pdf(x, 3.0, 1.5)\n",
        "    plt.plot(x,y)\n",
        "    y = dist.pdf(x, 3.0, 3.0)\n",
        "    plt.plot(x,y)\n",
        "    y = dist.pdf(x, 3.0, 20.0)\n",
        "    plt.plot(x,y)\n",
        "    y = dist.pdf(x, 20.0, 20.0)\n",
        "    plt.plot(x,y)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reminder: Popular PDF's\n",
        "*(Source: https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers*)\n",
        "\n",
        "Let $Z$ be some random variable. Then associated with $Z$ is a *probability distribution function* that assigns probabilities to the different outcomes $Z$ can take. Graphically, a probability distribution is a curve where the probability of an outcome is proportional to the height of the curve.\n",
        "\n",
        "### Discrete Case\n",
        "\n",
        "If $Z$ is discrete, then its distribution is called a *probability mass function*, which measures the probability $Z$ takes on the value $k$, denoted $P(Z=k)$. \n",
        "\n",
        "There are a lot of popular probability mass functions, but let's introduce the first very useful probability mass function. We say $Z$ is *Poisson*-distributed if:\n",
        "\n",
        "$$P(Z = k) =\\frac{ \\lambda^k e^{-\\lambda} }{k!}, \\; \\; k=0,1,2, \\dots $$\n",
        "\n",
        "It can express the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event.\n",
        "\n$\\lambda$ is called a parameter of the distribution, and it controls the distribution's shape. For the Poisson distribution, $\\lambda$ can be any positive number. By increasing $\\lambda$, we add more probability to larger values, and conversely by decreasing $\\lambda$ we add more probability to smaller values. One can describe $\\lambda$ as the *intensity* of the Poisson distribution. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike $\\lambda$, which can be any positive number, the value $k$ in the above formula must be a non-negative integer, i.e., $k$ must take on values 0,1,2, and so on. This is very important, because if you wanted to model a population you could not make sense of populations with 4.25 or 5.612 members. \n",
        "\n",
        "One useful property of the Poisson distribution is that its expected value is equal to its parameter, i.e.:\n",
        "\n",
        "$$E\\large[ \\;Z\\; | \\; \\lambda \\;\\large] = \\lambda $$\n",
        "\nThis property is often used, so it's useful to remember. Below, we plot the probability mass distribution for different $\\lambda$ values. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "figsize(12.5, 4)\n",
        "\n",
        "import scipy.stats as stats\n",
        "a = np.arange(16)\n",
        "poi = stats.poisson\n",
        "lambda_ = [1.5, 4.25]\n",
        "colours = [\"#348ABD\", \"#A60628\",]\n",
        "\n",
        "plt.bar(a, poi.pmf(a, lambda_[0]), color=colours[0],\n",
        "        label=\"$\\lambda = %.1f$\" % lambda_[0], alpha=0.60,\n",
        "        edgecolor=colours[0], lw=\"3\")\n",
        "\n",
        "plt.bar(a, poi.pmf(a, lambda_[1]), color=colours[1],\n",
        "        label=\"$\\lambda = %.1f$\" % lambda_[1], alpha=0.60,\n",
        "        edgecolor=colours[1], lw=\"3\")\n",
        "\n\n",
        "plt.xticks(a + 0.4, a)\n",
        "plt.legend()\n",
        "plt.ylabel(\"probability of $k$\")\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.title(\"Probability mass function of a Poisson random variable; differing \\\n",
        "$\\lambda$ values\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continuous Case\n",
        "Instead of a probability mass function, a continuous random variable has a *probability density function*. \n",
        "\n",
        "An example of continuous random variable is a random variable with **exponential density**. The density function for an exponential random variable looks like this:\n",
        "\n",
        "$$f_Z(z | \\lambda) = \\lambda e^{-\\lambda z }, \\;\\; z\\ge 0$$\n",
        "\n",
        "Like a Poisson random variable, an exponential random variable can take on only non-negative values. But unlike a Poisson variable, the exponential can take on *any* non-negative values, including non-integral values such as 4.25 or 5.612401. \n",
        "\n",
        "This property makes it a poor choice for count data, which must be an integer, but a great choice for time data, temperature data, or any other *precise and positive* variables. The graph below shows two probability density functions with different $\\lambda$ values. \n",
        "\n",
        "Given a specific $\\lambda$, the expected value of an exponential random variable is equal to the inverse of $\\lambda$, that is:\n",
        "\n$$E[\\; Z \\;|\\; \\lambda \\;] = \\frac{1}{\\lambda}$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.linspace(0, 4, 100)\n",
        "expo = stats.expon\n",
        "\n",
        "mean = [0.5, 1]\n",
        "\n",
        "for l, c in zip(mean, colours):\n",
        "    plt.plot(a, expo.pdf(a, scale=1. / l), lw=3,\n",
        "             color=c, label=\"$\\lambda = %.1f$\" % l)\n",
        "    plt.fill_between(a, expo.pdf(a, scale=1. / l), color=c, alpha=.33)\n",
        "\n",
        "plt.legend()\n",
        "plt.ylabel(\"PDF at $z$\")\n",
        "plt.xlabel(\"$z$\")\n",
        "plt.ylim(0, 1.2)\n",
        "plt.title(\"Probability density function of an Exponential random variable;\\\n",
        " differing $\\lambda$\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### But what is $\\lambda \\;$?\n",
        "\n",
        "**This question is what motivates statistics**. In the real world, $\\lambda$ is hidden from us. We see only $Z$, and must go backwards to try and determine $\\lambda$. The problem is difficult because there is no one-to-one mapping from $Z$ to $\\lambda$. Many different methods have been created to solve the problem of estimating $\\lambda$, but since $\\lambda$ is never actually observed, no one can say for certain which method is best! \n",
        "\nBayesian inference is concerned with *beliefs* about what $\\lambda$ might be. Rather than try to guess $\\lambda$ exactly, we can only talk about what $\\lambda$ is likely to be by assigning a probability distribution to $\\lambda$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The World Cup Problem: Germany v. Brazil\n",
        "*Source: Allen Downey*\n",
        "\n",
        "> In the 2014 FIFA World Cup, Germany played Brazil in a semifinal\n",
        "> match.  Germany scored after 11 minutes and again at the 23 minute\n",
        "> mark.  At that point in the match, how many goals would you expect\n",
        "> Germany to score after 90 minutes?  What was the probability that they\n",
        "> would score 5 more goals?\n",
        "\n",
        "Scoring in games like soccer and hockey can be (reasonably) well modeled by a Poisson process, which assumes that each team, against a given opponent, will score goals at some goal-scoring rate, $\\lambda$, and that this rate is stationary; in other words, the probability of scoring a goal is about the same at any point during the game.\n",
        "\n",
        "Then, it can be shown that:\n",
        "\n",
        "+ **If the average number of goals (goal scoring rate) in a game is $\\lambda$, the distribution of goals per game is given by the Poisson PMF with parameter $\\lambda$.**\n",
        "\n",
        "+ **If goal-scoring is a Poisson process, the time between each pair of consecutive goals** (with mean ``time_per_game/average_number_goals``) **has an exponential distribution with parameter $\\lambda$**. In other words,  the goal-scoring-rate has an exponential distribution with parameter $1/\\lambda$.\n",
        "\n",
        "Based on this modeling decision, we can answer the questions by\n",
        "\n",
        "1. Defining a prior distribution for Germany's goal-scoring rate against Brazil,\n",
        "2. Updating the prior based on the first two goals, and\n",
        "3. Generating a predictive distribution for the number of goals they would score in the remaining minutes. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a probabilistic programming environment: ``pymc``.\n",
        "\n",
        "PyMC is a python module that implements Bayesian statistical models and fitting algorithms. \n",
        "\n",
        "PyMC provides functionalities to make Bayesian analysis as painless as possible. Here is a short list of some of its features:\n",
        "\n",
        "+ Fits Bayesian statistical models with Markov chain Monte Carlo and other algorithms.\n",
        "+ Includes a large suite of well-documented statistical distributions.\n",
        "+ Uses NumPy for numerics wherever possible.\n",
        "+ Creates summaries including tables and plots.\n",
        "+ Several convergence diagnostics are available."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **What can we do when the posterior can not be analitically computed?**\n",
        "\n",
        "> Markov chain Monte Carlo (MCMC) methods allow us to estimate the shape of a posterior distribution in case we can’t compute it directly. \n",
        "\n",
        "> Monte Carlo simulations are just a way of estimating a fixed parameter by repeatedly generating random numbers. By taking the random numbers generated and doing some computation on them, Monte Carlo simulations provide an approximation of a parameter where calculating it directly is impossible or prohibitively expensive.\n",
        "\n> The only think we need is to specify the prior distribution and the likelihood."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Constructing the prior\n",
        "\n",
        "Before the game starts, what should we believe about Germany's goal-scoring rate against Brazil?  \n",
        "\n",
        "We could use previous tournament results to construct the prior $P(\\lambda)$, but to keep things simple, We'll just use the average goal-scoring rate from all matches in the tournament, which was 2.67 goals per game (total for both teams).\n",
        "\n",
        "The, we can construct a (exponential) prior where the mean matches the observed rate for a single team, 1.34 goals per game.\n",
        "\nWe will work in rates per minute to be able to give an answer to the questions."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "%matplotlib inline\n",
        "figsize(12,6)\n",
        "\n",
        "avg_goals_per_minute = 1.34/90.\n",
        "print('The average number of goals per minute is ', avg_goals_per_minute)\n",
        "\n",
        "duration_of_game = 90.\n",
        "\n",
        "#prior\n",
        "lambda_ = pm.Exponential('lambda_', 1.0/avg_goals_per_minute)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now generate a sample of $\\lambda$ values from the exponential distribution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#samples from prior\n",
        "goals_per_minute  = np.array([lambda_.random() for i in range(100000)])\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.show()\n",
        "    plt.hist(goals_per_minute, bins=100);\n",
        "    plt.title('Prior distribution: Goal scoring rates');\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are able compute the histogram of goals per game per team from a Poisson distribution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lambda2_ = duration_of_game*goals_per_minute\n",
        "sample_points_per_team = np.random.poisson(lambda2_)\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.hist(sample_points_per_team, bins=sample_points_per_team.max(), density=True);\n",
        "    plt.title('Hypothetical goals/game/team,\\n given Poisson model assumptions')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.xlabel('Number of goals');\n",
        "\nprint(sample_points_per_team.mean())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Updating\n",
        "\n",
        "In this case we are given as data the inter-arrival time of the first two goals, 11 minutes and 12 minutes. We use the MCMC method to compute a posterior distribution for the rest of the game.\n",
        "\n",
        "If we knew the actual goal scoring rate, $\\lambda$, we could predict how many goals Germany would score in the remaining $t = 90-23$ minutes.  The distribution of goals would be Poisson with parameter $\\lambda t$.\n",
        "\n",
        "We don't actually know $\\lambda$, but we can use the posterior distribution of $\\lambda$, $P(\\lambda | D)$, to generate a predictive distribution for the number of additional goals:\n",
        "\n$$ P(\\lambda | D) \\approx P(\\lambda)_{exp} P(D|\\lambda)_{poi} $$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "duration_between_goals = [11, 12]\n",
        "\n",
        "# prior\n",
        "avg_goals_per_minute = 1.34/90.\n",
        "expected_duration_between_goals = 1.0/avg_goals_per_minute\n",
        "lambda_ = pm.Exponential('lambda_', expected_duration_between_goals)\n",
        "\n",
        "# connection of observed data with data generation process\n",
        "obs = pm.Exponential('obs', lambda_, observed=True, value=duration_between_goals)\n",
        "\n",
        "# likelihood\n",
        "prediction = pm.Poisson('pred', (duration_of_game-23)*lambda_)\n",
        "\n",
        "# computation of the posterior distribution (that is returned as a distribution)\n",
        "mcmc = pm.MCMC([lambda_, obs, prediction])\n",
        "mcmc.sample(25000,5000)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can recover the values of the posterior distribution of $\\lambda$:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "l_trace = mcmc.trace('lambda_')[:]\n",
        "\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.hist(l_trace,bins=100, label=\"posterior of $\\lambda$\", density=True);\n",
        "    plt.legend(loc=\"upper right\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "And also the posterior of the prediction:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_trace = mcmc.trace('pred')[:]\n",
        "print(prediction_trace.mean())\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.hist(prediction_trace,bins=max(prediction_trace), density=True);\n",
        "    plt.title(\"Predictive distribution of Germany's goals in the next 70 minutes\")\n",
        "    plt.ylabel('Probability')\n",
        "    plt.xlabel('Number of goals');"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "(prediction_trace >= 5).mean()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revisiting P-values."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm  \n",
        "import numpy as np  \n",
        "import pandas as pd\n",
        "import seaborn"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('files/2002FemPreg.dat', 'r')\n",
        "\n\n",
        "def chr_int(a):\n",
        "    if a == '  ':\n",
        "        return 0\n",
        "    else:\n",
        "        return int(a)\n",
        "        \n",
        "preg=[]\n",
        "for line in file:\n",
        "    lst  = [int(line[:12]), int(line[274:276]), int(line[276]), \\\n",
        "                 chr_int(line[277:279]), float(line[422:440])]\n",
        "    preg.append(lst)\n",
        "    \n",
        "import pandas as pd\n",
        "df = pd.DataFrame(preg)\n",
        "df.columns = ['caseid', 'prglength', 'outcome', 'birthord', 'finalwgt']\n",
        "\n",
        "#data cleaning\n",
        "df2 = df.drop(df.index[(df.outcome == 1) & (df['prglength'] > df['prglength'].median() + 6)])\n",
        "df2[(df2.outcome == 1) & (df2['prglength'] > df2['prglength'].median() + 6)]\n",
        "df3 = df2.drop(df2.index[(df2.outcome == 1) & (df2['prglength'] < df2['prglength'].median() - 10)])\n",
        "df3[(df3.outcome == 1) & (df3['prglength'] < df3['prglength'].median() - 10)]\n",
        "\n",
        "firstbirth = df3[(df3.outcome == 1) & (df3.birthord == 1)]\n",
        "othersbirth = df3[(df3.outcome == 1) & (df3.birthord >= 2)]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we want to do is for two given populations, estimate the means ($\\mu_1$ and $\\mu_2$) and variances ($\\sigma_1$ and $\\sigma_2$) of the populations. \n",
        "\n",
        "Mathematically we want to find the posterior distribution, $P(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2 |D)$.\n",
        "\n",
        "Invoking Bayes Theorem we know:\n",
        "\n",
        "$$  P(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2 |D) = \n",
        "\\frac{P(D|\\mu_1, \\sigma_1, \\mu_2, \\sigma_2 ) P(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2 )}{P(D)} $$\n",
        "\n",
        "With the posterior distribution we can compute the probability of $\\mu_1 > \\mu_2$ and other measures of interest like the differences in variance of the groups.\n",
        "\n",
        "To keep the prior distribution of the mean broad, we use a Normal distribution centered in the pooled data mean with a standard deviation set to 1000 times the STD of the pooled data.\n",
        "\nTo model the prior distribution of the standard deviation we use a Uniform distribution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "group1 = firstbirth['prglength']\n",
        "group2 = othersbirth['prglength']\n",
        "\n",
        "# Generate Pooled Data\n",
        "pooled = np.concatenate((group1,group2))\n",
        "\n",
        "# Setup our priors\n",
        "# In pymc, tau :  1/standard_deviation.\n",
        "\n",
        "mu1 = pm.Normal(\"mu_1\",mu=pooled.mean(), tau=1.0/pooled.var()/1000.0)\n",
        "mu2 = pm.Normal(\"mu_2\",mu=pooled.mean(), tau=1.0/pooled.var()/1000.0)\n",
        "\n",
        "sig1 = pm.Uniform(\"sigma_1\",lower=pooled.var()/1000.0,upper=pooled.var()*1000)\n",
        "sig2 = pm.Uniform(\"sigma_2\",lower=pooled.var()/1000.0,upper=pooled.var()*1000)\n",
        "\n",
        "# Now we want to setup our posterior distributions. \n",
        "# Include our observed data into the model\n",
        "\n",
        "t1 = pm.Normal(\"t_1\",mu=mu1, tau=1.0/sig1, value=group1[:], observed=True)  \n",
        "t2 = pm.Normal(\"t_2\",mu=mu2, tau=1.0/sig2, value=group2[:], observed=True)  \n",
        "\n",
        "# Push our priors into a model\n",
        "model = pm.Model( [t1, mu1, sig1, t2, mu2, sig2] )\n",
        "\n",
        "# Generate our MCMC object\n",
        "mcmc = pm.MCMC(model)  \n",
        "\n",
        "# Run MCMC sampler\n",
        "mcmc.sample(40000,10000,2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we want to actually get the data out of it. The reason that we set string representations of each object in our model was so that we can pull out the results from our sampler like this:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mus1 = mcmc.trace('mu_1')[:]  \n",
        "mus2 = mcmc.trace('mu_2')[:]  \n",
        "sigmas1 = mcmc.trace('sigma_1')[:]  \n",
        "sigmas2 = mcmc.trace('sigma_2')[:]  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are sampling the distribution of our parameters, not computing them exactly. The result is that we have a measure of confidence on where we think a parameter should be. This allows us to do things like compute the distributions of parameters and the distributions of their differences.\n",
        "\nThe most interesting metrics for this test are the following:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "diff_mus = mus1-mus2  \n",
        "diff_sigmas = sigmas1-sigmas2  \n",
        "effect_size = (mus1-mus2)/np.sqrt((sigmas1**2+sigmas2**2)/2.) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate estimates of parameters using the expectation of their posteriors. So for estimating our means we can just take the means of our traces above:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"mu_1\", mus1.mean())\n",
        "print(\"mu_2\", mus2.mean())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "How confidence are we in these values? We can plot the kernel densities of our parameters to see exactly what the probability of a given value is. In IPython we just do the following:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline \n",
        "from scipy.stats import gaussian_kde \n",
        "import matplotlib.pylab as plt\n",
        "# plt.figsize(16,9)\n",
        "\n",
        "# prepare plotting area to fit both graphs\n",
        "minx = min(min(mus1),min(mus2))  \n",
        "maxx = max(max(mus1),max(mus2))  \n",
        "# x values to plot on\n",
        "xs = np.linspace(minx,maxx,100)\n",
        "\n",
        "# generate density estimates\n",
        "gkde1 = gaussian_kde(mus1)  \n",
        "gkde2 = gaussian_kde(mus2)\n",
        "\n",
        "# draw plots\n",
        "plt.plot(xs,gkde1(xs),label='$\\mu_1$')  \n",
        "plt.plot(xs,gkde2(xs),label='$\\mu_2$')  \n",
        "plt.legend()  \n",
        "plt.show()  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have our estimates and we have distributions for them, let's see what questions we can answer and how certain we are about our answers.\n",
        "\n",
        "In statistics the common way to go about answering a question is hypothesis testing, and since we're working in the Bayesian framework, we will be doing Bayesian Testing.\n",
        "\n",
        "Suppose we want to analyze the following hypotheses: ($H_0:\\mu_1 \\geq \\mu_2$) vs ($H_1:\\mu_1  < \\mu_2$). \n",
        "\n",
        "To test the null hypothesis we need to estimate $P(\\mu_1−\\mu_2 > 0|D)$, which is easily computed because we've already computed the distribution of $\\mu_1−\\mu_2$. \n",
        "\nAll we have left to do is compute the probability from our samples that $H_0$ is true, i.e. our 'p-value' is generated by:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "diff_mus = mus1-mus2  \n",
        "p = (diff_mus > 0.0).mean()  \n",
        "print(p)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly we do the complement of this to test whether $H_1$ is true. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "p = (diff_mus < 0.0).mean()  \n",
        "print(p)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize this probability through the following:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "p = (diff_mus>0).mean()  \n",
        "print(p)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(121)\n",
        "\n",
        "minx = min(min(mus1),min(mus2))  \n",
        "maxx = max(max(mus1),max(mus2))  \n",
        "xs = np.linspace(minx,maxx,1000)\n",
        "\n",
        "gkde1 = gaussian_kde(mus1)  \n",
        "gkde2 = gaussian_kde(mus2)\n",
        "\n",
        "plt.plot(xs,gkde1(xs),label='$\\mu_1$')  \n",
        "plt.plot(xs,gkde2(xs),label='$\\mu_2$')  \n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "\n",
        "minx = min(diff_mus)  \n",
        "maxx = max(diff_mus)  \n",
        "xs = np.linspace(minx,maxx,100)  \n",
        "gkde = gaussian_kde(diff_mus)\n",
        "\n",
        "plt.plot(xs,gkde(xs),label='$\\mu_1-\\mu_2$')  \n",
        "plt.legend()\n",
        "\n",
        "plt.axvline(0, color='#000000',alpha=0.3,linestyle='--')\n",
        "\nplt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Inferring behaviour from text-message data.\n",
        "\n",
        "> You are given a series of daily text-message counts from a user of your system. The data, plotted over time, appears in the chart below. You are curious to know if the user's text-messaging habits have changed over time, either gradually or suddenly. How can you model this? (This is in fact my own text-message data. Judge my popularity as you wish.)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "figsize(12.5, 3.5)\n",
        "count_data = np.loadtxt(\"files/txtdata.csv\")\n",
        "n_count_data = len(count_data)\n",
        "plt.bar(np.arange(n_count_data), count_data, color=\"#348ABD\")\n",
        "plt.xlabel(\"Time (days)\")\n",
        "plt.ylabel(\"count of text-msgs received\")\n",
        "plt.title(\"Did the user's texting habits change over time?\")\n",
        "plt.xlim(0, n_count_data);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "\n",
        "alpha = 1.0 / count_data.mean()  # Recall count_data is the\n",
        "                               # variable that holds our txt counts\n",
        "lambda_1 = pm.Exponential(\"lambda_1\", alpha)\n",
        "lambda_2 = pm.Exponential(\"lambda_2\", alpha)\n",
        "\n",
        "tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_count_data)\n",
        "\n",
        "@pm.deterministic\n",
        "def lambda_(tau=tau, lambda_1=lambda_1, lambda_2=lambda_2):\n",
        "    out = np.zeros(n_count_data)\n",
        "    out[:tau] = lambda_1  # lambda before tau is lambda1\n",
        "    out[tau:] = lambda_2  # lambda after (and including) tau is lambda2\n",
        "    return out\n",
        "\n",
        "observation = pm.Poisson(\"obs\", lambda_, value=count_data, observed=True)\n",
        "\n",
        "model = pm.Model([observation, lambda_1, lambda_2, tau])\n",
        "\n",
        "mcmc = pm.MCMC(model)\n",
        "mcmc.sample(40000, 10000, 1)\n",
        "\n",
        "lambda_1_samples = mcmc.trace('lambda_1')[:]\n",
        "lambda_2_samples = mcmc.trace('lambda_2')[:]\n",
        "tau_samples = mcmc.trace('tau')[:]\n",
        "\n",
        "figsize(12.5, 5)\n",
        "# tau_samples, lambda_1_samples, lambda_2_samples contain\n",
        "# N samples from the corresponding posterior distribution\n",
        "N = tau_samples.shape[0]\n",
        "expected_texts_per_day = np.zeros(n_count_data)\n",
        "for day in range(0, n_count_data):\n",
        "    # ix is a bool index of all tau samples corresponding to\n",
        "    # the switchpoint occurring prior to value of 'day'\n",
        "    ix = day < tau_samples\n",
        "    # Each posterior sample corresponds to a value for tau.\n",
        "    # for each day, that value of tau indicates whether we're \"before\"\n",
        "    # (in the lambda1 \"regime\") or\n",
        "    #  \"after\" (in the lambda2 \"regime\") the switchpoint.\n",
        "    # by taking the posterior sample of lambda1/2 accordingly, we can average\n",
        "    # over all samples to get an expected value for lambda on that day.\n",
        "    # As explained, the \"message count\" random variable is Poisson distributed,\n",
        "    # and therefore lambda (the poisson parameter) is the expected value of\n",
        "    # \"message count\".\n",
        "    expected_texts_per_day[day] = (lambda_1_samples[ix].sum()\n",
        "                                   + lambda_2_samples[~ix].sum()) / N\n",
        "\n\n",
        "plt.plot(range(n_count_data), expected_texts_per_day, lw=4, color=\"#E24A33\",\n",
        "         label=\"expected number of text-messages received\")\n",
        "plt.xlim(0, n_count_data)\n",
        "plt.xlabel(\"Day\")\n",
        "plt.ylabel(\"Expected # text-messages\")\n",
        "plt.title(\"Expected number of text-messages received\")\n",
        "plt.ylim(0, 60)\n",
        "plt.bar(np.arange(len(count_data)), count_data, color=\"#348ABD\", alpha=0.65,\n",
        "        label=\"observed texts per day\")\n",
        "\nplt.legend(loc=\"upper left\");"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "More information: [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}