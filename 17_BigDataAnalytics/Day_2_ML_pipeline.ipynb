{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n#Practica sobre como generar un flujo de ejecucion en un problema de Machine Learning\n\nEsta practica simula un ejercicio completo de ETL (Extract-Transform-Load) junto a un analisis exploratorio de un dataset real, para posteriormente aplicar differentes algoritmos de aprendizaje automatico que resuelvan un problema de regresion.\n\n** This notebook covers: **\n* *Parte 1: Conocimiento del dominio*\n* *Parte 2: Extraccion, transformacion y carga [ETL] del dataset* (1 punto sobre 10)\n* *Parte 3: Explorar los datos* (1 puntos sobre 10)\n* *Parte 4: Visualizar los datos* (1 puntos sobre 10)\n* *Parte 5: Preparar los datos* (1 puntos sobre 10)\n* *Parte 6: Modelar los datos* (2 puntos sobre 10)\n* *Parte 7: Ajustar y evaluar* (4 puntos sobre 10)\n\n*Nuestro objetivo sera predecir de la forma mas exacta posible la energia generada por un conjunto de de plantas electricas usando los datos generados por un conjunto de sensores.*\n\n\n## Parte 1: Conocimiento del dominio\n\n** Background **\n\n\nLa generacion de energia es un proceso complejo, comprenderlo para poder predecir la potencia de salida es un elemento vital en la gestion de una planta energetica y su conexion a la red. Los operadores de una red electrica regional crean predicciones de la demanda de energia en base a la informacion historica y los factores ambientales (por ejemplo, la temperatura). Luego comparan las predicciones con los recursos disponibles (por ejemplo, plantas, carbon, gas natural, nuclear, solar, eolica, hidraulica, etc). Las tecnologias de generacion de energia, como la solar o la eolica, dependen en gran medida de las condiciones ambientales, pero todas las centrales electricas son objeto de mantenimientos tanto planificados y como puntuales debidos a un problema.\n\nEn esta practica usaremos un ejemplo del mundo real sobre la demanda prevista (en dos escalas de tiempo), la demanda real, y los recursos disponibles de la red electrica de California: http://www.caiso.com/Pages/TodaysOutlook.aspx\n\n![](http://content.caiso.com/outlook/SP/ems_small.gif)\n\nEl reto para un operador de red de energia es como manejar un deficit de recursos disponibles frente a la demanda real. Hay tres posibles soluciones a un deficit de energia: construir mas plantas de energia base (este proceso puede costar muchos anos de planificacion y construccion), comprar e importar de otras redes electricas regionales energia sobrabte (esta opcion puede ser muy cara y esta limitado por las interconexiones entre las redes de transmision de energia y el exceso de potencia disponible de otras redes), o activar pequenas [plantas de pico](https://en.wikipedia.org/wiki/Peaking_power_plant). Debido a que los operadores de red necesitan responder con rapidez a un deficit de energia para evitar un corte del suministro, estos basan sus decisiones en una combinacion de las dos ultimas opciones. En esta practica, nos centraremos en la ultima eleccion.\n\n** La logica de negocio **\n\nDebido a que la demanda de energia solo supera a la oferta ocasionalmente, la potencia suministrada por una planta de energia pico tiene un precio mucho mas alto por kilovatio hora que la energia generada por las centrales electricas base de una red electrica. Una planta pico puede operar muchas horas al dia, o solo unas pocas horas al ano, dependiendo de la condicion de la red electrica de la region. Debido al alto coste de la construccion de una planta de energia eficiente, si una planta pico solo va a funcionar por un tiempo corto o muy variable, no tiene sentido economico para que sea tan eficiente como una planta de energia base. Ademas, el equipo y los combustibles utilizados en las plantas base a menudo no son adecuados para uso en plantas de pico.\n\nLa salida de potencia de una central electrica pico varia dependiendo de las condiciones ambientales, por lo que el problema de negocio a resolver se podria describir como _predecir la salida de potencia de una central electrica pico en funcion de la condiciones ambientales_  - ya que esto permitiria al operador de la red hacer compensaciones economicas sobre el numero de plantas pico que ha de conectar en cada momento (o si por el contrario le interesa comprar energia mas cara de otra red).\n\nUna vez descrita esta logica de negocio, primero debemos proceder a realizar un analisis exploratorio previo y trasladar el problema de negocio (predecir la potencia de salida en funcion de las condiciones medio ambientales) en un tarea de aprendizaje automatico (ML). Por ejemplo, una tarea de ML que podriamos aplicar a este problema es la regression, ya que tenemos un variable objetivo (dependiente) que es numerica. Para esto usaremos [Apache Spark ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark-ml-package) para calcular dicha regresion.\n\nLos datos del mundo real que usaremos en esta practica se componen de 9.568 puntos de datos, cada uno con 4 atributos ambientales recogidos en una Central de Ciclo Combinado de mas de 6 anos (2006-2011), proporcionado por la Universidad de California, Irvine en [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)). Para mas detalles sobre el conjunto de datos visitar la pagina de la UCI, o las siguientes referencias:\n\n* Pinar Tufekci, [Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods](http://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems/), International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615.\n* Heysem Kaya, Pinar Tufekci and Fikret S. Gurgen: [Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine](http://www.cmpe.boun.edu.tr/~kaya/kaya2012gasturbine.pdf), Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai).\n\n**Ejercicio 1(a)**: Leer la documentacion y referencias de [Spark Machine Learning Pipeline](https://spark.apache.org/docs/1.6.2/ml-guide.html#main-concepts-in-pipelines)."],"metadata":{}},{"cell_type":"markdown","source":["## Part 2: Extraccion, transformacion y carga [ETL] del dataset\n\n\nAhora que entendemos lo que estamos tratando de hacer, el primer paso consiste en cargar los datos en un formato que podemos consultar y utilizar facilmente. Esto se conoce como ETL o \"extraccion, transformacion y carga\". Primero, vamos a cargar nuestro archivo de Amazon S3.\n\nNota: Como alternativa podemos subir nuestros datos utilizando \"Databricks Menu> Tablas> Crear tabla\", suponiendo que tengamos los archivos sin procesar en nuestro ordenador local.\n\nNuestros datos esta disponible en Amazon S3 en la siguiente ruta:\n\n```\ndbfs:/databricks-datasets/power-plant/data\n```\n\n**Ejercicio 1(b):** Empezaremos por visualizar una muestra de los datos. Para esto usaremos las funciones pre-definidas en los notebooks de Databricks para explorar su sistema de archivos.\n\nUsar `display(dbutils.fs.ls(\"/databricks-datasets/power-plant/data\"))` para listar los ficheros del directorio"],"metadata":{}},{"cell_type":"code","source":["#TODO: use display to list all the files of the directory containing the data\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Ahora, usaremos el comando `dbutils.fs.head` para ver los primeros 65,536 bytes del primer archivo del directorio.\n\nUsar `print dbutils.fs.head(\"/databricks-datasets/power-plant/data/Sheet1.tsv\")` para ver el contenido del archivo Sheet1.tsv"],"metadata":{}},{"cell_type":"code","source":["#TODO: print the first 65,536 bytes of the file Sheet1.tsv\nprint <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["`dbutils.fs` dispone de su propio help, esta ayuda nos sera de gran utilidad cuando deseemos ver las diferentes funciones disponibles."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Ejercicio 2(a)\n\nAhora usaremos PySpark para visualizar las 5 primeras lineas de los datos\n\n*Hint*: Primero crea un RDD a partir de los datos usando [`sc.textFile(\"dbfs:/databricks-datasets/power-plant/data\")`](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext.textFile).\n\n*Hint*: Luego piensa como usar el RDD creado para mostrar datos, el metodo [`take()`](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.take) puede ser una buena opcion a considerar."],"metadata":{}},{"cell_type":"code","source":["# TODO: Load the data and print the first five lines.\nrawTextRdd = <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["A partir nuestra exploracion inicial de una muestra de los datos, podemos hacer varias observaciones sobre el proceso de ETL:\n- Los datos son un conjunto de .tsv (archivos con valores separados Tab) (es decir, cada fila de datos se separa mediante tabuladores)\n- Hay una fila de cabecera, que es el nombre de las columnas\n- Parece que el tipo de los datos en cada columna es constante (es decir, cada columna es de tipo double)\n\nEl esquema de datos que hemos obtenido de UCI es:\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output.  Esta es la variable dependiente que queremos predecir usando los otras cuatro\n\n\nAhora estamos en disposicion de crear un DataFrame a partir de los datos de TSV. Spark no tiene un metodo nativo para realizar esta operacion, sin embargo, podemos utilizar [spark-csv](https://spark-packages.org/package/databricks/spark-csv), un paquete de un tercero de [SparkPackages](https://spark-packages.org/). La documentacion y el codigo fuente para [spark-csv](https://spark-packages.org/package/databricks/spark-csv) pueden encontrarse en [GitHub](https://github.com/databricks/spark-csv). La API de Python puede encontrarse [aqui](https://github.com/databricks/spark-csv#python-api).\n\n(**Nota**: En Spark 2.0, El paquete CSV se encuenta dentro de las llamadas de la API de la clase DataFrame.)\n\nPara usar el paquete Spark CSV [spark-csv](https://spark-packages.org/package/databricks/spark-csv), usaremos el metodo [sqlContext.read.format()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.format) para especificar el formato de la fuente de datos de entrada: `'com.databricks.spark.csv'`\n\nPodemos especificar diferentes opciones de como importar los datos usando el metodo [options()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.options). Encontramos las opciones disponible en la documentacion de GitHub del paquete [aqui](https://github.com/databricks/spark-csv#features).\n\nUsaremos las siguientes opciones:\n- `delimiter='\\t'` porque nuestros datos se encuentran delimitados por tabulaciones\n- `header='true'` porque nuestro dataset tiene una fila que representa la cabezera de los datos\n- `inferschema='true'` porque creemos que todos los datos son numeros reales, por lo tanto la libreria puede inferir el tipo de cada columna de forma automatica.\n\nEl ultimo componente necesario para crear un DataFrame es determinar la ubicacion de los datos usando el metodo [load()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.load): `\"/databricks-datasets/power-plant/data\"`\n\nJuntando todo, usaremos la siguiente operacion:\n\n`sqlContext.read.format().options().load()`\n\n### Exercicio 2(b)\n\n**To Do:** Crear un DataFrame a partir de los datos."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\npowerPlantDF = sqlContext.read.format(<FILL_IN>).options(<FILL_IN>).load(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# TEST\nfrom databricks_test_helper import *\nexpected = set([(s, 'double') for s in ('AP', 'AT', 'PE', 'RH', 'V')])\nTest.assertEquals(expected, set(powerPlantDF.dtypes), \"Incorrect schema for powerPlantDF\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Vamos a comprobar los tipos de las columnas usando el metodo [dtypes](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes)."],"metadata":{}},{"cell_type":"code","source":["print powerPlantDF.dtypes"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Tambien podemos examinar los datos usando el metodo `display()`."],"metadata":{}},{"cell_type":"code","source":["display(powerPlantDF)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Ahora en lugar de usar [spark-csv](https://spark-packages.org/package/databricks/spark-csv) para inferir los tipos de las columnas, especificaremos el esquema como [DataType](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.DataType), el cual es una lista de [StructField](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.StructType).\n\nLa lista completa de tipos se encuetra en el modulo [pyspark.sql.types](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.types). Para nuestros datos, usaremos [DoubleType()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType).\n\nPor ejemplo, para especificar cual es el nombre de la columna usaremos: `StructField(`_name_`,` _type_`, True)`. (El tercer parametro, `True`, significa que permitimos que la columna tenga valores null.)\n\n### Exercicio 2(c)\n\nCrea un esquema a medida para el dataset."],"metadata":{}},{"cell_type":"code","source":["# TO DO: Fill in the custom schema.\nfrom pyspark.sql.types import *\n\n# Custom Schema for Power Plant\ncustomSchema = StructType([ \\\n    <FILL_IN>, \\\n    <FILL_IN>, \\\n    <FILL_IN>, \\\n    <FILL_IN>, \\\n    <FILL_IN> \\\n                          ])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# TEST\nTest.assertEquals(set([f.name for f in customSchema.fields]), set(['AT', 'V', 'AP', 'RH', 'PE']), 'Incorrect column names in schema.')\nTest.assertEquals(set([f.dataType for f in customSchema.fields]), set([DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType()]), 'Incorrect column types in schema.')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Exercicio 2(d)\n\nAhora, usaremos el esquema que acabamos de crear para leer los datos. Para realizar esta operacion, modificaremos el paso anterior `sqlContext.read.format`. Podemos especificar el esquema haciendo:\n- Anadir `schema = customSchema` al metodo load (simplemente anadelo usando una coma justo despues del nombre del archivo)\n- Eliminado la opcion `inferschema='true'` ya que ahora especificamos el esquema que han de seguir los datos"],"metadata":{}},{"cell_type":"code","source":["# TODO: Use the schema you created above to load the data again.\naltPowerPlantDF = sqlContext.read.format(<FILL_IN>).options(<FILL_IN>).load(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# TEST\nfrom databricks_test_helper import *\nexpected = set([(s, 'double') for s in ('AP', 'AT', 'PE', 'RH', 'V')])\nTest.assertEquals(expected, set(altPowerPlantDF.dtypes), \"Incorrect schema for powerPlantDF\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Es importante darse cuenta que esta vez no se ha ejecutado ningun job de Spark. Esto se debe a que hemos especificado el esquema, por tanto el paquete [spark-csv](https://spark-packages.org/package/databricks/spark-csv) no tiene porque leer los datos para inferir el esquema. Podemos usar el metodo [dtypes](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes) para examinar el nombre y el tipo de los atributos del dataset. Estos deberian ser identicos a los que hemos inferido anteriormente de los datos.\n\nCuando ejecutes la siguiente celda, los datos no deberian leerse."],"metadata":{}},{"cell_type":"code","source":["print altPowerPlantDF.dtypes"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Ahora podemos examinar los datos utilizando el metodo display(). * Ten en cuenta que esta operacion hara que los datos que se lean y se creara el DataFrame. *"],"metadata":{}},{"cell_type":"code","source":["display(altPowerPlantDF)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Parte 3: Explorar tus Datos\n\nAhora que ya hemos cargado los datos, el siguiente paso es explorarlos y realizar algunos analisis y visualizaciones basicas.\n\nEste es un paso que siempre se debe realizar **antes de** intentar ajustar un modelo a los datos, ya que este paso muchas veces nos permitira conocer una gran informacion sobre los datos."],"metadata":{}},{"cell_type":"markdown","source":["En primer lugar vamos a registrar nuestro DataFrame como una tabla de SQL llamado `power_plant`. Debido a que es posible que repitas esta practica varias veces, vamos a tomar la precaucion de eliminar cualquier tabla existente en primer lugar.\n\nPodemos eliminar cualquier tabla SQL existente `power_plant` usando el comando SQL:` DROP TABLE IF EXISTS power_plant` (tambien debemos que eliminar todos los ficheros asociados a la tabla, lo que podemos hacer con una operacion de sistema de archivos Databricks).\n\nUna vez ejecutado el paso anterior, podemos registrar nuestro DataFrame como una tabla de SQL usando [sqlContext.registerDataFrameAsTable()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerDataFrameAsTable).\n\n### Ejercicio 3(a)\n\nEjecutar la siguiente celda con el codigo ya preparado."],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql(\"DROP TABLE IF EXISTS power_plant\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/power_plant\", True)\nsqlContext.registerDataFrameAsTable(powerPlantDF, \"power_plant\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Ahora que nuestro DataFrame existe como una tabla SQL, podemos explorarlo utilizando comandos SQL.\n\nPara ejecutar SQL en una celda, utilizamos el operador `%sql`. La celda siguiente es un ejemplo del uso de SQL para consultar las filas de la tabla de SQL.\n\n**NOTE**: `%sql` es una sentencia que solo funciona en los notebooks de Databricksis. Este ejecuta `sqlContext.sql()` y pasa los resultados a la funcion `display()`. Estas dos sentencias son equivalentes:\n\n`%sql SELECT * FROM power_plant`\n\n`display(sqlContext.sql(\"SELECT * FROM power_plant\"))`\n\n### Ejercicio 3(b)\n\nEjecutar la siguiente celda con el codigo ya preparado."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- We can use %sql to query the rows\nSELECT * FROM power_plant"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Ejercicio 3(c)\n\nUsa el comando de SQL `desc` para describir el esquema ejecutando la siguiente celda."],"metadata":{}},{"cell_type":"code","source":["%sql\ndesc power_plant"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["**Definicion de Esquema**\n\nUna vez mas, nuestro esquema es el siguiente:\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output\n\nPE es nuestra variable objetivo. Este es el valor que intentamos predecir usando las otras mediciones.\n\n*Referencia [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"],"metadata":{}},{"cell_type":"markdown","source":["Ahora vamos a realizar un analisis estadistico basico de todas las columnas.\n\nPodemos obtener el DataFrame asociado a una tabla SQL usando el metodo [sqlContext.table()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.table) pasando como argumento el nombre de la tabla SQL. Una vez hecho esto, es posible usar el metodo nativo de un DataFrame [describe()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) sin argumentos para calcular algunos estadisticos basicos para cara una de las columnas, como por ejemplo contar, media, max, min o la desviacion estandar."],"metadata":{}},{"cell_type":"code","source":["df = sqlContext.table(\"power_plant\")\ndisplay(df.describe())"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["##Parte 4: Visualizar los datos\n\n\nPara entender nuestros datos, intentamos buscar correlaciones entre las diferentes caracteristicas y sus correspondientes etiquetas. Esto puede ser importante cuando seleccionamos un modelo. Por ejemplo, si una etiqueta y sus caracteristicas se correlacionan de forma lineal, un modelo lineal de regresion lineal obtendra un buen rendimiento; por el contrario si la relacion es no lineal, modelos mas complejos, como arboles de decision pueden ser una mejor opcion. Podemos utilizar herramientas de visualizacion para observar cada uno de los posibles predictores en relacion con la etiqueta como un grafico de dispersion para ver la correlacion entre ellos.\n\n### Ejercicio 4(a)\n\n** Anade las siguientes figuras: **\nVamos a ver si hay una correlacion entre la temperatura y la potencia de salida. Podemos utilizar una consulta SQL para crear una nueva tabla que contenga solo el de temperatura (AT) y potencia (PE), y luego usar un grafico de dispersion con la temperatura en el eje X y la potencia en el eje Y para visualizar la relacion (si la hay) entre la temperatura y la energia.\n\nRealiza los siguientes pasos:\n\n- Ejecuta la siguiente celda\n- Haz clic en el menu desplegable junto al icono de \"Bar Chart\" y selecciona \"Scatter\" para convertir la tabla en un grafico de dispersion\n- Haz click en \"Plot Options...\"\n- En la caja de valores, haz clic en \"Temperature\" y arrastralo antes de \"Power\"\n- Aplicar los cambios haciendo clic en el boton \"Apply\"\n- Aumentar el tamano del grafico haciendo clic y arrastrando el control del tamano"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect AT as Temperature, PE as Power from power_plant"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Parece que hay una gran correlacion entre temperatura y power output. Esta correlacion es esperable gracias a la segunda ley de la termodinamica [thermal efficiency](https://en.wikipedia.org/wiki/Thermal_efficiency). Ir mas alla en este analisis queda fuera del ambito de esta practica."],"metadata":{}},{"cell_type":"markdown","source":["### Ejercicio 4(b)\n\nUsa una sentencia SQL para crear un grafico de dispersion entre las variables Power (PE) y Exhaust Vacuum Speed (V)."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TO DO: Replace <FILL_IN> with the appropriate SQL command."],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Ahora vamos a repetir este ejercicio con el resto de variables y la etiqueta Power Output.\n\n### Ejercicio 4(c)\n\nUsa una sentencia SQL para crear un grafico de dispersion entre las variables Power (PE) y Pressure (AP)."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TO DO: Replace <FILL_IN> with the appropriate SQL command.\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["### Ejercicio 4(d)\n\nUsa una sentencia SQL para crear un grafico de dispersion entre las variables Power (PE) y Humidity (RH)."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TO DO: Replace <FILL_IN> with the appropriate SQL command.\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["##Parte 5: Preparacion de los datos\n\nEl siguiente paso es preparar los datos para aplicar la regresion. Dado que todo el dataset es numerico y consistente, esta sera una tarea sencilla y directa.\n\nEl objetivo es utilizar el metodo de regresion para determinar una funcion que nos de la potencia de salida como una funcion de un conjunto de caracteristicas de prediccion. El primer paso en la construccion de nuestra regresion es convertir las caracteristicas de prediccion de nuestro DataFrame a un vector de caracteristicas utilizando el metodo [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n\nEl VectorAssembler es una transformacion que combina una lista dada de columnas en una unico vector. Esta transformacion es muy util cuando queremos combinar caracteristicas en crudo de los datos con otras generadas al aplicar diferentes funciones sobre los datos en un unico vector de caracteristicas. Para integrar en un unico vector toda esta informacion antes de ejecutar un algoritmo de aprendizaje automatico, el VectorAssembler toma una lista con los nombres de las columnas de entrada (lista de strings) y el nombre de la columna de salida (string).\n\n### Ejercicio 5(a)\n\n- leer la documentacion y los ejemplos de uso de [VectorAssembler](https://spark.apache.org/docs/1.6.2/ml-features.html#vectorassembler)\n- Convertir la tabla SQL `power_plant` en un `dataset` llamado datasetDF\n- Establecer las columnas de entrada del VectorAssember: `[\"AT\", \"V\", \"AP\", \"RH\"]`\n- Establecer la columnas de salida como `\"features\"`"],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code\nfrom pyspark.ml.feature import VectorAssembler\n\ndatasetDF = <FILL_IN>\n\nvectorizer = VectorAssembler()\nvectorizer.setInputCols(<FILL_IN>)\nvectorizer.setOutputCol(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# TEST\nTest.assertEquals(set(vectorizer.getInputCols()), {\"AT\", \"V\", \"AP\", \"RH\"}, \"Incorrect vectorizer input columns\")\nTest.assertEquals(vectorizer.getOutputCol(), \"features\", \"Incorrect vectorizer output column\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["##Parte 6: Modelar los datos\n\nAhora vamos a modelar nuestros datos para predecir que potencia de salida se dara cuando tenemos una serie de lecturas de los sensores\n\nNuestro primer modelo se basara en una simple regresion lineal ya que vimos algunos patrones lineales en nuestros datos en los graficos de dispersion durante la etapa de exploracion.\n\nNecesitamos una forma de evaluar como de bien nuestro modelo de regresion lineal predice la produccion de potencia en funcion de parametros de entrada. Podemos hacer esto mediante la division de nuestros datos iniciales establecidos en un _Training set_ utilizado para entrenar a nuestro modelo y un _Test set_ utilizado para evaluar el rendimiento de nuestro modelo. Podemos usar el metodo nativo de los DataFrames [randomSplit()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) para dividir nuestro dataset. El metodo toma una lista de pesos y una semilla aleatoria opcional. La semilla se utiliza para inicializar el generador de numeros aleatorios utilizado por la funcion de division.\n\n### Ejercicio 6(a)\n\nUtiliza el metodo [randomSplit()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) para dividir `datasetDF` en trainingSetDF (80% del DataFrame de entrada) y testSetDF (20% del DataFrame de entrada), para poder reproducir siempre el mismo resultado, usar la semilla 1800009193L. Finalmente, cachea (cache()) cada datafrane en memoria para maximizar el rendimiento."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n# We'll hold out 20% of our data for testing and leave 80% for training\nseed = 1800009193L\n(split20DF, split80DF) = datasetDF.<FILL_IN>\n\n# Let's cache these datasets for performance\ntestSetDF = <FILL_IN>\ntrainingSetDF = <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# TEST\nTest.assertEquals(trainingSetDF.count(), 38243, \"Incorrect size for training data set\")\nTest.assertEquals(testSetDF.count(), 9597, \"Incorrect size for test data set\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["A continuacion vamos a crear un modelo de regresion lineal y utilizar su ayda para entender como entrenarlo. Ver la API de [Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) para mas detalles.\n\n### Ejercicio 6(b)\n\n- Lee la documentacion y los ejemplos de [Linear Regression](https://spark.apache.org/docs/1.6.2/ml-classification-regression.html#linear-regression)\n- Ejecuta la siguiente celda"],"metadata":{}},{"cell_type":"code","source":["# ***** LINEAR REGRESSION MODEL ****\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.regression import LinearRegressionModel\nfrom pyspark.ml import Pipeline\n\n# Let's initialize our linear regression learner\nlr = LinearRegression()\n\n# We use explain params to dump the parameters we can use\nprint(lr.explainParams())"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["La siguiente celda esta basada en [Spark ML Pipeline API for Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression).\n\nEl primer paso es establecer los valores de los parametros:\n- Define el nombre de la columna a donde guardaremos la prediccion como \"Predicted_PE\"\n- Define el nombre de la columna que contiene la etiqueta como \"PE\"\n- Define el numero maximo de iteraciones a 100\n- Define el parametro de regularizacion a 0.1\n\nAhora, crearemos el [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) (flujo de ejecucion) y estableceremos las fases del pipeline como vectorizar y posteriormente aplicar el regresor lineal que hemos definido.\n\nFinalmente, crearemos el modelo entrenandolo con el DataFrame `trainingSetDF`.\n\n### Ejercicio 6(c)\n\n- Lee la documentacion [Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) documentation\n- Completa y ejecuta la siguiente celda, asegurate de entender que es lo que sucede."],"metadata":{}},{"cell_type":"code","source":["## TODO: Replace <FILL_IN> with the appropriate code\n# Now we set the parameters for the method\nlr.setPredictionCol(<FILL IN>)\\\n  .setLabelCol(<FILL IN>)\\\n  .setMaxIter(<FILL IN>)\\\n  .setRegParam(<FILL IN>)\n\n\n# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nlrPipeline = Pipeline()\n\nlrPipeline.setStages([vectorizer, lr])\n\n# Let's first train on the entire dataset to see what we get\nlrModel = lrPipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["Del articulo de Wikipedia [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) podemos leer:\n> In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable \\\\( y \\\\) and one or more explanatory variables (or independent variables) denoted \\\\(X\\\\). In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.\n\nLos modelos de regresion lineal tienen muchos usos practicos. La mayoria de los cuales se clasifican en de las siguientes dos categorias:\n- Si el objetivo es la prediccion o la reduccion de errores, la regresion lineal puede utilizarse para adaptar un modelo predictivo a un conjunto de datos observados \\\\(y\\\\) y \\\\(X\\\\). Despues de desarrollar un modelo de este tipo, dado un cierto valor  \\\\( X\\\\) del que no conocemos su valor de \\\\(y \\\\), el modelo ajustado se puede utilizarse para hacer una prediccion del valor del posible valor \\\\(y \\\\).\n- Dada una variable \\\\(y\\\\) y un numero de variables \\\\( X_1 \\\\), ..., \\\\( X_p \\\\) que pueden estar relacionadas con \\\\(y\\\\), un analisis de regresion lineal puede ser aplicado a cuantificar como de fuerte es la relacion entre \\\\(y\\\\) y cada \\\\( X_j\\\\), para evaluar que \\\\( X_j \\\\) puede no tener ninguna relacion con \\\\(y\\\\), y de esta forma identificar que subconjuntos de \\\\( X_j \\\\) contienen informacion redundante sobre \\\\(y\\\\).\n\nComo estamos interesados en ambos usos, nos gustaria para predecir la potencia de salida en funcion de las variables de entrada, y nos gustaria saber cuales de las variables de entrada estan debilmente o fuertemente correlacionadas con la potencia de salida.\n\nYa que una regresion lineal tan solo calcula la linea que minimiza el error cuadratico medio en el dataset de entrenamiento, dadas multiples dimensiones de entrada podemos expresar cada predictor como una funcion lineal en la forma:\n\n\\\\[ y = a + b x_1 + b x_2 + b x_i ... \\\\]\n\ndonde \\\\(a\\\\) es el intercept (valor para el punto 0) y las \\\\(b\\\\) son los coeficientes.\n\nPara expresar los coeficientes de esa linea podemos recuperar la etapa del Estimador del Modelo del pipeline y de expresar los pesos y el intercept de la funcion.\n\n### Ejercicio 6(d)\n\nEjecuta la celda siguiente y asegurate que entiendes lo que sucede."],"metadata":{}},{"cell_type":"code","source":["# The intercept is as follows:\nintercept = lrModel.stages[1].intercept\n\n# The coefficents (i.e., weights) are as follows:\nweights = lrModel.stages[1].coefficients\n\n# Create a list of the column names (without PE)\nfeaturesNoLabel = [col for col in datasetDF.columns if col != \"PE\"]\n\n# Merge the weights and labels\ncoefficents = zip(weights, featuresNoLabel)\n\n# Now let's sort the coefficients from greatest absolute weight most to the least absolute weight\ncoefficents.sort(key=lambda tup: abs(tup[0]), reverse=True)\n\nequation = \"y = {intercept}\".format(intercept=intercept)\nvariables = []\nfor x in coefficents:\n    weight = abs(x[0])\n    name = x[1]\n    symbol = \"+\" if (x[0] > 0) else \"-\"\n    equation += (\" {} ({} * {})\".format(symbol, weight, name))\n\n# Finally here is our equation\nprint(\"Linear Regression Equation: \" + equation)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["### Ejercicio 6(e)\n\nAhora estudiaremos como se comportan nuestras predicciones en este modelo. Aplicamos nuestro modelo de regresion lineal para el 20% de los datos que hemos separado del conjunto de datos de entrada. La salida del modelo sera una columna de produccion de electricidad teorica llamada \"Predicted_PE\".\n\n- Ejecuta la siguiente celda\n- Desplazate por la tabla de resultados y observa como los valores de la columna de salida de corriente (PE) se comparan con los valores correspondientes en la salida de potencia predecida  (Predicted_PE)"],"metadata":{}},{"cell_type":"code","source":["# Apply our LR model to the test data and predict power output\npredictionsAndLabelsDF = lrModel.transform(testSetDF).select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\")\n\ndisplay(predictionsAndLabelsDF)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["A partir de una inspeccion visual de las predicciones, podemos ver que estan cerca de los valores reales.\n\nSin embargo, nos gustaria disponer de una medida cientifica exacta de la bondad del modelo de regresion lineal. Para realizar esta medicion, podemos utilizar una metrica de evaluacion como la [Error cuadratico medio](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) para validar nuestro modelo lineal.\n\nRSME se define como: \\\\( RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}\\\\) donde \\\\(y_i\\\\) es el valor observado \\\\(x_i\\\\) es el valor predecido\n\nRMSE es una medida muy habitual para calcular las diferencias entre los valores predecidos por un modelo o un estimador y los valores realmente observados. Cuanto menor sea el RMSE, mejor sera nuestro modelo.\n\nSpark ML Pipeline proporciona diferentes metricas para evaluar modelos de regresion, incluyendo [RegressionEvaluator()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator).\n\nDespues de crerar una instancia de [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator), fijaremos el nombre de la columna objetivo \"PE\" y  el nombre de la columna de prediccion a \"Predicted_PE\". A continuacion, invocaremos el evaluador en las predicciones.\n\n\n### Ejercicio 6(f)\n\nEjecuta la celda siguiente y asegurate que entiendes lo que sucede."],"metadata":{}},{"cell_type":"code","source":["# Now let's compute an evaluation metric for our test dataset\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RegressionEvaluator(predictionCol=\"Predicted_PE\", labelCol=\"PE\", metricName=\"rmse\")\n\n# Run the evaluator on the DataFrame\nrmse = regEval.evaluate(predictionsAndLabelsDF)\n\nprint(\"Root Mean Squared Error: %.2f\" % rmse)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Otra medida de evaluacion estadistica muy util es el coeficiente de determinacion, que se denota \\\\(R ^ 2 \\\\) o \\\\(r ^ 2\\\\) y pronunciado \"R cuadrado\". Es un numero que indica la proporcion de la variacion en la variable dependiente que es predecible a partir de las variables independientes y proporciona una medida de lo bien que los resultados observados son replicados por el modelo, basado en la proporcion de la variacion total de los resultados explicada por el modelo. El coeficiente de determinacion va de 0 a 1 (mas cerca a 1), y cuanto mayor sea el valor, mejor es nuestro modelo.\n\n\nPara calcular \\\\(r^2\\\\), hemos de ejecutar el evaluador `regEval.metricName: \"r2\"`\n\n### Ejercicio 6(g)\n\nEjecuta la celda siguiente y asegurate que entiendes lo que sucede."],"metadata":{}},{"cell_type":"code","source":["# Now let's compute another evaluation metric for our test dataset\nr2 = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"r2: {0:.2f}\".format(r2))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["En general, suponiendo una distribucion Gaussiana de errores, un buen modelo tendra 68% de las predicciones dentro de 1 RMSE y 95% dentro de 2 RMSE del valor real (ver http://statweb.stanford.edu/~susan/courses/s60/split/node60.html).\n\nVamos a examinar las predicciones y ver si un RMSE de 4,59 cumple este criterio.\n\nCrearemos un nuevo DataFrame usando [selectExpr()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr) para generar un conjunto de expresiones SQL, y registrar el DataFrame como una tabla de SQL utilizando [registerTempTable()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable).\n\n### Ejercicio 6(h)\n\nEjecuta la celda siguiente y asegurate que entiendes lo que sucede."],"metadata":{}},{"cell_type":"code","source":["# First we remove the table if it already exists\nsqlContext.sql(\"DROP TABLE IF EXISTS Power_Plant_RMSE_Evaluation\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/Power_Plant_RMSE_Evaluation\", True)\n\n# Next we calculate the residual error and divide it by the RMSE\npredictionsAndLabelsDF.selectExpr(\"PE\", \"Predicted_PE\", \"PE - Predicted_PE Residual_Error\", \"(PE - Predicted_PE) / {} Within_RSME\".format(rmse)).registerTempTable(\"Power_Plant_RMSE_Evaluation\")"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["Podemos utilizar sentencias SQL para explorar la tabla `Power_Plant_RMSE_Evaluation`. En primer lugar vamos a ver que datos en la tabla utilizando una sentencia SELECT de SQL.\n\n### Exercise 6(i)\n\nEjecuta la celda siguiente y asegurate que entiendes lo que sucede."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * from Power_Plant_RMSE_Evaluation"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["Ahora podemos mostrar el RMSE como un histograma.\n\n### Ejercicio 6(j)\n\nEjecuta los siguientes pasos:\n\n- Ejecuta la siguiente celda\n- Haz clic en el menu desplegable junto al icono \"Bar chart\" y selecciona \"Histogram\" para convertir la tabla en un histograma\n- Haz clic en \"Plot Options...\"\n- En la caja \"All Fields:\", haz clic \"&lt;id&gt;\" y arrastralo dentro de la caja \"Keys:\"\n- Cambia el valor \"Aggregation\" a \"COUNT\"\n- Aplicar los cambios haciendo clic en el boton Aplicar\n- Aumentar el tamano del grafico haciendo clic y arrastrando el control del tamano\n\nObserva que el histograma muestra claramente que el RMSE se centra alrededor de 0 con la gran mayoria de errores dentro de 2 RMSE."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Now we can display the RMSE as a Histogram\nSELECT Within_RSME  from Power_Plant_RMSE_Evaluation"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["Usando una instruccion SELECT de SQL un poco mas compleja, podemos contar el numero de predicciones dentro de + o - 1,0 y + o - 2,0 y luego mostrar los resultados como un grafico circular.\n\n### Ejercicio 6(k)\n\nEjecuta los siguientes pasos:  \n  - Ejecutar la siguiente celda\n  - Haz clic en el menu desplegable junto al icono de \"Bar chart\" y selecciona \"Pie\" para convertir la tabla en un grafico de sectores\n  - Aumentar el tamano del grafico haciendo clic y arrastrando el control del tamano"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1\n            when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3\n       end RSME_Multiple, COUNT(*) AS count\nFROM Power_Plant_RMSE_Evaluation\nGROUP BY case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1  when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3 end"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["### Conclusiones\nA partir del pie chart, podemos ver que el 68% de nuestras predicciones de datos de prueba estan a 1 RMSE de los valores reales, y el 97% (68% + 29%) de nuestras predicciones de datos de prueba se encuentran a 2 RMSE. Por lo que el modelo es bastante decente."],"metadata":{}},{"cell_type":"markdown","source":["##Parte 7: Ajustar y evaluar\n\nAhora que tenemos un primer modelo bastante bueno vamos a tratar de hacer uno aun mejor ajustando sus parametros. El proceso de ajustar un modelo se conoce como [Model Selection](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning) o [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning). Spark ML Pipeline hace que el proceso de ajuste sea sencillo.\n\nSpark ML Pipeline soporta la seleccion de modelos usando herramientas herramientas como el [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator), que requiere los siguientes elementos:\n- [Estimator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Estimator): un algoritmo o un pipeline a ajustar\n- [Conjunto de ParamMaps](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder): parametros para elegir, tambien conocido como _parameter grid_\n- [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator): metrica para medir que tan bien lo hace un modelo sobre los datos de entrenamiento\n\nA un alto nivel, las herramientas de seleccion de modelos, tales como [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) trabajan de la siguiente manera:\n\n- Se separaran los datos de entrada en dos conjuntos entrenamiento y test.\n- Para cada uno de estos pares (entrenamiento, test), hay iterar a traves del conjunto de ParamMaps:\n    - Para cada [ParamMap](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder), se ajusta el [Estimador](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Estimator) usando dichos parametros, se obtiene el modelo ajustado, y se evaluar su rendimiento usando el [Evaluator] (https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator).\n    - Seleccionan el mejor modelo producido por el conjunto de parametros.\n\nEl [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator) puede ser por ejemplo un [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) para problemas de regresion. Como ayuda a construir el conjunto de parametros, los usuarios pueden utilizar la utilidad [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder).\n\nTen en cuenta que la validacion cruzada sobre una conjunto grande de parametros es costosa.\n\nEn el siguiente apartado llevaremos a cabo los siguientes pasos:\n- Crear un [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) utilizando un pipeline y un [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) que hemos creado anteriormente, y establecer el numero de pliegues (folds) a 5\n- Crear una lista de 10 parametros de regularizacion\n  - Crear una lista de 5 parametros de numero maximo de iteraciones\n- Usar [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) para construir un conjunto de parametros con los parametros de regularizacion y numero de iteraciones y anadir dicho conjunto al [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n- Ejecutar el [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) para encontrar los parametros que producen el mejor modelo (es decir, mas bajo RMSE) y devolver el mejor modelo."],"metadata":{}},{"cell_type":"code","source":["#TODO: Find the best parameter combination for a linear regression model\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# We can reuse the RegressionEvaluator, regEval, to judge the model based on the best Root Mean Squared Error\n# Let's create our CrossValidator with 3 fold cross validation\ncrossval = CrossValidator(estimator=lrPipeline, evaluator=regEval, numFolds=3)\n\n# Let's tune over our regularization parameter from 0.01 to 0.10\nregParam = [x / 100.0 for x in range(1, 11)]\n\n# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, regParam)\n             .build())\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ncvModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["Ahora que ya hemos ajustado nuestro modelo de regression, vamos a ver cuales son los nuevos valores de RMSE y \\\\(r^2\\\\) que hemos obtenido en comparacion con los anteriores.\n\n### Ejercicio 7(b)\n\nCompleta y ejecuta la siguiente celda"],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n# Now let's use cvModel to compute an evaluation metric for our test dataset: testSetDF\npredictionsAndLabelsDF = <FILL_IN>\n\n# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\nrmseNew = <FILL_IN>\n\n# Now let's compute the r2 evaluation metric for our test dataset\nr2New = <FILL_IN>\n\nprint(\"Original Root Mean Squared Error: {0:2.2f}\".format(rmse))\nprint(\"New Root Mean Squared Error: {0:2.2f}\".format(rmseNew))\nprint(\"Old r2: {0:2.2f}\".format(r2))\nprint(\"New r2: {0:2.2f}\".format(r2New))"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["# TEST\nTest.assertEquals(round(rmse, 2), 4.59, \"Incorrect value for rmse\")\nTest.assertEquals(round(rmseNew, 2), 4.59, \"Incorrect value for rmseNew\")\nTest.assertEquals(round(r2, 2), 0.93, \"Incorrect value for r2\")\nTest.assertEquals(round(r2New, 2), 0.93, \"Incorrect value for r2New\")"],"metadata":{},"outputs":[],"execution_count":77}],"metadata":{"name":"Lab_2_ML_pipeline","notebookId":2706105638400394},"nbformat":4,"nbformat_minor":0}
