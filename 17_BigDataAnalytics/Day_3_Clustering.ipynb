{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n#Practica sobre como usar Machine learning no supervisado (clustering) para realizar una segmentacion comportamental\n\nEsta practica simula un ejercicio completo sobre como generar grupos (clusters) de clientes usando un dataset de usuarios y ratings liberados por una empresa de streaming multimedia bajo demanda por Internet.\n\n**Este notebook cubre:**\n* *Parte 1:* ETL de los ficheros (3 puntos)\n* *Parte 2:* Ingenieria de caracteristicas (3 puntos)\n* *Parte 3:* Clustering (2 puntos)\n* *Parte 4:* Validacion de los resultados (2 punto)\n* *Parte 5:* Normalizacion de los datos --Opcional--\n\n**IMPORTANTE:** Usar un cluster de spark 2.0 para realizar esta PEC"],"metadata":{}},{"cell_type":"markdown","source":["## Parte 1: ETL de los ficheros\n\n** Resumen **\n\nLos archivos que usaremos contienen 1.000.209 calificaciones anonimas de aproximadamente 3.900 peliculas Realizado por 6.040 usuarios realizadas durante el ano 2000.\n\n** Fichero de calificaciones **\n\nTodas las calificaciones estan contenidas en el archivo \"ratings.dat\" y estan en el siguiente formato:\n\n*UserID::MovieID::Rating::Timestamp*\n\n- UserIDs: cuyo rango se encuentra entre 1 y 6040\n- MovieIDs: cuyo rango se encuentra entre 1 y 3952\n- Rating: las calificaciones se realizan en una escala de 5 estrellas (sin decimales)\n- Timestamp: la marca de tiempo se representa en segundos\n\nNota: Cada usuario tiene al menos 20 calificaciones\n\n** Fichero de usuarios **\n\nLa informacion sobre los usuarios esta contenida en el fichero \"users.dat\" y estan en el siguiente formato:\n\n*UserID::Gender::Age::Occupation::Zip-code*\n\nToda la informacion demografica es proporcionada voluntariamente por los usuarios y no se comprueba su exactitud. Solo los usuarios que hayan proporcionado datos demograficos se incluyen en este conjunto de datos.\n\n- Gender: El genero se denota por \"M\" para hombres y \"F\" para mujeres\n- Edad: Esta distibuida en los siguientes rangos:\n\n\t*  1:  \"- 18\"\n\t* 18:  \"18-24\"\n\t* 25:  \"25-34\"\n\t* 35:  \"35-44\"\n\t* 45:  \"45-49\"\n\t* 50:  \"50-55\"\n\t* 56:  \"56 - \"\n\n- Profesion: Se elige de las siguientes opciones:\n\n\t*  0:  \"other\" or not specified\n\t*  1:  \"academic/educator\"\n\t*  2:  \"artist\"\n\t*  3:  \"clerical/admin\"\n\t*  4:  \"college/grad student\"\n\t*  5:  \"customer service\"\n\t*  6:  \"doctor/health care\"\n\t*  7:  \"executive/managerial\"\n\t*  8:  \"farmer\"\n\t*  9:  \"homemaker\"\n\t* 10:  \"K-12 student\"\n\t* 11:  \"lawyer\"\n\t* 12:  \"programmer\"\n\t* 13:  \"retired\"\n\t* 14:  \"sales/marketing\"\n\t* 15:  \"scientist\"\n\t* 16:  \"self-employed\"\n\t* 17:  \"technician/engineer\"\n\t* 18:  \"tradesman/craftsman\"\n\t* 19:  \"unemployed\"\n\t* 20:  \"writer\"\n    \n** Fichero de peliculas **\n\nLa informacion se ecuentra en el archivo \"movies.dat\" y esta en el siguiente\nformato:\n\n*MovieID::Title::Genres*\n\n- Title: Los titulos son identicos a los titulos proporcionados por la IMDB (incluyendo ano de lanzamiento)\n- Genres: Los generos estan separados por *pipes* y se seleccionan de la siguiente lista:\n    * Action\n\t* Adventure\n\t* Animation\n\t* Children's\n\t* Comedy\n\t* Crime\n\t* Documentary\n\t* Drama\n\t* Fantasy\n\t* Film-Noir\n\t* Horror\n\t* Musical\n\t* Mystery\n\t* Romance\n\t* Sci-Fi\n\t* Thriller\n\t* War\n\t* Western\n    \n- Algunos MovieIDs no corresponden a una pelicula debido a un duplicado accidental a su entradas y/o a entradas de prueba\n- Las peliculas se introducen principalmente a mano, por lo que pueden existir errores e incoherencias"],"metadata":{}},{"cell_type":"markdown","source":["**Ejercicio 1(a):** Empezaremos por visualizar una muestra de los datos. Para esto usaremos las funciones pre-definidas en los notebooks de Databricks para explorar su sistema de archivos.\n\nUsar `display(dbutils.fs.ls(\"/databricks-datasets/cs110x/ml-1m/data-001\")` para listar los ficheros del directorio%md descripcion de los datos que se van a usar y decir que los vamos a cargar"],"metadata":{}},{"cell_type":"code","source":["#TODO: use display to list all the files of the directory containing the data\n<FILL IN>\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Luego usaremos el comando `print dbutils.fs.head` para visualizar el contenido de los ficheros \"movies.dat\", \"user.dat\" y \"ratings.dat\""],"metadata":{}},{"cell_type":"code","source":["#TODO: use dbutils.fs.head to inspect the file \"user.dat\"\n<FILL IN>"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#TODO: use dbutils.fs.head to inspect the file \"ratings.dat\"\n<FILL IN>"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#TODO: use dbutils.fs.head to inspect the file \"movies.dat\"\n<FILL IN>\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Ejercicio 1(b):\n\nCarga los diferentes ficheros (usuarios, peliculas y calificaciones) en tres rdds para poder procesarlos posteriormente."],"metadata":{}},{"cell_type":"code","source":["# TODO: Load the users data and print the first five lines.\nrawUsersTextRdd = <FILL IN>\nprint <FILL IN>"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# TODO: Load the ratings data and print the first five lines.\nrawRatingsTextRdd = <FILL IN>\nprint <FILL IN>"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# TODO: Load the movies data and print the first five lines.\nrawMoviesTextRdd = <FILL IN>\nprint <FILL IN>"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Ejercicio 1(c): \nComo has observado inspecionando los ficheros, el fichero de usuarios contiene algunas lineas con mas de un zip-code. Para simplificar esta PEC, eliminaremos estas lineas del fichero. Para eso crearemos una funcion que elimine todas las lineas que contenga algun caracter que no sea '0123456789:MF'."],"metadata":{}},{"cell_type":"code","source":["#TODO: Complete the function invalidLine\ndef invalidLine(line):\n    \"\"\"Verifies if a line is valid to be converted to a dataframe.\n    Args:\n        line (str): A string.\n\n    Returns:\n        boolean: True if valid, False otherwise.\n    \"\"\"\n  <FILL IN>"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Load in the testing code and check to see if your answer is correct\n# If incorrect it will report back '1 test failed' for each failed test\n# Make sure to rerun any cell you change before trying the test again\nfrom databricks_test_helper import Test\n# TEST invalidLine (1b)\nTest.assertEquals(invalidLine('161::M::45::16::98107-2117'), False, 'incorrect result: invalidLine does not remove the incorrect lines')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Exercicio 1(d)\n\nComo en la PEC anterior, crea un esquema a medida para cada uno de los ficheros. Puedes usar un codigo muy similar al de la PEC2 para esto."],"metadata":{}},{"cell_type":"code","source":["#TODO: Fill in the user schema.\nfrom pyspark.sql.types import *\n\n# Custom Schema for users\nuserSchema = StructType([ \\\n    <FILL IN>\n                          ])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Custom Schema for ratings\nratingsSchema = StructType([ \\\n    <FILL IN>\n                           ])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Custom Schema for movies\nmoviesSchema = StructType([ \\\n    <FILL IN>\n                          ])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Ejercicio 1(d)\n\nElimina las lineas no validas contenidas en rawUsersTextRdd."],"metadata":{}},{"cell_type":"code","source":["#TODO: filter invalid lines\nfilteredUsersTextRdd = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# TEST filter invalidLines (1d)\ncounter = filteredUsersTextRdd.count()\nTest.assertEquals(counter, 5974, 'incorrect result: lines are incorrectly filtered')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Ejercicio 1(d)\n\nPara poder usar facilmente diferentes algoritmos de clustering es conveniente que todos los atributos sean numericos, para esto convertiremos el atributo gender de los usuarios en 1 si es hombre ('M') o 0 si es mujer ('F'), cambia en el esquema que has creado anteriormente el atributo gener a `IntegerType()` si lo tenias como char o string.\n\nLuego transforma el RDD en un DataFrame usando la funcion [toDF()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext) indicandole que schema debe usar como un parametro de la funcion (`schema= customSchema`).\n\nPara realizar esta transformacion a DataFrame, primero hay que convertir cada linea en una lista de enteros, ya que asi se indica en nuestro esquema. Para esto usaremos las funciones de Python [split()](https://docs.python.org/2/library/stdtypes.html#str.split) y [int()](https://docs.python.org/2/library/functions.html#int). Puedes combinar estas dos transformaciones junto con la conversion del atributo gender en la misma funcion lambda o hacer una funcion con nombre. Luego usa una funcion map para aplicar los cambios a todas las lineas del RDD de usuarios.\n\nFinalmente, comprueba que los datos mostrados por el comando `display(usersDF)` son correctos."],"metadata":{}},{"cell_type":"code","source":["#TODO: transform the Gender type and create a DataFrame\nintegerGendersUsersTextRdd = filteredUsersTextRdd.<FILL IN>\nusersDF = <FILL IN>\n\ndisplay(usersDF)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Ejercicio 1(e)\n\nAhora aplica las mismas transformaciones, menos las del atributo gender, al RDD de ratings."],"metadata":{}},{"cell_type":"code","source":["#TODO: Create a DataFrame for ratings RDD\nratingsDF = <FILL IN>\n\ndisplay(ratingsDF)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Parte 2: Ingenieria de caracteristicas\n\nLa ingenieria de la caracteristicas es el proceso de utilizar el conocimiento del dominio de los datos para crear las caracteristicas que hacen que los algoritmos de machine learning trabajen de forma correcta. La ingenieria de la caracteristica es fundamental en la aplicacion del machine learning, y en general es dificil y costosa.vLa ingenieria de caracteristicas es un tema informal, pero se considera esencial en el machine learning aplicado."],"metadata":{}},{"cell_type":"markdown","source":["## Ejercicio 2(a)\n\nPara esta PEC calcularemos unas cuantas caracteristicas muy sencillas como el numero de peliculas vistas por cada usuario, la media de sus calificaciones y su varianza.\n\nPara obtener estas caracteristicas usaremos la funcion [groupBy()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) de los DataFrames de Spark, asi como las funciones de agregacion que nos proporciona SparkQL (https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.functions).\n\nPor simplicidad usaremos la funcion [alias()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext) para poner los siguientes nombres a las columnas: NumMovies, AvgRating y VarRating."],"metadata":{}},{"cell_type":"code","source":["#TODO: Compute aggregated values by user using groupBy and SQL functions\nimport pyspark.sql.functions as func\n\naggRatingsDF = <FILL IN>\n\ndisplay(aggRatingsDF)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# TEST compute aggregated features (2a)\ncounter = aggRatingsDF.count()\nresult = aggRatingsDF.where(aggRatingsDF.UserID == 31).first()\nTest.assertEquals(counter, 6040, 'incorrect result: aggregation is incorreclty done')\nTest.assertEquals(result[\"NumMovies\"], 119, 'incorrect result: NumMovies is incorrect')\nTest.assertEquals(result[\"AvgRating\"], 3.73109243697479, 'incorrect result: AvgRating is incorrect')\nTest.assertEquals(result[\"VarRating\"], 1.0457199829084174, 'incorrect result: NumMovies is incorrect')\nprint result"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["##Ejercicio 2(b)\n\nAhora juntaremos las caracteristicas que hemos extraido del fichero de ratings con el fichero de usuarios. Como no queremos tener que preocuparnos por los valores nulos en las caracteristicas, solo nos quedaremos con los usuarios que hayan calificado alguna pelicula. Para realizar esto, usaremos un inner join. Encontraremos los detalles de la funcion aqui [join()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html)."],"metadata":{}},{"cell_type":"code","source":["#TODO: Compute an inner join between usersDF and aggRatingsDF\njoinedDF = <FILL IN>\ndisplay(joinedDF)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# TEST join (2b)\ncounter = joinedDF.count()\nTest.assertEquals(counter, 5974, 'incorrect result: join is incorreclty done')\nprint counter"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["## Ejercicio 2(c)\n\nAhora vamos a guardar los datos generados para poder reutilizarlos en un futuro sin tener que re-ejecutar todo el notebook, para esto ejecutaremos el siguiente codigo."],"metadata":{}},{"cell_type":"code","source":["#TODO: Read, understand and execute the cell\nsqlContext.sql(\"DROP TABLE IF EXISTS joinedDF\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/joinedDF\", True)\nsqlContext.registerDataFrameAsTable(joinedDF, \"joinedDF\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Parte 3: Clustering\n\nUn algoritmo de agrupamiento (en ingles, clustering) es un procedimiento de agrupacion de una serie de vectores de acuerdo con un criterio. Esos criterios son por lo general distancia o similitud. La cercania se define en terminos de una determinada funcion de distancia, como la euclidea, aunque existen otras mas robustas o que permiten extenderla a variables discretas.\n\nExisten dos grandes familias de clustering:\n\n* *Agrupamiento jerarquico*, que puede ser aglomerativo o divisivo.\n* *Agrupamiento no jerarquico*, en los que el numero de grupos se determina de antemano y las observaciones se van asignando a los grupos en funcion de su cercania. En esta familia existen una gran cantidad de metodos, en esta PEC usaremos el metodos de k-means (k-medias).\n\n### k-means\n\nK-means es un metodo de clustering, que tiene como objetivo la particion de un conjunto de _n_ observaciones en _k_ grupos en el que cada observacion pertenece al grupo cuyo valor medio es mas cercano. Una de las ventajas de este metodo es que la agrupacion del conjunto de datos puede ilustrarse en una particion del espacio de datos en [celdas de Voronoi](https://es.wikipedia.org/wiki/Pol%C3%ADgonos_de_Thiessen#Diagramas_de_Voron.C3.B3i_en_el_plano_euclidiano_.7F.27.22.60UNIQ--postMath-00000001-QINU.60.22.27.7F).\n\nEl problema es computacionalmente dificil (NP-hard). Sin embargo, hay heuristicas muy eficientes que se emplean comunmente y convergen rapidamente a un optimo local.\n\nEl algoritmo de k-means mas comun utiliza una tecnica de refinamiento iterativo, tal y como se describe a continuacion:\n\nDado un conjunto inicial de _k_ centroides $$ m_1^{(1)},...,m_k^{(1)} $$ el algoritmo continua alternando entre estos dos pasos:\n\n* *Paso de asignacion:* Asigna cada observacion al grupo con la media mas cercana (es decir, la particion de las observaciones de acuerdo con el diagrama de Voronoi generado por los centroides).\n\n$$ S_{i}^{(t)} = \\\\{ x_p: || x_p - m_i^{(t)} || \\leq || x_p - m_j^{(t)} || \\forall 1 \\leq j \\leq k \\\\} $$\n\n* *Paso de actualizacion:* Calcular los nuevos centroides como el centroide de las observaciones en el grupo.\n$$ m_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum^{x_j \\in S_i^{(t)}} x_j$$\n\nEl algoritmo se considera que ha convergido cuando las asignaciones ya no cambian. Los centroides suelen iniciarse de forma aleatoria."],"metadata":{}},{"cell_type":"markdown","source":["El siguiente paso es preparar los datos para aplicar el k-means. Dado que todo el dataset es numerico y consistente, esta sera una tarea sencilla y directa.\n\nEl objetivo es utilizar el metodo de clustering para determinar _k_ usuarios estandar (promedio) que representen a la totalidad de los usuarios que tenemos en nuestro dataset. El primer paso en la construccion de nuestro modelo de clustering es convertir las caracteristicas que hemos calculado en nuestro DataFrame a un vector de caracteristicas utilizando el metodo [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n\nEl VectorAssembler es una transformacion que combina una lista dada de columnas en una unico vector. Esta transformacion es muy util cuando queremos combinar caracteristicas en crudo de los datos con otras generadas al aplicar diferentes funciones sobre los datos en un unico vector de caracteristicas. Para integrar en un unico vector toda esta informacion antes de ejecutar un algoritmo de aprendizaje automatico, el VectorAssembler toma una lista con los nombres de las columnas de entrada (lista de strings) y el nombre de la columna de salida (string).\n\n### Ejercicio 3(a)\n\n- leer la documentacion y los ejemplos de uso de [VectorAssembler](https://spark.apache.org/docs/1.6.2/ml-features.html#vectorassembler)\n- Convertir la tabla SQL `joinedDF` en un `dataset` llamado datasetDF usando la funcion table del sqlContext\n- Establecer las columnas de entrada del VectorAssember: `['Age','NumMovies','AvgRating','VarRating']`\n- Establecer la columnas de salida como `\"features\"`"],"metadata":{}},{"cell_type":"code","source":["#TODO: Replace <FILL_IN> with the appropriate code\nfrom pyspark.ml.feature import VectorAssembler\n\ndatasetDF = <FILL IN>\n\nvectorizer = VectorAssembler()\nvectorizer.setInputCols(<FILL IN>)\nvectorizer.setOutputCol(<FILL IN>)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Ejercicio 3(b)\n\nGuarda en cache el dataframe `datasetDF` y llamalo clusteringDF"],"metadata":{}},{"cell_type":"code","source":["#TODO: Let's cache the dataset for performance\nclusteringDF = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["## Ejercicio 3(c)\n\n- Lee la documentacion y los ejemplos de [k-means](https://spark.apache.org/docs/1.6.2/ml-clustering.html#k-means)\n- Ejecuta la siguiente celda"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\n\nkmeans = KMeans()\nprint(kmeans.explainParams())"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["La siguiente celda esta basada en [Spark 2.0.1 ML Pipeline API for clustering](https://spark.apache.org/docs/2.0.1/mllib-clustering.html#k-means).\n\nEl primer paso es establecer los valores de los parametros:\n- Define el numero de clusters (k) como 10\n- Define el numero maximo de iteraciones a 25\n- Define el random seed a 1\n\nAhora, crearemos el [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) (flujo de ejecucion) y estableceremos las fases del pipeline como vectorizar y posteriormente aplicar el algoritmo de clustering que hemos definido.\n\nFinalmente, crearemos el modelo entrenandolo con el DataFrame `clusteringDF`.\n\n### Ejercicio 3(d)\n\n- Lee la documentacion [k-means](https://spark.apache.org/docs/2.0.1/mllib-clustering.html#k-means) documentation\n- Completa y ejecuta la siguiente celda, asegurate de entender que es lo que sucede."],"metadata":{}},{"cell_type":"code","source":["## TODO: Replace <FILL_IN> with the appropriate code\n# Now we set the parameters for the method\nkmeans = KMeans().setK(<FILL IN>).setSeed(<FILL IN>)\n\nclusteringPipeline = Pipeline()\n\n# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nclusteringPipeline.setStages(<FILL IN>)\n\n# Let's train on the entire dataset to see what we get\nclusteringModel = clusteringPipeline.fit(<FILL IN>)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["## Ejercicio 3(e)\n\nEjecuta la siguiente celda y observa los centroides que has obtenido y responde a la siguienes preguntas:\n\n- Crees que los centrodides son significativos?\n- Que crees que ha sucedido?"],"metadata":{}},{"cell_type":"code","source":["# Shows the result.\ncenters = clusteringModel.stages[1].clusterCenters()\nprint(\"Centroids: \")\nfor center in centers:\n    print(center)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["## Parte 4: Validacion de los resultados\n\nAhora estudiaremos como se comportan nuestras predicciones en este modelo.\n\nPara realizar esta medicion, podemos utilizar la misma metrica de evaluacion de la PEC2, el [Error cuadratico medio](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE).\n\nRecordar que el RSME se define como: \\\\( RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}\\\\) donde \\\\(y_i\\\\) es el valor observado \\\\(x_i\\\\) es el valor predecido\n\nRMSE es una medida habitual en clustering para calcular las diferencias entre los valores predecidos (centroides) y los elementos que componen ese cluster. Cuanto menor sea el RMSE, mejor sera nuestro metodo de clustering."],"metadata":{}},{"cell_type":"markdown","source":["## Ejercicio 4(a)\n\nEl modelo de k-means de Spark proporciona un metodo para calcular la suma de las distancias al cuadrado de todos los elementos de un DataFrame a su centroide mas cercano, conocido como SSE (Sum of Square Errors). Con este valor vamos a calcular el RMSE."],"metadata":{}},{"cell_type":"code","source":["## TODO: Replace <FILL_IN> with the appropriate code\n\nkmeansModel = clusteringModel.stages[1]\ntransformedDF = clusteringModel.stages[0].transform(clusteringDF)\n\n#Obtain the SSE\nSSE = kmeansModel.computeCost(<FILL IN>)\n\n#obtain the number of elements of the DataFrame clusteringDF\nn = <FILL IN>\n\n#compute RMSE\nRMSE = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["#TEST RMSE 4(a)\nTest.assertEquals(n, 5974, 'incorrect result: number of elements is wrong')\nTest.assertEquals(round(RMSE,3), 29.437, 'incorrect result: RMSE error')\n"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## Ejercicio 4(b)\n\nInspecciona visualmente si los centroides predecidos de algunos de los elementos de clusteringDF estan cerca de los valores reales y si tienene sentido, luego responde las siguientes preguntas:\n\n- Como afecta el no haber normalizado los datos? \n- Hay algun atributo mas importante que los demas? Si la respuesta es afirmativa, Cual?\n\nEjecuta la siguiente celda para ver en que cluster se han asignado cada uno de los registros del DataFrame. Los identificadores de cluster empiezan por 0."],"metadata":{}},{"cell_type":"code","source":["display(kmeansModel.transform(transformedDF).select(['Age','NumMovies','AvgRating','VarRating','prediction']))"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["## Parte 5: Normalizacion de los datos --Opcional--\n\nUna buena practica cuando usamos metodos de machine learning basados en distancias es asegurarnos que todos los atributos estan en la misma escala. Esto no sucede en nuestro dataset ya que cada atributo tiene un rango diferente.\n\nPrueba las siguientes opciones de normalizacion y re-ejecuta el notebook con los datos normalizados:\n\n* **normalizacion:** Convierte todos los atributos al rango [0,1] usando la siguiente formula\n$$ x' = \\frac{x - min}{max-min} $$\n\n* **standarizacion (z-scores):** Convierte todos los atributos en una distribucion normal con media = 0 y varianza = 1 usando la siguiente formula\n$$ z = \\frac{x - \\mu}{\\sigma}$$\n\n- Tienen ahora los clusters mas sentido?\n- El Error se ha reducido?"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":54}],"metadata":{"name":"PEC3_Clustering","notebookId":672025954145537},"nbformat":4,"nbformat_minor":0}
