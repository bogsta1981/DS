{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# **Apache Hadoop (MapReduce)**\n",
    "\n",
    "![Hadoop Logo](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Hadoop_logo.svg/220px-Hadoop_logo.svg.png)\n",
    "\n",
    "It is an open source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures (of individual machines, or racks of machines) are common and thus should be automatically handled in software by the framework.\n",
    "\n",
    "The core of Apache Hadoop consists of a storage part (Hadoop Distributed File System (HDFS)) and a processing part (MapReduce). Hadoop splits files into large blocks and distributes them amongst the nodes in the cluster. To process the data, Hadoop MapReduce transfers packaged code for nodes to process in parallel, based on the data each node needs to process. This approach takes advantage of data locality — nodes manipulating the data that they have on hand — to allow the data to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are connected via high-speed networking.\n",
    "\n",
    "![caption](./images/data_sharing_mapreduce.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ** Apache Spark**\n",
    "\n",
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)\n",
    "\n",
    "Apache Spark is an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's multi-stage in-memory primitives provides performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.\n",
    "\n",
    "![caption](./images/data_sharing_spark.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Spark Driver and Workers**\n",
    "\n",
    "* A Spark code runs two different programs:\n",
    "    * A driver program and several workers programs\n",
    "* The driver program runs in a given gateway node\n",
    "* Worker programs run on cluster nodes or in local threads\n",
    "* RDDs are distributed  across workers\n",
    "\n",
    "![caption](./images/driver_workers_spark_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Spark Context**\n",
    "\n",
    "* A Spark program first creates a SparkContext object\n",
    "    * Tells Spark how and where to access a cluster\n",
    "    * pySpark shell automatically creates the sc variable\n",
    "    * Notebooks and programs must use a constructor to create a new SparkContext\n",
    "* Use SparkContext to create RDDs\n",
    "\n",
    "* The master parameter for a SparkContext determines which type and size of cluster to use\n",
    "\n",
    "In the Databricks cloud, the SparkContext is already created for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Resilient Distributed Datasets (RDD)**\n",
    "\n",
    "* The primary abstraction in Spark\n",
    "    * Immutable once constructed\n",
    "    * Track lineage information to efficiently recompute lost data\n",
    "    * Enable operations on collection of elements in parallel\n",
    "\n",
    "* You construct RDDs\n",
    "    * by parallelizing existing Python collections (lists)\n",
    "    * by transforming an existing RDDs\n",
    "    * from files in HDFS or any other storage system  \n",
    "    \n",
    "* Programmer specifies number of partitions for an RDD (Default value used if unspecified)\n",
    "    * In general:  more partitions = more parallelism (be carefull with the number of nodes / threads)\n",
    "    \n",
    "\n",
    "![caption](./images/RDD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Two types of operations: transformations and actions\n",
    "    * Transformations are lazy (not computed immediately)\n",
    "    * Transformed RDD is executed when action runs on it\n",
    "    * Persist (cache) RDDs in memory or disk\n",
    "\n",
    "* Example: \n",
    "    * Create an RDD from a data source: from a python list (sc.parallelize()) or a file (sc.textFile())\n",
    "    * Apply transformations to an RDD: map filter\n",
    "    * Apply actions to an RDD: collect\n",
    " \n",
    "![caption](./images/RDD_actions.png)\n",
    "\n",
    "* Further information about RDD: http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'cats']\n"
     ]
    }
   ],
   "source": [
    "#Creation\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList)\n",
    "#Filter\n",
    "FilteredWordsRDD = wordsRDD.filter(lambda w: w != \"rat\")\n",
    "#map\n",
    "FilteredPluralWordsRDD = FilteredWordsRDD.map(lambda w: w + \"s\" )\n",
    "# Collect\n",
    "print FilteredPluralWordsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **SparkTransformations**\n",
    "\n",
    "* Create new datasets from an existing one\n",
    "* Use lazy evaluation: results not computed right away – instead Spark remembers set of transformations applied to base dataset\n",
    "    * Spark optimizes the required calculations\n",
    "    * Spark recovers from failures and slow workers\n",
    "\n",
    "* Hint: Think of this as a recipe for creating result\n",
    "\n",
    "![caption](./images/transformations.png)\n",
    "\n",
    "* Difference between ``map`` and ``flatMap``: \n",
    "    - ``map`` transforms an RDD of length N into another RDD of length N\n",
    "    - ``flatMap`` transforms an RDD of length N into a collection of N collections, then flattens these into a single RDD of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [2, 4], [3, 9]]\n"
     ]
    }
   ],
   "source": [
    "#flatmap example\n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "squaredRdd = rdd.flatMap(lambda x: [[x, x**2]])\n",
    "print squaredRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Spark Actions**\n",
    "\n",
    "*  Cause Spark to execute recipe to transform source\n",
    "*  Mechanism for getting results out of Spark\n",
    "\n",
    "![caption](./images/actions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "#takeOrdered example\n",
    "rdd = sc.parallelize([5,3,1,2])\n",
    "print rdd.takeOrdered(3, lambda s: -1 * s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Review: Python lambda Functions ** \n",
    "\n",
    "* Small anonymous functions (not bound to a name)\n",
    "    * lambda a, b: a + b => returns the sum of its two arguments\n",
    "* Can use lambda functions wherever function objects are required\n",
    "* Restricted to a single expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 5\n",
    "b = 3\n",
    "\n",
    "f = lambda a, b: a + b\n",
    "\n",
    "f(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Caching RDDs**\n",
    "\n",
    "* You can mark an RDD to be persisted using the _persist()_ or _cache()_ methods on it \n",
    "* The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:392"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache() # save, don't recompute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Spark Program Lifecycle**\n",
    "\n",
    "1. Create RDDs from external data or parallelize a collection in your driver program\n",
    "2. Lazily transform them into new RDDs\n",
    "3. _cache( )_ some RDDs for reuse\n",
    "4.  Perform actions to execute parallel computation and produce results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Spark Key-Value RDDs**\n",
    "\n",
    "* Similar to Map Reduce, Spark supports Key-Value pairs\n",
    "* Each element of a Pair RDD is a pair tuple ex. [(1, 2), (3, 4)]\n",
    "* Some Key-Value Transformations\n",
    "* <font color=\"red\"> Be careful using _groupByKey()_ as it can cause a lot of data movement across the network and create large Iterables at workers </font>\n",
    "\n",
    "![caption](./images/key-value_transformations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (3, 10)]\n",
      "[(1, 'a'), (1, 'b'), (2, 'c')]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,2), (3,4), (3,6)])\n",
    "reducedRdd = rdd.reduceByKey(lambda a, b: a + b)\n",
    "print reducedRdd.collect()\n",
    "\n",
    "rdd = sc.parallelize([(1,'a'), (2,'c'), (1,'b')])\n",
    "sortedRdd = rdd.sortByKey()\n",
    "print sortedRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **pySpark Shared Variables**\n",
    "\n",
    "\n",
    "* Spark automatically creates closures (functions with its own envirnment) when\n",
    "    * Functions that run on RDDs at workers (ex. map function)\n",
    "    * Any global variables used by those workers\n",
    "\n",
    "* Nice, but why? No communication between workers is required\n",
    "\n",
    "* Problems:\n",
    "    * Changes to global variables at workers are not sent to driver (one way closures: from driver to workers). Imagine that you want to count missing values at the workers and obtain the total number of missing values\n",
    "    * Even worse -> Iterative or single jobs with large global variables send large read-only lookup table to workers. ex. A large feature vector in a ML algorithm (very Inefficient)\n",
    "    \n",
    "* Solution:\n",
    "    * Broadcast Variables\n",
    "        * Efficiently send large, read-only value to all workers (distributed using efficient broadcast algorithms)\n",
    "        * Saved at workers for use in one or more Spark operations\n",
    "        * Like sending a large, read-only lookup table to all the nodes\n",
    "    * Accumulators (Types: integers, double, long, float)\n",
    "        * Aggregate values from workers back to driver\n",
    "        * Only driver can access value of accumulator (Tasks at workers cannot access accumulator’s values)\n",
    "        * For tasks, accumulators are write-only\n",
    "        * Accumulators can be used in actions or transformations, but remember only actions guarantee its execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Broadcast variable\n",
    "\n",
    "#At the driver:\n",
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "#At a worker (in code passed via a closure)\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accumulator variable\n",
    "\n",
    "#At the driver\n",
    "accum = sc.accumulator(0)\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "#At the worker\n",
    "def f(x):\n",
    "    global accum #global (like python global variables)\n",
    "    accum += x\n",
    "\n",
    "rdd.foreach(f)\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
