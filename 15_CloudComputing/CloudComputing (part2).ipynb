{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Francesc Dant√≠ (fdanti@ub.edu)\n",
    "# Cloud Computing (part 2)\n",
    "\n",
    "In this second part of the lesson we will focus on how to work with **big files** and how to avoid **slowdowns** due to Operating System (OS) **memory system**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Memory usage\n",
    "\n",
    "Something important to be considered in our programming model is memory usage.  \n",
    "\n",
    "Memory is where our CPU stores all instructions and data that needs to operate. In a computer, memory is a hierarchical structure wirh four levels:  \n",
    "\n",
    "+ Internal. Processor registers and cache. Are \"as fast as expensive\".\n",
    "+ Main. Outside the cpu, we have RAM. It's cheaper and pretty fast; in most of cases we have some Gb in our computers.\n",
    "+ On-line mass storage. Some Operating systems includes a swap space in the HDD.\n",
    "\n",
    "The computer will store most used data \"as near as possible\". If the nearest level is full, it will use next level and so on.  \n",
    "So, overloading our computer RAM means that OS is going to use Swap, that means a signifficant slowdown of the system. \n",
    "\n",
    "How can we work with multiple Gb files in a computer with few Gb memory? We must change our programming model to be able to work with **Big Data**!\n",
    "\n",
    "First of all, let's see how do we work with small files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With small files, we can load them into memory at once.\n",
    "Let's load a CSV file contining few thousands crime records in Sacramento. With Pandas read.csv() method, we are able to load an entire csv file into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import time as tm\n",
    "import pandas as pd\n",
    "\n",
    "#We use a sample CSV file from samplecsvs website.\n",
    "# It contains a ten thousand registers (1.3Mb)\n",
    "\n",
    "#Get the file from internet\n",
    "fileURL = \"http://samplecsvs.s3.amazonaws.com/SacramentocrimeJanuary2006.csv\"\n",
    "fileName = \"SacramentocrimeJanuary2006.csv\"\n",
    "urllib.request.urlretrieve(fileURL,fileName)\n",
    "\n",
    "csvFile = \"SacramentocrimeJanuary2006.csv\"\n",
    "\n",
    "#Let's store a timestamp\n",
    "t0 = tm.time() \n",
    "\n",
    "#We put all data in a DataFrame. That is: loaded ~1000 rows in memory\n",
    "data = pd.read_csv(csvFile)\n",
    "\n",
    "#And we store the timestamp again:\n",
    "t1 = tm.time()\n",
    "\n",
    "#How long it takes to load data?\n",
    "print(\"Time loading: {:.3f}s\".format(t1 - t0))\n",
    "\n",
    "#How many records do we have in DataFrame?\n",
    "print(\"Total number of registers loaded into dataFrame: {0}\".format(len(data)))\n",
    "\n",
    "#What kind of data do we have in DataFrame?\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done is loading 7584 registers from a ~700Kb the file into a DataFrame.  \n",
    "\n",
    "If you try to laod a multiple GB file into memory at once, in a well configured OS your notebook will hang, due to problems allocating memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Create a big file\n",
    "\n",
    "We must define a method for creating a big file, choosing its size and with no random contents. \n",
    "In Jesse Noller blog [1], there is code that does what we need. It uses collections, a module that implements a very fast container datatype (deque) .\n",
    "\n",
    "This code generates a file with \"reproducible-random\" words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "#Create a non-random number. This one permits us to generate the same file\n",
    "seed = \"578945241245768521425\"\n",
    "\n",
    "#Get a file with some sample words.\n",
    "lorem_URL = \"https://raw.githubusercontent.com/mxw/grmr/master/src/inputs/lorem.txt\"\n",
    "lorem_file = \"lorem.txt\"\n",
    "urllib.request.urlretrieve(lorem_URL,lorem_file)\n",
    "\n",
    "# Split words in the file, and replace newlines, commas and dots to spaces\n",
    "words = open(lorem_file, \"r\").read().replace(\"\\n\",' ').replace(\",\",'').replace(\".\",'').split() #Get the words in lorem.txt\n",
    "\n",
    "#Creates a yield that provides non-random words as needed.\n",
    "def fdata():\n",
    "    # Deque is a container that implements fast append and pops.\n",
    "    a = collections.deque(words)\n",
    "    b = collections.deque(seed)\n",
    "    while True:\n",
    "        yield '\\n '.join(list(a)[0:1024])\n",
    "        a.rotate(int(b[0]))\n",
    "        b.rotate(1)\n",
    "\n",
    "# \"Connect\" with the yield        \n",
    "g = fdata()\n",
    "\n",
    "# Set the size and the file you want to create.\n",
    "#size = 17179869184 #16Gb (~5minutes)\n",
    "#size = 4294967296 #4Gb (~1.2minutes)\n",
    "size = 419430400 #400Mb (~6s)\n",
    "\n",
    "#Define output file and its fileHandler\n",
    "output_path = \"/tmp/bigFile1.out\"\n",
    "output_file = open(output_path, 'w')\n",
    "\n",
    "#Add words into the file, until its bigger than size:\n",
    "t0 = time.time()\n",
    "while os.path.getsize(output_path) < size:\n",
    "    output_file.write(next(g)) #in python 2, is g.next()\n",
    "    \n",
    "t1 = time.time()\n",
    "\n",
    "output_file.close()\n",
    "\n",
    "print(\"Time generating file: {:.2f}m\".format((t1-t0)/60))\n",
    "print(\"Size of generated file: {0}bytes\".format(os.path.getsize(output_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Count the words\n",
    "\n",
    "How many words are in the file /tmp/bigFile1.out?  \n",
    "\n",
    "### Loading the entire file into memory \n",
    "This code loads the entire file into memory and counts the numer of words.\n",
    "With a big File, can you guess the result? If you want to try it, first of all, save your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/tmp/bigFile1.out\"\n",
    "\n",
    "t0 = tm.time()\n",
    "\n",
    "count = 0\n",
    "with open(file_path, \"r\") as f: #Use the with statement to avoid calling close() method\n",
    "    data = f.read()            #Loads file contents into memory!\n",
    "    count = len(data.split())\n",
    "    \n",
    "t1 = tm.time()\n",
    "\n",
    "print(\"We have {0} words in file {1}\".format(count, file_path))\n",
    "print(\"Time: \", t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not able to load a big file into memory. Read method will hang our computer with a 40Gb file.\n",
    "\n",
    "### Reading the file by lines\n",
    "\n",
    "Python solves this problem using **Iterators** in a transparent way. Itrerables are objects with elements that can be readed by parts: strings, lists, dicts, **files** and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three iterable objects\n",
    "a_string = 'Hello'\n",
    "a_list = [23,4,76,1]\n",
    "a_file = \"/tmp/bigFile1.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **for** statement reads each part of an interator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in a_string:\n",
    "    print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in a_list:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **for** statement recives an iterable, creates the iterator and uses it to operate over each element.  \n",
    "By default, strings are splitted by words, lists by elements, dicts by keys and files by lines.\n",
    "\n",
    "Let's see how to read our bigFile line by line using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count words in a file line by line\n",
    "file_path = \"/tmp/bigFile1.out\"\n",
    "\n",
    "t0 = tm.time()\n",
    "with open(file_path, \"r\") as f:\n",
    "    count = 0\n",
    "    # Creates an interator in the file, reading line by line\n",
    "    for line in f:\n",
    "        count = count + len(line.split())\n",
    "            \n",
    "t1 = tm.time()\n",
    "\n",
    "print(\"We have {0} words in file {1}\".format(count, file_path))\n",
    "print(\"Time: \", t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we have spplitted the file in lines. This gives us the chance to operate with a very large file and we can modify our code to use multiple engines to operate with each \"chunk\".\n",
    "\n",
    "Anyway, this method has some disadvantages:\n",
    "+ Time is bigger than with read (with 400Mb, 22sec vs 4sec)\n",
    "+ What happens if our big file has only a few lines? We would have to create a **second for** to iterate over line.\n",
    "+ What happens if our lines has **different lenght**? It would be difficult to predict time spent in parallel programming, so it'll be difficult to optimize our code.\n",
    "\n",
    "### Binary split\n",
    "\n",
    "To overcome this, we can use a binary split. That is, cutting the file by fixed size blocks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from: http://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python\n",
    "\n",
    "import time as tm\n",
    "\n",
    "def read_in_chunks(file, size=1024):\n",
    "    \"\"\"Generator that returns contents of a file in chunks of fixed size (default = 1k)\"\"\"\n",
    "    while True:\n",
    "        data = file.read(size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "        \n",
    "def words_in_file(file):\n",
    "    \"\"\"Counts words in a file\"\"\"\n",
    "    with open(fname, \"r\") as f:\n",
    "        count = 0\n",
    "        for chunk in read_in_chunks(f):\n",
    "            count = count + len(chunk.split())\n",
    "    return count\n",
    "\n",
    "file_path = \"/tmp/bigFile1.out\"\n",
    "t0 = tm.time()\n",
    "num_words = words_in_file(file_path)\n",
    "t1 = tm.time()\n",
    "\n",
    "print(\"We have {0} words in file {1}\".format(num_words, file_path))\n",
    "print(t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's awesome, with no parallelization, the code is so fast: 400Mb in ~2.8seconds.  \n",
    "But wait, is the result correct? **Binary split is counting more words**\n",
    "\n",
    "## 4.- Exercice: think ways to get better results when using binary splitting.\n",
    "\n",
    "Your solution must be parallelizable, with no RAM overlads and mus use binary split.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources\n",
    "[1] http://jessenoller.com/blog/2009/02/27/generating-re-creatable-random-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
