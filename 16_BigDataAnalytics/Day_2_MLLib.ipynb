{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ** Spark stack ** \n",
    "\n",
    "![caption](https://spark.apache.org/images/spark-stack.png)\n",
    "\n",
    "* Spark combines a stack of libraries including\n",
    "    * SQL and DataFrames. Mix SQL queries with Spark programs Spark DataFrames are based on RDDs\n",
    "    * MLlib. A library for machine learning, it provides data structures on the top of RDDs and ML algorithms\n",
    "    * GraphX. a distributed graph database based on RDDs (limited number of functions)\n",
    "    * Spark Streaming. it brings Spark to stream processing, letting the programmer write streaming jobs the same way than batch jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "* Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed:\n",
    "    - Seamlessly mix SQL queries with Spark programs.\n",
    "    - Spark SQL lets you query structured data inside Spark programs, using SQL \n",
    "    - Apply functions to results of SQL queries.\n",
    "```Python\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()\n",
    "# +----+-------+\n",
    "# | age|   name|\n",
    "# +----+-------+\n",
    "# |null|Michael|\n",
    "# |  30|   Andy|\n",
    "# |  19| Justin|\n",
    "# +----+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataSets and DataFrames\n",
    "\n",
    "A **Dataset** is a distributed collection of data. It provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.).\n",
    "\n",
    "**Problem:** It is not supported by Python ... why?\n",
    "\n",
    "A **DataFrame** is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "    - Example: \n",
    "    \n",
    "```Python\n",
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "# +----+-------+\n",
    "# | age|   name|\n",
    "# +----+-------+\n",
    "# |null|Michael|\n",
    "# |  30|   Andy|\n",
    "# |  19| Justin|\n",
    "# +----+-------+\n",
    "```\n",
    "\n",
    "In Python it’s possible to access a DataFrame’s columns either by attribute (```df.age```) or by indexing (```df['age']```).\n",
    "\n",
    "```Python\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "# root\n",
    "# |-- age: long (nullable = true)\n",
    "# |-- name: string (nullable = true)\n",
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "# +-------+\n",
    "# |   name|\n",
    "# +-------+\n",
    "# |Michael|\n",
    "# |   Andy|\n",
    "# | Justin|\n",
    "# +-------+\n",
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "# +-------+---------+\n",
    "# |   name|(age + 1)|\n",
    "# +-------+---------+\n",
    "# |Michael|     null|\n",
    "# |   Andy|       31|\n",
    "# | Justin|       20|\n",
    "# +-------+---------+\n",
    "\n",
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()\n",
    "# +---+----+\n",
    "# |age|name|\n",
    "# +---+----+\n",
    "# | 30|Andy|\n",
    "# +---+----+\n",
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "# +----+-----+\n",
    "# | age|count|\n",
    "# +----+-----+\n",
    "# |  19|    1|\n",
    "# |null|    1|\n",
    "# |  30|    1|\n",
    "# +----+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames Data Types\n",
    "\n",
    "**DataType**: this abstract class is the base type of all built-in data types in Spark SQL, e.g. strings, longs. Besides, it is possible to use DataTypes objects in your code to create complex Spark SQL types, such as arrays or maps.\n",
    "\n",
    "**StructType** is a built-in data type in Spark SQL to represent a collection of StructFields.\n",
    "```Python\n",
    "schemaTyped.printTreeString\n",
    "root\n",
    " |-- a: integer (nullable = true)\n",
    " |-- b: string (nullable = true)\n",
    "```\n",
    "\n",
    "A **StructField** describes a single field in a ```StructType```. It has a name, the type and whether or not it be empty, and an optional metadata and a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VectorAssembler\n",
    "\n",
    "**VectorAssembler** is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.\n",
    "\n",
    "_Examples_\n",
    "\n",
    "Assume that we have a DataFrame with the columns id, hour, mobile, userFeatures, and clicked:\n",
    "```Python\n",
    " id | hour | mobile | userFeatures     | clicked\n",
    "----|------|--------|------------------|---------\n",
    " 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0\n",
    "```\n",
    "\n",
    "userFeatures is a vector column that contains three user features. We want to combine hour, mobile, and userFeatures into a single feature vector called features and use it to predict clicked or not. If we set VectorAssembler’s input columns to hour, mobile, and userFeatures and output column to features, after transformation we should get the following DataFrame:\n",
    "```Python\n",
    " id | hour | mobile | userFeatures     | clicked | features\n",
    "----|------|--------|------------------|---------|-----------------------------\n",
    " 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]\n",
    " \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)\n",
    " ```\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Annex 1.\n",
    "\n",
    "\n",
    "# ***Linear Regression***\n",
    "\n",
    "* Goal: Learn a mapping from observations (features) to continuous labels given a training set (supervised learning)\n",
    "    * Example: Height, Gender, Weight → Shoe Size\n",
    "    \n",
    "    \n",
    "![caption](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/220px-Linear_regression.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Linear Regression General Formulation**\n",
    "\n",
    "For each observation we have a feature vector $x_i$ and label $y$ where $i = 1, \\dots ,d$\n",
    "\n",
    "$$x^T  =  [x_1, \\dots x_d]$$\n",
    "\n",
    "and we assume a linear mapping between features and label: \n",
    " \n",
    "$$y \\sim  w_0 + w_1 x_ 1 + \\dots + w_d x_d $$\n",
    "\n",
    "We can augment the feature vector to incorporate offset: \n",
    "\n",
    "$$ x  =  [1, x_1, \\dots,  x_d] $$ \n",
    "\n",
    "Then, we can then rewrite this linear mapping as scalar product: \n",
    "\n",
    "$$ y \\sim \\hat{y} = \\sum_{i=0}^d w_i x_i = w^T x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **1D Example**\n",
    "\n",
    "__Goal__: find the line of best fit\n",
    "\n",
    "$x$ coordinate: features\n",
    "\n",
    "$y$ coordinate: labels\n",
    "\n",
    "$\\hat{y} \\sim y = w_0 + w_1 x$\n",
    "\n",
    "$w_0$ intercept\n",
    "\n",
    "$w_1$ slope\n",
    "\n",
    "    \n",
    "![caption](./images/Linear_regression_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Evaluating Predictions**\n",
    "\n",
    "* Can measure ‘closeness’ between label and prediction\n",
    "    * Example -> Shoe size: better to be off by one size than 5 sizes\n",
    "\n",
    "* What is an appropriate evaluation metric or ‘loss’ function? \n",
    "    * Option 1: Absolute loss: $|y - \\hat{y}|$\n",
    "    * Squared loss: $(y - \\hat{y})^2$ -> Has nicer mathematical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **How Can We Learn Model (w)?**\n",
    "\n",
    "* Assume we have $n$ training points, where $x^i$ denotes the $i\\ th$ point\n",
    "\n",
    "* Recall two earlier points:\n",
    "    * Linear assumption: $\\hat{y} = w^T x$ \n",
    "    * We use squared loss: $(y - \\hat{y})^2$\n",
    "\n",
    "* Idea: Find $w$ that minimizes squared loss over training points:\n",
    "\n",
    "$$ min_w \\sum_{i=1}^n (\\frac{w^T x^i}{y^i}-y^i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given $n$ training points with $d$ features, we define:\n",
    "\n",
    "* $X \\in \\mathbb{R}^{n \\times d}$ : matrix storing points\n",
    "* $y \\in \\mathbb{R}^n $ : real-valued labels\n",
    "* $\\hat{y} \\in \\mathbb{R}^n $ : predicted labels, where $ \\hat{y} = X w$\n",
    "* $w \\in \\mathbb{R}^d $ : regression parameters / model to learn\n",
    "\n",
    "__Least Squares Regression__: Learn mapping (w) from features to labels that minimizes residual sum of squares:\n",
    "\n",
    "$$ min_w ||Xw - y||^2_2 $$\n",
    "\n",
    "Equivalent to $min_w \\sum_{i=0}^n (w^T x^i - y^i)$ by definition of Euclidean norm\n",
    "\n",
    "Closed form solution (if inverse exists): $w = (X^T X)^{-1} X^T y$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
