{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "# Machine Learning: Unsupervised learning (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> \n",
    "\n",
    "# 5. CASE STUDY: EUROSTAT data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eurostat is the home of the [European Commission data](http://ec.europa.eu/eurostat). Eurostat’s main role is to process and publish comparable statistical information at European level. Data in Eurostat is provided by each member state. Eurostat's re-use policy is free re-use of its data, both for non-commercial and commercial purposes (with some minor exceptions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Applying K-means to cluster countries according to their education resourses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study, we are going to retrieve Eurostat data and test clustering algorithms on it. The amount of data in the database is huge, thus we are going to use a small subset for illustration purposes. In our first study, we are going to focus on **indicators on education finance data** among the member states. The data is already downloaded and provided as is in the file `educ_figdp_1_Data.csv`. You can download it directly following this link: `Database by terms>Population and social conditions>Education and training>Indicators on education finance`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start having a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Read and check the dataset downloaded from the EuroStat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "edu=pd.read_csv('./files/education_GDP/educ_figdp_1_Data.csv',na_values=':')\n",
    "edu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in CSV and databases are often organized in what is called **stacked** or **record** formats. In our case for each year (`TIME`) and country (`GEO`) of the EU as well as some reference countries such as Japan and United States, we have twelve indicators (`INDIC_ED`) on education finance with their values (`Value`). Let us reshape the table into a feature vector style data set. \n",
    "\n",
    "To the process of reshaping stacked data into a table is sometimes called **pivoting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot table in order to get a nice feature vector representation with dual indexing by TIME and GEO \n",
    "pivedu=pd.pivot_table(edu, values='Value', index=['TIME', 'GEO'], columns=['INDIC_ED'])\n",
    "\n",
    "pivedu.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Let us check the two indices:\\n')\n",
    "print('\\nPrimary index (TIME): \\n' + str(pivedu.index.levels[0].tolist()))\n",
    "print('\\nSecondary index (GEO): \\n' + str(pivedu.index.levels[1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we have ten years information on these indicators, and as expected we have all members of the European Union with some aggregates and control/reference countries. For the sake of simplicity, let us focus on values on year 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 2010 set of values\n",
    "edu2010=pivedu.loc[2010]\n",
    "edu2010.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us clean and store the names of the features and the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store column names and clear them for better handling. Do the same with countries\n",
    "edu2010 = edu2010.rename(index={'Euro area (13 countries)': 'EU13',\n",
    "                                'Euro area (15 countries)': 'EU15',\n",
    "                                'European Union (25 countries)': 'EU25',\n",
    "                                'European Union (27 countries)': 'EU27',\n",
    "                                'Former Yugoslav Republic of Macedonia, the': 'Macedonia',\n",
    "                                'Germany (until 1990 former territory of the FRG)': 'Germany'\n",
    "                        })\n",
    "features = edu2010.columns.tolist()\n",
    "countries = edu2010.index.tolist()\n",
    "\n",
    "edu2010.columns=range(12)\n",
    "edu2010.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, this is not a clean data set, there are missing values. Some countries may not collect or have access to some indicators and there are countries without any indicators. Let us display this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check what is going on in the NaN data \n",
    "nan_countries=np.sum(np.where(edu2010.isnull(),1,0),axis=1)\n",
    "\n",
    "plt.bar(np.arange(nan_countries.shape[0]),nan_countries)\n",
    "plt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',fontsize=12)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have info on Albania, Macedonia and Greece. And very limited info from Liechtenstein, Luxembourg and Turkey. So let us work without them. Now let us check the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non info countries\n",
    "wrk_countries = nan_countries<4\n",
    "\n",
    "educlean=edu2010.loc[wrk_countries] #.loc - Construct an open mesh from multiple sequences.\n",
    "\n",
    "#Let us check the features we have\n",
    "na_features = np.sum(np.where(educlean.isnull(),1,0),axis=0)\n",
    "print(na_features)\n",
    "\n",
    "plt.bar(np.arange(na_features.shape[0]),na_features)\n",
    "plt.xticks(fontsize=12)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four features with missing data. At this point we can proceed in two ways:\n",
    "\n",
    "+ **Drop** the features with missing values.\n",
    "+ **Fill in** the features with some non-informative, non-biasing data.\n",
    "\n",
    "If we have many features and only a few have missing values then it is not much harmful to drop them. However, if missing values are spread across the features, we have to eventually deal with them. In our case, both options seem reasonable, so we will proceed with both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Options A: drop those features\n",
    "edudrop=educlean.dropna(axis=1) \n",
    "         #dropna: Return object with labels on given axis omitted where alternately any or \n",
    "          # all of the data are missing\n",
    "print('Drop data shape: ' + str(edudrop.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option B fill those features with some value, at risk of extracting wrong information\n",
    "#Constant filling : edufill0=educlean.fillna(0)\n",
    "edufill=educlean.fillna(educlean.mean())\n",
    "print('Filled in data shape: ' + str(edufill.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fill-in option, we have decided to fill the data with the mean value of the feature. This will not bias the distribution of the feature, though it has consequences in the interpretation of the results. \n",
    "\n",
    "Let us now apply a k-means clustering technique on this data in order to partition the countries according to their investment in education and check their profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have clean data, create the data set to analyze\n",
    "X_train = edudrop.values\n",
    "\n",
    "from sklearn import cluster\n",
    "clf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=42)\n",
    "    #‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to \n",
    "    # speed up convergence.\n",
    "    \n",
    "clf.fit(X_train) #Compute k-means clustering.\n",
    "\n",
    "y_pred_drop = clf.predict(X_train) #Predict the closest cluster each sample in X belongs to.\n",
    "print(y_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=y_pred_drop.argsort()\n",
    "plt.plot(np.arange(35),y_pred_drop[idx],'ro')\n",
    "\n",
    "#visualize it\n",
    "wrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n",
    "plt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\n",
    "           horizontalalignment='left',fontsize=12)\n",
    "plt.title('Using drop features',size=15)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((12,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = edufill.values\n",
    "clf.fit(X_train) #Compute k-means clustering.\n",
    "\n",
    "y_pred_fill = clf.predict(X_train) #Predict the closest cluster each sample in X belongs to.\n",
    "print(y_pred_fill)\n",
    "\n",
    "#visualize it\n",
    "idx=y_pred_fill.argsort()\n",
    "plt.plot(np.arange(35),y_pred_fill[idx],'ro')\n",
    "\n",
    "wrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n",
    "plt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\n",
    "           horizontalalignment='left',fontsize=12)\n",
    "plt.title('Using filled in data', size=15)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sorted the data for better visualization. At a simple glance we can see that both partitions can be different. We can better check this effect plotting the clusters values of one technique against the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_fill+0.2*np.random.rand(35),'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, looking at both methods, both may yield the same results, but not necessarily always. This is mainly due to two aspects: the random initialization of the k-means clustering and the fact that each method works in a different space (dropped data vs filled-in data). \n",
    "\n",
    "Let us check the list of countries in both methods. Do not consider the cluster value, since it is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==0]))\n",
    "print('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill) if item==0]))\n",
    "print('\\n')\n",
    "print('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==1]))\n",
    "print('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill) if item==1]))\n",
    "print('\\n')\n",
    "print('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==2]))\n",
    "print('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill) if item==2]))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 Shell we consider a data normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been a little careless and proceeded with the clustering of data, without any normalization. Let us go into a little more detail in the data. For the sake of simplicity, let us work just with the drop set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y_pred_drop_unnorm=y_pred_drop.copy()\n",
    "X_train = edudrop.values\n",
    "scaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "clf.fit(X_train)\n",
    "y_pred_drop = clf.predict(X_train)\n",
    "\n",
    "#visuaize it\n",
    "plt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_drop_unnorm+0.2*np.random.rand(35),'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUIZ:** In this case it seems both clustering techniques yield almost the same clusters. Why do you think both yield almost the same results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us now check the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==0]))\n",
    "\n",
    "print('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==1]))\n",
    "\n",
    "print('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the average expenditure per cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the profile of the clusters by looking at the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=0.3\n",
    "p1 = plt.bar(np.arange(8),scaler.inverse_transform(clf.cluster_centers_[1]),width,color='b')\n",
    "        # Scale back the data to the original representation\n",
    "    \n",
    "p2 = plt.bar(np.arange(8)+width,scaler.inverse_transform(clf.cluster_centers_[2]),width,color='yellow')\n",
    "p0 = plt.bar(np.arange(8)+2*width,scaler.inverse_transform(clf.cluster_centers_[0]),width,color='r')\n",
    "\n",
    "#visualize\n",
    "plt.legend( (p0[0], p1[0], p2[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2') ,loc=9)\n",
    "plt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\n",
    "plt.yticks(size=12)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like cluster `1` spends more on education while cluster `0` is the one with less resources on education. \n",
    "\n",
    "What about Spain?\n",
    "\n",
    "Let us refine a little bit more cluster `0` and check how close are members from this cluster to cluster `1`. This may give us a hint on a possible ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "p = distance.cdist(X_train[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean') \n",
    "                              #the distance of the elements of cluster 0 to the center of cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.bar(np.arange(p.shape[0]), p.flatten())\n",
    "\n",
    "wrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n",
    "zero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==0]\n",
    "plt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,horizontalalignment='left',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems that Spain belongs to cluster `0`, it is the closest to change to a policy in the lines of the other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can also check the distance to the centroid of cluster `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "p = distance.cdist(X_train[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\n",
    "pown = distance.cdist(X_train[y_pred_drop==0,:],[clf.cluster_centers_[0]],'euclidean')\n",
    "\n",
    "width=0.45\n",
    "p0=plt.bar(np.arange(p.shape[0]),p.flatten(),width)\n",
    "p1=plt.bar(np.arange(p.shape[0])+width,pown.flatten(),width,color = 'red')\n",
    "\n",
    "wrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n",
    "zero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop) if item==0]\n",
    "plt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,horizontalalignment='left',fontsize=12)\n",
    "plt.legend( (p0[0], p1[0]), ('d -> 1', 'd -> 0') ,loc=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 The number of clusters\n",
    "The number of cluster has been arbitrarely chosen. We will explore the **elbow/knee technique** for looking for a \"good\" number $K$. Other references for finding the number of clusters are the GAP statistic or $f(K)$ (Pham et al. 2004) among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 The elbow technique\n",
    "\n",
    "By the definition of clustering, we want clusters to be compact. The notion of compactness can be measured by checking the distance of the members of the cluster to its centroid. The average distance to its centroid is a naive measure of the overall quality of the cluster. The elbow technique distinguish two phases in the process of checking this value against the number of clusters. In the first phase, the average will decrease dramatically. In the second phase, it will slowly stabilize. The elbow technique consists of selecting the value, where this transition occurs. This can not always be easily found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Create some data\n",
    "MAXN=40\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,3]+1*np.random.randn(MAXN,2)])\n",
    "\n",
    "#Just for visualization purposes\n",
    "y = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,3*np.ones((MAXN,1))])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:,0],X[:,1],color='r')\n",
    "plt.title('Data as we see it', size=14)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n",
    "plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n",
    "plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n",
    "plt.title('Data as it was generated',size=14)\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches((14,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "d=[]\n",
    "iter=20\n",
    "for K in range(2,iter):\n",
    "    clf = cluster.KMeans(init='random', n_clusters=K, random_state=0)\n",
    "    clf.fit(X)\n",
    "    y_pred = clf.predict(X)\n",
    "    p=[]\n",
    "    for i in range(K): # compute the distance of each point to its cluster center\n",
    "        p.append(np.sum(distance.cdist(X[y_pred==i,:],[clf.cluster_centers_[i]],'euclidean')))\n",
    "    d.append(np.sum(p))\n",
    "plt.plot(np.arange(2,iter),d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking at this graph it seems that $3$ is a good number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the Eurostat data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "d=[]\n",
    "for K in range(2,10):\n",
    "    clf = cluster.KMeans(init='random', n_clusters=K, random_state=0)\n",
    "    clf.fit(X_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    p=[]\n",
    "    for i in range(K):\n",
    "        p.append(np.sum(distance.cdist(X_train[y_pred==i,:],[clf.cluster_centers_[i]],'euclidean')))\n",
    "    d.append(np.sum(p))\n",
    "plt.plot(np.arange(2,10),d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like $4$ can be a better choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us redo the clustering with $K=4$ and see what we can conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = edudrop.values\n",
    "clf = cluster.KMeans(init='k-means++', n_clusters=4, random_state=0)\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=y_pred.argsort()\n",
    "plt.plot(np.arange(35),y_pred[idx],'ro')\n",
    "wrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n",
    "\n",
    "plt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n",
    "plt.title('Using drop features',size=15)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=0.2\n",
    "p0 = plt.bar(np.arange(8)+1*width,clf.cluster_centers_[0],width,color='r')\n",
    "p1 = plt.bar(np.arange(8),clf.cluster_centers_[1],width,color='b')\n",
    "p2 = plt.bar(np.arange(8)+3*width,clf.cluster_centers_[2],width,color='yellow')\n",
    "p3 = plt.bar(np.arange(8)+2*width,clf.cluster_centers_[3],width,color='pink')\n",
    "\n",
    "plt.legend( (p0[0], p1[0], p2[0], p3[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3') ,loc=9)\n",
    "plt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\n",
    "plt.yticks(size=12)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spain is still in cluster `0`. But as we observed in our previous clustering it was very close to changing cluster. This time cluster `0` includes the averages values for the EU members. Just for the sake of completeness, let us write down the name of the countries in the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==0]))\n",
    "\n",
    "print('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==1]))\n",
    "\n",
    "print('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==2]))\n",
    "\n",
    "print('Cluster 3: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==3]))\n",
    "\n",
    "#Save data for future use.\n",
    "import pickle\n",
    "ofname = open('edu2010.pkl', 'wb')\n",
    "s = pickle.dump([edu2010, wrk_countries_names,y_pred ],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">TAKE HOME NOTES:\n",
    "<p>\n",
    "<ul>\n",
    "<li>K-means is a simple, but quite powerful clustering partition technique.\n",
    "<li>It is very sensitive to variance differences on the features / features at different scales.\n",
    "<li>It is resistant to irrelevant dimensions as far as they do not introduce biases.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the process using the alternative clustering techniques and compare their results. Let us first apply the **spectral clustering**. The corresponding code will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "X = StandardScaler().fit_transform(edudrop.values)\n",
    " \n",
    "distances = euclidean_distances(edudrop.values)\n",
    "    \n",
    "spectral = cluster.SpectralClustering(n_clusters=4, affinity=\"nearest_neighbors\")\n",
    "spectral.fit(edudrop.values)\n",
    " \n",
    "y_pred = spectral.labels_.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=y_pred.argsort()\n",
    "\n",
    "plt.plot(np.arange(35),y_pred[idx],'ro')\n",
    "wrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n",
    "\n",
    "plt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] \n",
    "        for i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n",
    "\n",
    "plt.yticks([0,1,2,3])\n",
    "\n",
    "plt.title('Applying Spectral Clustering on the drop features',size=15)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((11,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that in general, the spectral clustering intends to obtain more balanced clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Applying the hierarchical clustering to agglomerate countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the agglomerative clustering, we obtain not only the different clusters, but also we can see how different clusters are obtained. This, in some way it is giving us information on which are the pairs of countries and clusters that are most similar. The corresponding code that applies the agglomerative clustering is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = edudrop.values\n",
    "\n",
    "dist = pdist(X_train,'euclidean')\n",
    "\n",
    "linkage_matrix = linkage(dist,method = 'complete');\n",
    "plt.figure(figsize=(7.3, 7.3))  # we need a tall figure\n",
    "\n",
    "dendrogram(linkage_matrix, orientation=\"right\", color_threshold = 2,labels = \n",
    "           wrk_countries_names, leaf_font_size=10);\n",
    "\n",
    "plt.tight_layout()  # fixes margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, the parameter color_threshold colors all the descendent links below a cluster node k the same color if k is the first node below the color threshold. All links connecting nodes with distances greater than or equal to the threshold are colored blue. Thus, if we use color threshold = 2, the obtained clusters are as follows:\n",
    "\n",
    "- Cluster 0: ['Cyprus', 'Denmark', 'Iceland']\n",
    "- Cluster 1: ['Bulgaria', 'Croatia', 'Czech Republic', 'Italy', 'Japan', 'Romania', 'Slovakia']\n",
    "- Cluster 2: ['Belgium', 'Finland', 'Ireland', 'Malta', 'Norway', 'Sweden']\n",
    "- Cluster 3: ['Austria', 'Estonia', 'EU13', 'EU15', 'EU25', 'EU27', 'France', 'Germany', 'Hungary', 'Latvia', 'Lithuania', 'Netherlands', 'Poland', 'Portugal', 'Slovenia', 'Spain', 'Switzerland', 'United Kingdom', 'United States']\n",
    "\n",
    "Note that they correspond in high degree to the clusters obtained by the K-means (except permutation of clusters labels that is irrelevant). The figure shows the construction of the clusters using the complete linkage agglomerative clustering. Different cuts at different levels of the dendrogram allow to obtain different number of clusters. As a summary, let us compare the results of the three approaches of clustering. We cannot expect that the results coincide since different approaches are based on different criteria to construct the clusters. Still, we can observe that in this case K-means and the agglomerative approaches gave the same results (up to a permutation of the number of cluster that is irrelevant), meanwhile the spectral clustering gave more evenly distributed clusters.  Note that these results can change when using different distance between data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i> This notebook was created by [Petia Radeva](http://www.cvc.uab.es/~petia) and [Oriol Pujol Vila](http://www.maia.ub.es/~oriol). Last edition: 5 of February, 2019.</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
