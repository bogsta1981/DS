{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "# Machine Learning: Unsupervised learning (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can dimensionality affect the clustering results?##\n",
    "\n",
    "## How to visualize the data in 60-dimensional space?##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a clustering  of handwritten digits data.  Let us consider the [letter dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits). Python source code: [plot_kmeans_digits.py](http://scikit-learn.org/stable/_downloads/plot_kmeans_digits.py).\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits  width=900 height=500></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to visualize the $R^{64}$ feature space?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a dimensionality reduction technique to:\n",
    "\n",
    "+ visualize the data,\n",
    "\n",
    "+ remove non-relevant information (redundancy), \n",
    "\n",
    "+ speed up the clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.datasets import load_digits \n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the digit data\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scale(digits.data)  #Standardize a dataset along any axis, \n",
    "                           #Center to the mean and component wise scale to unit variance.\n",
    "\n",
    "print(digits.target[0:35])  # data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as pl        #pylab combines pyplot with numpy into a single namespace. \n",
    "                          #For more information, look at: http://wiki.scipy.org/PyLab\n",
    "pl.gray()                 #defines the gray colormap to display images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.matshow(digits.images[34]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.matshow(digits.images[3]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "labels = digits.target\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\" % (n_digits, n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "from matplotlib import pyplot as plt #Provides a MATLAB-like plotting framework.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "ones=data[labels==1]\n",
    "fives=data[labels==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2,x3=0,1,2 #let's consider the first three features\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "ax.plot(ones[:, x1], ones[:, x2], ones[:, x3], 'o', markersize=8, color='blue', alpha=0.5, label='ones')\n",
    "ax.plot(fives[:, x1], fives[:, x2], fives[:, x3], '^', markersize=8, alpha=0.5, color='red', label='fives')\n",
    "\n",
    "plt.title('Samples for class 1 and class 5')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlabel('x1='+str(x1))\n",
    "ax.set_ylabel('x2='+str(x2))\n",
    "ax.set_zlabel('x3='+str(x3))\n",
    "\n",
    "plt.show()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ones.shape[1]):\n",
    "    plt.plot(range(1,n_features+1), ones[i,:], c='b')\n",
    "for i in range(fives.shape[1]):\n",
    "    plt.plot(range(1,n_features+1), fives[i,:], c='r')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((18,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dif=abs(np.mean(fives,axis=0)-np.mean(ones,axis=0))\n",
    "ind=np.argsort(dif) # look for the biggest difference\n",
    "ind=ind[::-1]   # sort and reverse the order\n",
    "x1,x2,x3=ind[0:3] # consider the most discriminant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "ax.plot(ones[:, x1], ones[:, x2], ones[:, x3], 'o', markersize=8, color='blue', alpha=0.5, label='ones')\n",
    "ax.plot(fives[:, x1], fives[:, x2], fives[:, x3], '^', markersize=8, alpha=0.5, color='red', label='fives')\n",
    "\n",
    "plt.title('Samples for class 1 and class 5')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlabel('x1='+str(x1))\n",
    "ax.set_ylabel('x2='+str(x2))\n",
    "ax.set_zlabel('x3='+str(x3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features (dimensions) to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a statistical procedure that decomposes a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. \n",
    "\n",
    "PCA uses an *orthogonal transformation* to convert a set of observations of possibly correlated variables into a set of values of *linearly uncorrelated* variables called **principal components**. \n",
    "\n",
    "The **number** of principal components is less than or equal to the number of original variables. \n",
    "\n",
    "This transformation is defined in such a way that the *first principal component* has the *largest possible variance* (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to obtain it?:** The principal components are obtained as the **eigenvectors of the covariance matrix**, hence are *orthogonal*. \n",
    "\n",
    "In scikit-learn, PCA is implemented as a transformer object that learns $n$ components in its *fit* method, and can be used on new data to project it on these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the PCA decomposition!\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "print(np.concatenate((ones,fives)).shape)\n",
    "\n",
    "newdataPCA=pca.fit_transform(np.concatenate((ones,fives))) #concatenates vectors row by row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize it\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "ax.plot(newdataPCA[1:ones.shape[0], 0], newdataPCA[1:ones.shape[0], 1],\\\n",
    "    newdataPCA[1:ones.shape[0], 2], 'o', markersize=8, color='blue', alpha=0.5, label='ones')\n",
    "ax.plot(newdataPCA[ones.shape[0]:, 0], newdataPCA[ones.shape[0]:, 1],\\\n",
    "    newdataPCA[ones.shape[0]:, 2], '^', markersize=8, alpha=0.5, color='red', label='fives')\n",
    "\n",
    "plt.title('Samples for class 1 and class 5 in the PCA space')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlabel('First PCA base vector')\n",
    "ax.set_ylabel('Second PCA base vector')\n",
    "ax.set_zlabel('Third PCA base vector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "plt.plot(newdataPCA[1:ones.shape[0], 0], newdataPCA[1:ones.shape[0], 1], 'o', markersize=8, color='blue', alpha=0.5, label='ones')\n",
    "plt.plot(newdataPCA[ones.shape[0]:, 0], newdataPCA[ones.shape[0]:, 1], '^', markersize=8, alpha=0.5, color='red', label='fives')\n",
    "\n",
    "plt.title('Samples for class 1 and class 5 in the PCA space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of performance of K-means with different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to compare the K-means performance using various strategies in terms of runtime and quality of the results. As the groundtruth is known here, we also apply different cluster quality metrics to judge the goodness of the fit of the cluster labels to the groundtruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator=KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "estimator.fit(data)\n",
    "\n",
    "print ('Silhouette score is: %.3f' % metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             \n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(79 * '_')\n",
    "print('% 9s' % 'initial-n    time  inertia    homo   compl  v-meas    silhouette')\n",
    "\n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n",
    "                              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n",
    "                              name=\"random\", data=data)\n",
    "\n",
    "                    # In this case, the seeding of the centers is deterministic, hence we run the\n",
    "                                        # k-means algorithm only once with n_init=1.\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "                              name=\"PCA-based\", data=data)\n",
    "print(79 * '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Visualize the results on PCA-reduced data\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "                    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02             # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each point\n",
    "x_min, x_max = reduced_data[:, 0].min() + 1, reduced_data[:, 0].max() - 1\n",
    "y_min, y_max = reduced_data[:, 1].min() + 1, reduced_data[:, 1].max() - 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        # Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#visualize\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                       cmap=plt.cm.Paired,aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=105)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
