prob=predict(Model.2.2,type=c("response"))
mydata$prob=prob
g=roc(y,prob, data=mydata)
plot(g)
auc(g)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(dplyr)
# read data
#setwd("..")
mydata <- read.csv2("bank.csv", header=TRUE, sep=";", dec=".")
n.var <- names(mydata)
glimpse(mydata)
# Model estimation
attach(mydata)
Model.1.1<- lm(duration~age+ balance+pdays, data=mydata )
summary(Model.1.1)
qplot(balance,duration, data = mydata,geom = c("smooth", "point"))
qplot(age,duration, data = mydata,geom = c("smooth", "point"))
qplot(pdays, duration, data = mydata,geom = c("smooth", "point"))
monthR=relevel(month, ref = 'mar')
loanR=relevel(loan, ref = 'no')
contactR=relevel(contact, ref = 'telephone')
Model.1.2<- lm(duration~age+ balance+factor(monthR)+factor(loanR)+factor(contactR), data=mydata )
summary(Model.1.2)
summary(Model.1.1)$adj.r.squared*100
summary(Model.1.2)$adj.r.squared*100
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30)
predict(Model.1.1, newdata)
predict(Model.1.2, newdata)
Model.2.1=glm(y~age+duration, family=binomial)
summary(Model.2.1)
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30, duration=250)
predict(Model.2.1, newdata, type="response")
Model.2.2=glm(y~age+duration+factor(month), family=binomial)
summary(Model.2.2)
#install.packages("pROC")
library(pROC)
prob=predict(Model.2.2,type=c("response"))
mydata$prob=prob
g=roc(y,prob, data=mydata)
plot(g)
auc(g)
library(knitr)
library(ggplot2)
library(dplyr)
# read data
#setwd("..")
mydata <- read.csv2("bank.csv", header=TRUE, sep=";", dec=".")
n.var <- names(mydata)
glimpse(mydata)
# Model estimation
attach(mydata)
Model.1.1<- lm(duration~age+ balance+pdays, data=mydata )
summary(Model.1.1)
qplot(balance,duration, data = mydata,geom = c("smooth", "point"))
qplot(age,duration, data = mydata,geom = c("smooth", "point"))
qplot(pdays, duration, data = mydata,geom = c("smooth", "point"))
monthR=relevel(month, ref = 'mar')
loanR=relevel(loan, ref = 'no')
contactR=relevel(contact, ref = 'telephone')
Model.1.2<- lm(duration~age+ balance+factor(monthR)+factor(loanR)+factor(contactR), data=mydata )
summary(Model.1.2)
summary(Model.1.1)$adj.r.squared*100
summary(Model.1.2)$adj.r.squared*100
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30)
predict(Model.1.1, newdata)
predict(Model.1.2, newdata)
Model.2.1=glm(y~age+duration, family=binomial)
summary(Model.2.1)
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30, duration=250)
predict(Model.2.1, newdata, type="response")
Model.2.2=glm(y~age+duration+factor(month), family=binomial)
summary(Model.2.2)
#install.packages("pROC")
library(pROC)
prob=predict(Model.2.2,type=c("response"))
mydata$prob=prob
g=roc(y,prob, data=mydata)
plot(g)
auc(g)
library(knitr)
library(ggplot2)
library(dplyr)
# read data
#setwd("..")
mydata <- read.csv2("bank.csv", header=TRUE, sep=";", dec=".")
n.var <- names(mydata)
glimpse(mydata)
# Model estimation
attach(mydata)
Model.1.1<- lm(duration~age+ balance+pdays, data=mydata )
summary(Model.1.1)
qplot(balance,duration, data = mydata,geom = c("smooth", "point"))
qplot(age,duration, data = mydata,geom = c("smooth", "point"))
qplot(pdays, duration, data = mydata,geom = c("smooth", "point"))
monthR=relevel(month, ref = 'mar')
loanR=relevel(loan, ref = 'no')
contactR=relevel(contact, ref = 'telephone')
Model.1.2<- lm(duration~age+ balance+factor(monthR)+factor(loanR)+factor(contactR), data=mydata )
summary(Model.1.2)
summary(Model.1.1)$adj.r.squared*100
summary(Model.1.2)$adj.r.squared*100
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30)
predict(Model.1.1, newdata)
predict(Model.1.2, newdata)
Model.2.1=glm(y~age+duration, family=binomial)
summary(Model.2.1)
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30, duration=250)
predict(Model.2.1, newdata, type="response")
Model.2.2=glm(y~age+duration+factor(month), family=binomial)
summary(Model.2.2)
#install.packages("pROC")
library(pROC)
prob=predict(Model.2.2,type=c("response"))
mydata$prob=prob
g=roc(y,prob, data=mydata)
plot(g)
auc(g)
library(knitr)
library(ggplot2)
library(dplyr)
# read data
#setwd("..")
mydata <- read.csv2("bank.csv", header=TRUE, sep=";", dec=".")
n.var <- names(mydata)
glimpse(mydata)
# Model estimation
attach(mydata)
Model.1.1<- lm(duration~age+ balance+pdays, data=mydata )
summary(Model.1.1)
qplot(balance,duration, data = mydata,geom = c("smooth", "point"))
qplot(age,duration, data = mydata,geom = c("smooth", "point"))
qplot(pdays, duration, data = mydata,geom = c("smooth", "point"))
monthR=relevel(month, ref = 'mar')
loanR=relevel(loan, ref = 'no')
contactR=relevel(contact, ref = 'telephone')
Model.1.2<- lm(duration~age+ balance+factor(monthR)+factor(loanR)+factor(contactR), data=mydata )
summary(Model.1.2)
summary(Model.1.1)$adj.r.squared*100
summary(Model.1.2)$adj.r.squared*100
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30)
predict(Model.1.1, newdata)
predict(Model.1.2, newdata)
Model.2.1=glm(y~age+duration, family=binomial)
summary(Model.2.1)
newdata=data.frame(age=30, balance=100.0, monthR='jun', loanR='yes', contactR='cellular', pdays=30, duration=250)
predict(Model.2.1, newdata, type="response")
Model.2.2=glm(y~age+duration+factor(month), family=binomial)
summary(Model.2.2)
#install.packages("pROC")
library(pROC)
prob=predict(Model.2.2,type=c("response"))
mydata$prob=prob
g=roc(y,prob, data=mydata)
plot(g)
auc(g)
plot(cars)
getwd()
bank<-read.csv("bank.csv",header=T,sep=";", dec=".")
set.seed(123456789)
random<-sample(1:nrow(bank))
num.bank.training<-as.integer(0.75*length(random))
bank.indices<-random[1:num.bank.training]
train<-bank[bank.indices,]
testing.indices<-random[(num.bank.training+1):length(random)]
testing.set<-bank[testing.indices,]
logis<-glm(y ~ ., data=train,family=binomial)
summary(logis)
plot(logis)
plot(logis,which=6)
plot(logis,which=1)
logis<-glm(y ~ ., data=train,family=binomial)
summary(logis)
plot(logis)
plot(logis,which=1)
logis<-glm(y ~ ., data=train,family=binomial)
summary(logis)
plot(logis)
plot(logis,which=6)
logis<-glm(y ~ ., data=train,family=binomial)
summary(logis)
plot(logis)
plot(logis,which=5)
logis<-glm(y ~ ., data=train,family=binomial)
summary(logis)
plot(logis)
plot(logis,which=6)
plot(logis,which=8)
plot(logis,which=6)
dffits(logis)
logit<-glm(y~.,data=bank2,family=binomial)
boxplot(train[,1:(ncol(bank)-1)])
bank1<-train
bank1$dffits<-0
bank1$dffits<-dffits(logis)
bank2<-bank1[!bank1$dffits>2*sqrt(ncol(bank)/nrow(bank)),]
bank1$dfitts<-NULL
bank2$dfitts<-NULL
bank1$out<-NULL
bank2$out<-NULL
logit<-glm(y~.,data=bank2,family=binomial)
summary(logit)
plot(logit)
plot(logit,which=4)
library("car")
outlierTest(logit)
bank3<-bank2[-c(1676, 52, 1904),]
logit.2<-glm(y~factor(contact) + factor(month) + duration +  balance + previous + factor(loan)+ factor(default) ,data=bank3,family=binomial)
summary(logit.2)
plot(logit.2,which=6)
plot(logit.2)
table(bank3$y)
table(y)
library("stats")
library("MASS")
stepAIC(logit.2,k=2)
logit.aic<- glm(formula = y ~ factor(contact) + factor(month) + duration +
balance + previous, family = binomial, data = bank3)
summary(logit.aic)
plot(logit.aic)
stepAIC(logit.2,k=log(length(bank3[,1])))
logit.bic<-glm(formula = y ~duration, family = binomial, data = bank3)
summary(logit.bic)
plot(logit.bic)
prediction <- data.frame(predict(logit.bic,bank3,type="response"))
prediction[prediction<0.5]=0
prediction[prediction>=0.5]=1
predictions <- data.frame(Prediction = as.numeric(prediction[,1]),Actual = as.numeric(bank3$y)-1)
predictions$Correct <- (predictions$Actual == predictions$Prediction)
logistic_accuracy<-table(predictions$Correct)/length(predictions$Correct)*100
logistic_accuracy
table(y , prediction)
table(bank3$y, bank3$prediction)
table(predictions$Actual, pedictions$Prediction)
table(predictions$Actual, predictions$Prediction)
table(predictions$Actual, predictions$Prediction)
table(predictions.test$Actual, predictions.test$Prediction)
prediction.test<-data.frame(predict(logit.bic,testing.set,type="response"))
prediction.test[prediction.test<0.5]=0
prediction.test[prediction.test>=0.5]=1
predictions.test <- data.frame(Prediction = as.numeric(prediction.test[,1]),Actual = as.numeric(testing.set$y)-1)
predictions.test$Correct <- (predictions.test$Actual == predictions.test$Prediction)
logistic_accuracy.test<-table(predictions.test$Correct)/length(predictions.test$Correct)*100
logistic_accuracy.test
table(predictions.test$Actual, predictions.test$Prediction)
#install.packages("ElemStatLearn")
#install.packages("tree")
#install.packages("rpart")
#install.packages("rattle")
#install.packages("rpart.plot")
#install.packages("RcolorBrewer")
library(ElemStatLearn)
library(tree)
require(rpart)
library(rpart)
tree <- rpart(y~factor(contact) + factor(month) + duration +
balance + previous, data=bank3, method="class")
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(tree,main = "", sub = "",cex=0.5)
printcp(tree)
plotcp(tree)
# visualize cross-validation results
# With the plot of CP, I select threshold cp as 0.0195
# Next, I prune my tree model to avoid over-fitting
tree.prune = prune(tree, cp = 0.0195)
fancyRpartPlot(tree.prune,main = "", sub = "",cex=0.5)
tree.prune = prune(tree, cp = 0.02)
fancyRpartPlot(tree.prune,main = "", sub = "",cex=0.5)
plotcp(tree)
tree.prune = prune(tree, cp = 0.05)
fancyRpartPlot(tree.prune,main = "", sub = "",cex=0.5)
#install.packages("randomForest")
library(randomForest)
forest<-randomForest(as.factor(y)~contact + month+ duration + balance+
previous,data=bank2, importance=TRUE, ntree=100)
#nstead of specifying method="class" as with rpart, we force the model
#to predict our classification by temporarily changing our target
#variable to a factor with only two levels using as.factor(). The
#importance=TRUE argument allows us to inspect variable importance
#as we'll see, and the ntree argument specifies how many trees we want to grow.
#If you were working with a larger dataset you may want to reduce
#the number of trees, at least for initial exploration, or restrict the complexity
#of each tree using nodesize as well as reduce the number of rows sampled with
#sampsize. You can also override the default number of variables to choose
#from with mtry, but the default is the square root of the total number
#available and that should work just fine. Since we only have a small
#dataset to play with, we can grow a large number of trees and not worry
#too much about their complexity, it will still run pretty fast.
summary(forest)
varImpPlot(forest)
#There's two types of importance measures shown above.
#The accuracy one tests to see how worse the model
#performs without each variable, so a high decrease
#in accuracy would be expected for very predictive variables.
#The Gini one digs into the mathematics behind decision trees,
#but essentially measures how pure the nodes are at the end of the tree.
#Again it tests to see the result if each variable is taken out and
#a high score means the variable was important.
#install.packages("e1071")
#install.packages("kernlab")
library(e1071)
library(kernlab)
#bank3$ynum=(bank3$y=='yes')*1+0
svm.fit = ksvm(y~ contact + month+ duration + balance+
previous, data = bank3, type="C-svc", kernel="rbfdot", C=10)
svm.pred <- predict(svm.fit, bank3, type = "decision")
library(ggplot2)
qplot(svm.pred, bank3$duration, color=bank3$y)
summary(svm.fit)
#install.packages("e1071")
#install.packages("kernlab")
library(e1071)
library(kernlab)
#bank3$ynum=(bank3$y=='yes')*1+0
svm.fit = ksvm(y~ contact + month+ duration + balance+
previous, data = bank3, type="C-svc", kernel="rbfdot", C=10)
svm.pred <- predict(svm.fit, bank3, type = "decision")
library(ggplot2)
qplot(svm.pred, bank3$duration, color=bank3$y)
summary(svm.fit)
#install.packages("e1071")
#install.packages("kernlab")
library(e1071)
library(kernlab)
#bank3$ynum=(bank3$y=='yes')*1+0
svm.fit = ksvm(y~ contact + month+ duration + balance+
previous, data = bank3, type="C-svc", kernel="rbfdot", C=10)
svm.pred <- predict(svm.fit, bank3, type = "decision")
library(ggplot2)
qplot(svm.pred, bank3$balance, color=bank3$y)
summary(svm.fit)
#install.packages("e1071")
#install.packages("kernlab")
library(e1071)
library(kernlab)
#bank3$ynum=(bank3$y=='yes')*1+0
svm.fit = ksvm(y~ contact + month+ duration + balance+
previous, data = bank3, type="C-svc", kernel="rbfdot", C=10)
svm.pred <- predict(svm.fit, bank3, type = "decision")
library(ggplot2)
qplot(svm.pred, bank3$duration, color=bank3$y)
summary(svm.fit)
plot(cars)
getwd()
bank<-read.csv("bank.csv",header=T,sep=";", dec=".")
set.seed(123456789)
random<-sample(1:nrow(bank))
num.bank.training<-as.integer(0.75*length(random))
bank.indices<-random[1:num.bank.training]
train<-bank[bank.indices,]
testing.indices<-random[(num.bank.training+1):length(random)]
testing.set<-bank[testing.indices,]
logis<-glm(y ~ ., data=train,family=binomial)
summary(logis)
plot(logis)
plot(logis,which=6)
boxplot(train[,1:(ncol(bank)-1)])
bank1<-train
bank1$dffits<-0
bank1$dffits<-dffits(logis)
bank2<-bank1[!bank1$dffits>2*sqrt(ncol(bank)/nrow(bank)),]
bank1$dfitts<-NULL
bank2$dfitts<-NULL
bank1$out<-NULL
bank2$out<-NULL
logit<-glm(y~.,data=bank2,family=binomial)
summary(logit)
plot(logit)
plot(logit,which=4)
library("car")
outlierTest(logit)
bank3<-bank2[-c(1676, 52, 1904),]
logit.2<-glm(y~factor(contact) + factor(month) + duration +  balance + previous + factor(loan)+ factor(default) ,data=bank3,family=binomial)
summary(logit.2)
plot(logit.2,which=6)
plot(logit.2)
table(bank3$y)
library("stats")
library("MASS")
stepAIC(logit.2,k=2)
logit.aic<- glm(formula = y ~ factor(contact) + factor(month) + duration +
balance + previous, family = binomial, data = bank3)
summary(logit.aic)
plot(logit.aic)
stepAIC(logit.2,k=log(length(bank3[,1])))
logit.bic<-glm(formula = y ~duration, family = binomial, data = bank3)
summary(logit.bic)
plot(logit.bic)
prediction <- data.frame(predict(logit.bic,bank3,type="response"))
prediction[prediction<0.5]=0
prediction[prediction>=0.5]=1
predictions <- data.frame(Prediction = as.numeric(prediction[,1]),Actual = as.numeric(bank3$y)-1)
predictions$Correct <- (predictions$Actual == predictions$Prediction)
logistic_accuracy<-table(predictions$Correct)/length(predictions$Correct)*100
logistic_accuracy
#table(predictions$Actual, predictions$Prediction)
prediction.test<-data.frame(predict(logit.bic,testing.set,type="response"))
prediction.test[prediction.test<0.5]=0
prediction.test[prediction.test>=0.5]=1
predictions.test <- data.frame(Prediction = as.numeric(prediction.test[,1]),Actual = as.numeric(testing.set$y)-1)
predictions.test$Correct <- (predictions.test$Actual == predictions.test$Prediction)
logistic_accuracy.test<-table(predictions.test$Correct)/length(predictions.test$Correct)*100
logistic_accuracy.test
#table(predictions.test$Actual, predictions.test$Prediction)
#install.packages("ElemStatLearn")
#install.packages("tree")
#install.packages("rpart")
#install.packages("rattle")
#install.packages("rpart.plot")
#install.packages("RcolorBrewer")
library(ElemStatLearn)
library(tree)
require(rpart)
library(rpart)
tree <- rpart(y~factor(contact) + factor(month) + duration +
balance + previous, data=bank3, method="class")
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(tree,main = "", sub = "",cex=0.5)
# proportion of "no", "yes" and sample proportion
printcp(tree)
plotcp(tree)
tree.prune = prune(tree, cp = 0.05)
fancyRpartPlot(tree.prune,main = "", sub = "",cex=0.5)
prediction.tree <- data.frame(predict(tree.prune, bank3, type = "class"))
predictions.tree <- data.frame(Prediction = as.numeric(prediction.tree[,1])-1,Actual = as.numeric(bank3$y)-1)
predictions.tree$Correct <- (predictions.tree$Actual == predictions.tree$Prediction)
Tree_Accuracy <- table(predictions.tree$Correct)/length(predictions.tree$Correct)*100
Tree_Accuracy
prediction.tree.test<-data.frame(predict(tree.prune,testing.set,
type="class"))
predictions.tree.t <- data.frame(Prediction = as.numeric(prediction.tree.test[,1])-1,Actual = as.numeric(testing.set$y)-1)
predictions.tree.t$Correct <- (predictions.tree.t$Actual == predictions.tree.t$Prediction)
Tree_Accuracy.t <- table(predictions.tree.t$Correct)/length(predictions.tree.t$Correct)*100
Tree_Accuracy.t
#install.packages("randomForest")
library(randomForest)
forest<-randomForest(as.factor(y)~contact + month+ duration + balance+
previous,data=bank2, importance=TRUE, ntree=100)
#nstead of specifying method="class" as with rpart, we force the model
#to predict our classification by temporarily changing our target
#variable to a factor with only two levels using as.factor(). The
#importance=TRUE argument allows us to inspect variable importance
#as we'll see, and the ntree argument specifies how many trees we want to grow.
#If you were working with a larger dataset you may want to reduce
#the number of trees, at least for initial exploration, or restrict the complexity
#of each tree using nodesize as well as reduce the number of rows sampled with
#sampsize. You can also override the default number of variables to choose
#from with mtry, but the default is the square root of the total number
#available and that should work just fine. Since we only have a small
#dataset to play with, we can grow a large number of trees and not worry
#too much about their complexity, it will still run pretty fast.
summary(forest)
varImpPlot(forest)
#There's two types of importance measures shown above.
#The accuracy one tests to see how worse the model
#performs without each variable, so a high decrease
#in accuracy would be expected for very predictive variables.
#The Gini one digs into the mathematics behind decision trees,
#but essentially measures how pure the nodes are at the end of the tree.
#Again it tests to see the result if each variable is taken out and
#a high score means the variable was important.
prediction.forest <- data.frame(predict(forest, bank2, type = "class"))
predictions.forest <- data.frame(Prediction = as.numeric(prediction.forest[,1])-1,Actual = as.numeric(bank2$y)-1)
predictions.forest$Correct <- (predictions.forest$Actual == predictions.forest$Prediction)
forest_Accuracy <- table(predictions.forest$Correct)/length(predictions.forest$Correct)*100
forest_Accuracy
prediction.forest.test<-data.frame(predict(forest,testing.set,type="class"))
predictions.forest.t <- data.frame(Prediction = as.numeric(prediction.forest.test[,1])-1,Actual = as.numeric(testing.set$y)-1)
predictions.forest.t$Correct <- (predictions.forest.t$Actual == predictions.forest.t$Prediction)
Forest_Accuracy.t <- table(predictions.forest.t$Correct)/length(predictions.forest.t$Correct)*100
Forest_Accuracy.t
#install.packages("e1071")
#install.packages("kernlab")
library(e1071)
library(kernlab)
svm.fit = ksvm(y~ contact + month+ duration + balance+
previous, data = bank3, type="C-svc", kernel="rbfdot", C=10)
svm.pred <- predict(svm.fit, bank3, type = "decision")
library(ggplot2)
qplot(svm.pred, bank3$duration, color=bank3$y)
summary(svm.fit)
prediction.svm <- data.frame(predict(svm.fit, bank3))
predictions.svm <- data.frame(Prediction = as.numeric(prediction.svm[,1])-1,Actual = as.numeric(bank3$y)-1)
predictions.svm$Correct <- (predictions.svm$Actual == predictions.svm$Prediction)
svm_Accuracy <- table(predictions.svm$Correct)/length(predictions.svm$Correct)*100
svm_Accuracy
prediction.svm.test<-data.frame(predict(svm.fit,testing.set))
predictions.svm.t <- data.frame(Prediction = as.numeric(prediction.svm.test[,1])-1,Actual = as.numeric(testing.set$y)-1)
predictions.svm.t$Correct <- (predictions.svm.t$Actual == predictions.svm.t$Prediction)
svm_Accuracy.t <- table(predictions.svm.t$Correct)/length(predictions.svm.t$Correct)*100
svm_Accuracy.t
load("Prog-09-allobjects.Rdata")
save.image("Prog-09-allobjects.Rdata")
load("Prog-09-allobjects.Rdata")
Pred<-rbind(logistic_accuracy, Tree_Accuracy, c(0, forest_Accuracy), svm_Accuracy)
rownames(Pred)[3]="Forest_accuracy"
knitr::kable(Pred,digits=2, caption="Prediction in Training data")
Pred<-rbind(logistic_accuracy.test, Tree_Accuracy.t, Forest_Accuracy.t, svm_Accuracy.t)
knitr::kable(Pred,digits=2, caption="Prediction in Test data")
install.packages("C:/Users/UBrisk/Desktop/POSTGRAU-DATA-SCIENCE/CARPETA_APUNTS/TO-GIVE/FINAL-DAY-2/R4DSUB.zip", repos = NULL, type = "win.binary")
library(R4DSUB)
install.packages("C:/Users/UBrisk/Desktop/POSTGRAU-DATA-SCIENCE/CARPETA_APUNTS/TO-GIVE/PACK-R4DS_UB/R4DSUB/R4DSUB.zip", repos = NULL, type = "win.binary")
install.packages("C:/Users/UBrisk/Desktop/POSTGRAU-DATA-SCIENCE/CARPETA_APUNTS/TO-GIVE/PACK-R4DS_UB/R4DSUB.zip", repos = NULL, type = "win.binary")
library(R4DSUB)
install.packages("C:/Users/UBrisk/Desktop/POSTGRAU-DATA-SCIENCE/CARPETA_APUNTS/TO-GIVE/PACK-R4DS_UB/R4DSUB.zip", repos = NULL, type = "win.binary")
library(R4DSUB)
